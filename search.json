[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html",
    "href": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html",
    "title": "Predicting swarm behaviour",
    "section": "",
    "text": "To practice my deep learning chops/model-writing skills, I pick a toy dataset. Here I aim to predict swarm behaviour.\n\n\nCode\nimport torch\nimport numpy as np, os, sys, requests, pandas as pd, zipfile as zf\nimport matplotlib.pyplot as plt\n\nprint(f'Kernel used is: {os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))}')\n\n\nKernel used is: dl-tools\n\n\n\n\nCode\nprint(f'Pytorch version is {torch.__version__}')\n\n\nPytorch version is 2.0.1+cu117\n\n\n\n\nCode\ntorch.cuda.is_available(), torch.cuda.current_device(), torch.cuda.device_count(), torch.cuda.get_device_name(0)\n\n\n(True, 0, 1, 'Quadro P1000')\n\n\n\n\nCode\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n\n\nUsing device: cuda\n\nQuadro P1000\nMemory Usage:\nAllocated: 0.0 GB\nCached:    0.0 GB\n\n\n\n\n\n\nCode\ndata_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00524/Swarm%20Behavior%20Data.zip'\ndata_dir = '/home/temi/Files/learning_stuff/pytorch_implementations/swarm behaviour'\n\n\n\n\nCode\nif not 'data' in os.listdir(data_dir):\n    os.mkdir('/home/temi/Projects/swarm_behaviour/data/')\nelse:\n    print('data directory already exists.')\n\n\ndata directory already exists.\n\n\n\n\nCode\ndef download_url(url, save_path, chunk_size=128):\n    r = requests.get(url, stream=True)\n    with open(save_path, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            fd.write(chunk)\n\n\n\n\nCode\ndata_path = os.path.join(data_dir, 'data', 'swarm_behaviour.zip')\n\nif not 'swarm_behaviour.zip' in os.listdir(os.path.join(data_dir, 'data')):\n    print('Downloading...')\n    download_url(data_link, )\n    print('Done.')\nelse:\n    print('File already present.')\n\n\nFile already present.\n\n\n\n\nCode\nwith zf.ZipFile(data_path, 'r') as obj:\n    name_of_files = obj.namelist()[1:]\n\nname_of_files\n\n\n['Swarm Behavior Data/Aligned.csv',\n 'Swarm Behavior Data/Flocking.csv',\n 'Swarm Behavior Data/Grouped.csv']\n\n\n\n\nCode\n# read in each file into a dictionary\nkeys = ['aligned', 'flocking', 'grouped']\nswarm_data = dict()\nfor i, file_name in enumerate(name_of_files):\n    zipfile = zf.ZipFile(data_path)\n    swarm_data[keys[i]] = pd.read_csv(zipfile.open(file_name))\n    zipfile.close()\n\n\n/tmp/ipykernel_184041/1264924189.py:6: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n  swarm_data[keys[i]] = pd.read_csv(zipfile.open(file_name))\n\n\n\n\nCode\nswarm_data['aligned'].iloc[0:5,:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.00\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\nswarm_data['flocking'].iloc[0:5,0:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.0\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\nswarm_data['grouped'].iloc[0:5,:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.00\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\n\ndef split_into_train_val_test(df, tr_frac=0.5, rem_frac=0.5, seed=1, where_target='last',\n                             return_what='dict'):\n    \n    # shuffle the dataframe\n    df = df.sample(frac=1, random_state=seed)\n\n    if where_target=='last':\n        df_preds = df.iloc[:, :-1]\n        df_targets = df.iloc[:, -1]\n\n    # split into training and testing\n    # 0.7, 0.3\n    split_one = int(df_preds.shape[0] * tr_frac)\n\n    df_preds_train = df_preds[:split_one]\n    df_targets_train = df_targets[:split_one]\n    remaining_preds, remaining_targets = df_preds[split_one:], df_targets[split_one:]\n\n    split_two = int(remaining_preds.shape[0]*rem_frac)\n    df_preds_val = remaining_preds[:split_two]\n    df_preds_test = remaining_preds[split_two:]\n\n    df_targets_val = remaining_targets[:split_two]\n    df_targets_test = remaining_targets[split_two:]\n\n    print(df_preds_train.shape, df_targets_train.shape)\n    print(df_preds_val.shape, df_targets_val.shape)\n    print(df_preds_test.shape, df_targets_test.shape)\n    \n    if return_what=='dict':\n        return  {'train': (df_preds_train.to_numpy(), df_targets_train.to_numpy()), \n                 'validate': (df_preds_val.to_numpy(), df_targets_val.to_numpy()),\n                 'test': (df_preds_test.to_numpy(), df_targets_test.to_numpy())}\n    elif return_what=='tuple':\n        return ((df_preds_train.to_numpy(), df_targets_train.to_numpy()), \n                (df_preds_val.to_numpy(), df_targets_val.to_numpy()), \n                (df_preds_test.to_numpy(), df_targets_test.to_numpy()))"
  },
  {
    "objectID": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html#downloading-the-data-and-so-on",
    "href": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html#downloading-the-data-and-so-on",
    "title": "Predicting swarm behaviour",
    "section": "",
    "text": "Code\ndata_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00524/Swarm%20Behavior%20Data.zip'\ndata_dir = '/home/temi/Files/learning_stuff/pytorch_implementations/swarm behaviour'\n\n\n\n\nCode\nif not 'data' in os.listdir(data_dir):\n    os.mkdir('/home/temi/Projects/swarm_behaviour/data/')\nelse:\n    print('data directory already exists.')\n\n\ndata directory already exists.\n\n\n\n\nCode\ndef download_url(url, save_path, chunk_size=128):\n    r = requests.get(url, stream=True)\n    with open(save_path, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            fd.write(chunk)\n\n\n\n\nCode\ndata_path = os.path.join(data_dir, 'data', 'swarm_behaviour.zip')\n\nif not 'swarm_behaviour.zip' in os.listdir(os.path.join(data_dir, 'data')):\n    print('Downloading...')\n    download_url(data_link, )\n    print('Done.')\nelse:\n    print('File already present.')\n\n\nFile already present.\n\n\n\n\nCode\nwith zf.ZipFile(data_path, 'r') as obj:\n    name_of_files = obj.namelist()[1:]\n\nname_of_files\n\n\n['Swarm Behavior Data/Aligned.csv',\n 'Swarm Behavior Data/Flocking.csv',\n 'Swarm Behavior Data/Grouped.csv']\n\n\n\n\nCode\n# read in each file into a dictionary\nkeys = ['aligned', 'flocking', 'grouped']\nswarm_data = dict()\nfor i, file_name in enumerate(name_of_files):\n    zipfile = zf.ZipFile(data_path)\n    swarm_data[keys[i]] = pd.read_csv(zipfile.open(file_name))\n    zipfile.close()\n\n\n/tmp/ipykernel_184041/1264924189.py:6: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n  swarm_data[keys[i]] = pd.read_csv(zipfile.open(file_name))\n\n\n\n\nCode\nswarm_data['aligned'].iloc[0:5,:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.00\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\nswarm_data['flocking'].iloc[0:5,0:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.0\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\nswarm_data['grouped'].iloc[0:5,:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.00\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\n\ndef split_into_train_val_test(df, tr_frac=0.5, rem_frac=0.5, seed=1, where_target='last',\n                             return_what='dict'):\n    \n    # shuffle the dataframe\n    df = df.sample(frac=1, random_state=seed)\n\n    if where_target=='last':\n        df_preds = df.iloc[:, :-1]\n        df_targets = df.iloc[:, -1]\n\n    # split into training and testing\n    # 0.7, 0.3\n    split_one = int(df_preds.shape[0] * tr_frac)\n\n    df_preds_train = df_preds[:split_one]\n    df_targets_train = df_targets[:split_one]\n    remaining_preds, remaining_targets = df_preds[split_one:], df_targets[split_one:]\n\n    split_two = int(remaining_preds.shape[0]*rem_frac)\n    df_preds_val = remaining_preds[:split_two]\n    df_preds_test = remaining_preds[split_two:]\n\n    df_targets_val = remaining_targets[:split_two]\n    df_targets_test = remaining_targets[split_two:]\n\n    print(df_preds_train.shape, df_targets_train.shape)\n    print(df_preds_val.shape, df_targets_val.shape)\n    print(df_preds_test.shape, df_targets_test.shape)\n    \n    if return_what=='dict':\n        return  {'train': (df_preds_train.to_numpy(), df_targets_train.to_numpy()), \n                 'validate': (df_preds_val.to_numpy(), df_targets_val.to_numpy()),\n                 'test': (df_preds_test.to_numpy(), df_targets_test.to_numpy())}\n    elif return_what=='tuple':\n        return ((df_preds_train.to_numpy(), df_targets_train.to_numpy()), \n                (df_preds_val.to_numpy(), df_targets_val.to_numpy()), \n                (df_preds_test.to_numpy(), df_targets_test.to_numpy()))"
  },
  {
    "objectID": "content/blog/posts/2023-08-29-simulating-populations/index.html",
    "href": "content/blog/posts/2023-08-29-simulating-populations/index.html",
    "title": "Simulating discrete and continuous populations",
    "section": "",
    "text": "Code\nlibrary(gtools) # a library\n\n\n\nIntroduction\nOftentimes, we want to simulate populations in order to do some analysis. In this demo, I go over a, perhaps, crude way to mimic discrete and continuous populations. Before simulating, we need to know what a population is. In genetics, a population is a grouping of individuals based on some similarity in allele frequencies between them. Of course, there are many other definitions of what a population is. In this demo, I will use the definition based on allele frequencies.\nOne way to generate allele frequencies that mimic that seen in typical populations is to use the \\(F_{ST}\\), AKA the fixation index, AKA F-statistic. \\(F_{ST}\\) measures population differentiation as a result of genetic structure. Values range between 0 and 1, and higher values mean that two populations are highly similar. There are many definitions of \\(F_{ST}\\), depending on what you are discussing - and I am still learning about most of them. In this case, however, I am primarily concerned with allele frequency-related definitions.\n\n\nSimulating discrete populations\nTo simulate populations based on the definition of \\(F_{ST}\\), one thing we can do is to simulate an ancestral population’s allele frequencies. This is similar to having a founder population from which every other population descends. Afterwards, we can simulate different populations from this ancestral population. This follows from the Balding-Nichols model.\n\\[\\begin{align}\nBeta(\\frac{1-F}{F}p, \\frac{1-F}{F}(1-p))\n\\end{align}\\]\nAssume that we have 500 alleles/loci, and 4 populations, and we intend to simulate 1000 individuals from each of these populations. We can define our \\(F_{ST}\\) to be, say, 0.09 between populations 1 and 2, 0.19 between populations 1 and 2, and population 3, and 0.4 between all these populations, and population 4. Therefore, populations 1 and 2 will be closely related, and different from population 3, and all of them will be different from population 4.\n\n\nCode\nM &lt;- 500 # number of alleles or SNPs\nn_pops &lt;- 4 # number of populations\nN &lt;- 1000 # 20 individuals in each n_pops\nf_pop12 &lt;- 0.09\nf_pop3 &lt;- 0.19 # a third, distant, unrelated population\nf_pop4 &lt;- 0.4 # a fourth population\n\n\nWhen we make plots of the allele frequencies of these populations, we expect high correlations between populations 1 and 2, and not-so-high correlations when we compare with the other populations 3 and 4.\nWe can generate some random minor allele frequencies (MAFs) to be the ancestral allele frequencies, and generate independent draws from the distribution of the ancestral allele frequencies, based on the equation above.\n\n\nCode\nancestral_allele_freqs &lt;- runif(M, 0.01, 0.5)\nsh1 &lt;- ((1-f_pop12)/f_pop12)*ancestral_allele_freqs\nsh2 &lt;- ((1-f_pop12)/(f_pop12))*(1-ancestral_allele_freqs)\npop1_allele_freqs &lt;- rbeta(M, shape1 = sh1, shape2 = sh2)\npop2_allele_freqs &lt;- rbeta(M, shape1 = sh1, shape2 = sh2)\npop3_allele_freqs &lt;- rbeta(M, shape1 = ((1-f_pop3)/f_pop3)*ancestral_allele_freqs, shape2=((1-f_pop3)/(f_pop3))*(1-ancestral_allele_freqs))\npop4_allele_freqs &lt;- rbeta(M, shape1 = ((1-f_pop4)/f_pop4)*ancestral_allele_freqs, shape2=((1-f_pop4)/(f_pop4))*(1-ancestral_allele_freqs))\n\nall_allele_freqs &lt;- list(pop1_allele_freqs, pop2_allele_freqs, pop3_allele_freqs, pop4_allele_freqs)\nnames(all_allele_freqs) &lt;- paste0('population_', 1:4)\n\n\n\nFirst, how does the expected value of the distribution of allele frequency changes as a factor of the \\(F_{ST}\\)? This should give us an idea of what to expect when we simulate allele frequencies.\n\n\nCode\n# generate many Fst values\nfst_vector &lt;- round(seq(0.01, 0.99, length.out=20), 2) # 20 fsts ranging from 0.01 to 0.99\nper_fst &lt;- lapply(fst_vector, function(each_fst){\n  \n  sh1 &lt;- ((1-each_fst)/each_fst)*ancestral_allele_freqs\n  sh2 &lt;- ((1-each_fst)/(each_fst))*(1-ancestral_allele_freqs)\n  temp_allele_frq &lt;- rbeta(M, shape1 = sh1, shape2 = sh2)\n  deviation &lt;- cor(ancestral_allele_freqs, temp_allele_frq) #|&gt; suppressWarnings()\n  return(deviation)\n})\n\n\n\n\nCode\ncor_deviation &lt;- per_fst |&gt; unlist()\n\nplot(cor_deviation, frame.plot=F, xaxt='n', yaxt='n', type='b', xlab = expression(F[ST]), ylab=expression(R^2))\naxis(1, at=rep(1:length(cor_deviation), by=1), labels = fst_vector, las=2)\naxis(2, at=seq(0, 1, 0.1))\n\n\n\n\n\nWe see that as the \\(F_{ST}\\) increases, the correlation between the ancestral allele frequency and the generated allele frequency reduces.\n\n\nCode\nlayout(matrix(c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = T))\nnames_pop &lt;- names(all_allele_freqs)\nfor(i in 1:4){\n    for(j in 1:4){\n        if(i == j){ # we don't want to plot pop1 vs pop1 e.t.c.\n            next\n        } else if (i &lt; j) {\n            #cor_ &lt;- round(cor(all_allele_freqs[[i]], all_allele_freqs[[j]]), 2)\n            plot(all_allele_freqs[[i]], all_allele_freqs[[j]], xlab = names_pop[i], ylab = names_pop[j], frame.plot = F)\n            #abline(a=0, b=cor_, col='red')\n            #mtext(cor_)\n        }\n    }\n}\nmtext('Scatterplot of allele frequencies for each population', outer = T)\n\n\n\n\n\nThe plot is approximately (roughly?) what we expect!\n\n\n\nSimulating genotypes using these allele frequencies\nGiven these allele frequencies, we can simulate genotypes from each of these populations by sampling from a binomial distribution.\n\\[\\begin{align}\nG_{k_{n}} \\sim Binom(n, p_{k})\n\\end{align}\\]\nwhere \\(n\\) is the number of success, and \\(p\\) is the probability of a success. Here, \\(n = 2\\) because we want 2 alleles per loci, and \\(p\\) is the allele frequency at that loci.\n\n\nCode\ngenotypes &lt;- lapply(seq_along(all_allele_freqs), function(i){\n    matrix(data=rbinom(n=N*M, size=2, prob = all_allele_freqs[[i]]), nrow = N, ncol = M, byrow = T)\n})\n\ngenotypes &lt;- do.call(rbind, genotypes)\nshuffle &lt;- sample(1:nrow(genotypes)) # I will shuffle the data\ngenotypes &lt;- genotypes[shuffle, ]\npops &lt;- rep(names(all_allele_freqs), each=N)\npops &lt;- pops[shuffle] # shuffle pops too, since it tells me what populations the genotypes come from\n\ncat('Here is what the genotypes look like\\n')\n\n\nHere is what the genotypes look like\n\n\nCode\ngenotypes[1:5, 1:5]\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    1    0    1    0    0\n[3,]    0    1    0    0    0\n[4,]    1    0    0    0    0\n[5,]    0    0    0    0    0\n\n\nCode\ncat('And here is what the population designation looks like \\n')\n\n\nAnd here is what the population designation looks like \n\n\nCode\npops[1:5]\n\n\n[1] \"population_1\" \"population_3\" \"population_2\" \"population_3\" \"population_4\"\n\n\nNext, we can calculate the principal components (PCs) of this data, and plot PC1 vs PC2, to see where the most variability lies in the data\n\n\nCode\n# first change the row names of the genotype data to reflect the populations\n#rownames(genotypes) &lt;- pops\n# I need to make sure that there are some variations in the SNPS - if there aren't any, I should remove them\npca_genotypes &lt;- genotypes[ , which(apply(genotypes, 2, var) != 0)]\n# PCA\npca_dt &lt;- prcomp(pca_genotypes, scale.=T, center=T)\npca_dt$x[1:5, 1:5]\n\n\n            PC1       PC2        PC3         PC4        PC5\n[1,]   3.263826 -7.849732 -6.9217690 -2.66479261 -3.9900565\n[2,]   6.544296  8.367773 -1.7339642 -0.06842935  2.8759937\n[3,]   6.086323 -4.769098  6.4930496 -1.81845701 -1.7244237\n[4,]   5.730047  7.073629 -1.7772414 -1.97970506 -0.2309777\n[5,] -12.399378  1.580702  0.6451658  0.24943503  1.3446776\n\n\n\n\nCode\n# colors\nmycolors &lt;- colorRampPalette(RColorBrewer::brewer.pal(8, \"Dark2\"))(n_pops)\n#mycolors &lt;- c('red', 'orange', 'green', 'blue')\ncolor_coding &lt;- factor(pops, labels = mycolors)\nplot(pca_dt$x[, 'PC1'], pca_dt$x[, 'PC2'], bg=as.character(color_coding), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of 4 discrete populations')\nlegend('topleft', legend=names_pop, pch=c(21, 21, 21, 21), pt.bg=mycolors, bty = 'n', xpd=NA)\n\n\n\n\n\nWe can look at populations 1 and 2 only, and see if there is any difference between them.\n\n\nCode\npop_names &lt;- c('population_1', 'population_2')\nidx &lt;- which(pops %in% pop_names)\npca_12 &lt;- pca_dt$x[idx, ]\ncolor_coding &lt;- factor(pops[idx], labels = mycolors[1:2])\nplot(pca_12[, 'PC1'], pca_12[, 'PC2'], bg=as.character(color_coding), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of the 2 similar populations 1 and 2')\nlegend('topleft', legend=pop_names, pch=c(21, 21), pt.bg=mycolors[1:2], bty = 'n', xpd=NA)\n\n\n\n\n\n\n\nSimulating continuous populations\nWhat I have done so far, is to simulate a discrete population of individuals. Think of it as people living in Mars, Earth, and Jupiter, and who have never intermingled since time immemorial. Population 3 is faraway from population 4. However, true human populations are not this way. How can we simulate a truly continuous population?\nFirst, notice that I am only simulating allele frequencies. Therefore, I can put a prior on my earlier-defined beta distribution to model the probability that an individual is from population \\(m\\). In other words, I am trying to estimate what percentage of an individual’s alleles might have come from a certain population \\(m\\). I will call this random variable, \\(Q\\).\n\\[\\begin{align}\nQ \\sim Dirichlet(\\alpha)\n\\end{align}\\]\nA trick I can use is to take an individual, ask how much of their genome is shared among each population I have, select that proportion of allele frequencies, and simulate their genotypes based on the selection for each population. To do this, I need to know how to distribute the \\(\\alpha\\) parameter used in the Dirichlet distribution. A trick I can use is to use the allele frequency distribution in the entire population. I will call this distribution the population grade distribution.\nEarlier, I simulated some allele frequencies all_allele_freqs\n\n\nCode\nss &lt;- sapply(all_allele_freqs, sum)\npopulation_grade &lt;- ss/sum(ss)\n#population_grade &lt;- rand_vect_cont(4, 1)\npopulation_grade\n\n\npopulation_1 population_2 population_3 population_4 \n   0.2462436    0.2509962    0.2512640    0.2514962 \n\n\nThen I can use this as a parameter to the Dirichlet distribution to simulate the percentage of an individual’s genotypes that is shared across all 4 populations.\n\n\nCode\nq_matrix &lt;- gtools::rdirichlet(N * n_pops, alpha = population_grade) # I want to simulate, N individuals per population\n\n\nHere is an illustration. Assuming that I am looking at the first individual\n\n\nCode\n# assuming one individual\nid1 &lt;- q_matrix[1, ]\nq_counts &lt;- floor(id1 * M) # number of loci shared\nq_counts\n\n\n[1]   0  43 340 116\n\n\nThe assumption here is that this individual shares 0 loci with population 1, 43 with population 2, 340 with population 3, and 116 loci with population 4, and 3 independent loci not share with any population. The sum is 499 (because I rounded down), but I don’t want this to be this case. Therefore, I will share the remaining loci with the highest number already available, so that the sum is 500.\n\n\nCode\nremainder_ &lt;- M - sum(q_counts)\nq_counts[which.max(q_counts)] &lt;- q_counts[which.max(q_counts)] + remainder_\nq_counts\n\n\n[1]   0  43 341 116\n\n\nNow, this sums to 500.\nPutting this into a reusable function…\n\n\nCode\nq_counts_fxn &lt;- function(q_matrix, M){\n    apply(q_matrix, 1, function(each_row){\n        temp &lt;- floor(each_row * M)\n        remainder_ &lt;- M - sum(temp)\n        temp[which.max(temp)] &lt;- temp[which.max(temp)] + remainder_\n        temp\n    }) |&gt; t()\n}\n\nq_counts &lt;- q_counts_fxn(q_matrix, M)\nq_counts[1:5, ] # for the first 5 individuals\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    0   43  341  116\n[2,]  385    0   43   72\n[3,]   10   16  364  110\n[4,]    0  370   12  118\n[5,]  103  256    0  141\n\n\n\n\nCode\ngenotypes &lt;- matrix(NA, nrow = N*n_pops, ncol = M) # remember, N individuals per population\n\nfor(q in 1:nrow(q_counts)){\n    pop_down &lt;- 1\n    m_loci &lt;- 1:M\n    i_loci &lt;- list() # this list contains the actual loci an individual has shared among the other populations\n    while(pop_down &lt;= ncol(q_counts)){\n        # sample pop 1 loci\n        pop_loci &lt;- sample(m_loci, size=q_counts[q, pop_down], replace = F)\n        if(length(pop_loci) == 0){\n            m_loci &lt;- m_loci[m_loci != 0]\n            i_loci[[pop_down]] &lt;- 0\n        } else {\n            m_loci &lt;- m_loci[!m_loci %in% pop_loci]\n            i_loci[[pop_down]] &lt;- pop_loci\n        }\n        pop_down &lt;- pop_down + 1\n    }\n    \n    g_loci &lt;- lapply(seq_along(i_loci), function(i){\n        temp &lt;- all_allele_freqs[[i]][i_loci[[i]]]\n        rbinom(n = length(temp), size = 2, prob = temp)\n    })\n\n    gi &lt;- vector(mode='numeric', length = M)\n    for(i in 1:n_pops){\n        gi[i_loci[[i]]] &lt;- g_loci[[i]]\n    }\n    \n    genotypes[q, ] &lt;- gi\n    \n}\n\n\n\n\nCode\npop &lt;- paste0('population_', apply(q_counts, 1, which.max))\n# first change the row names of the genotype data to reflect the populations\nrownames(genotypes) &lt;- pop\n# I need to make sure that there are some variations in the SNPS - if there aren't any, I should remove them\npca_genotypes &lt;- genotypes[ , which(apply(genotypes, 2, var) != 0)]\n# PCA\npca_dt &lt;- prcomp(pca_genotypes, scale.=T, center=T)\npca_dt$x[1:5, 1:5]\n\n\n                    PC1       PC2         PC3        PC4        PC5\npopulation_3  1.7973144 -5.028205 -0.35698361  2.6108432 -0.5580345\npopulation_1 -0.4990041  4.185811 -5.96843755  0.1924663  0.4836813\npopulation_3  1.9174774 -5.916136  0.09453596  1.5513878 -0.3693294\npopulation_2  1.6417866  2.424741  6.31743092 -1.8844746 -0.3070795\npopulation_2 -0.6605363  3.049585  0.82280376  1.9657889  0.9151204\n\n\n\n\nCode\nmycolors &lt;- colorRampPalette(RColorBrewer::brewer.pal(8, \"Dark2\"))(n_pops)\ncolor_coding &lt;- factor(rownames(pca_dt$x), labels = mycolors)\nplot(pca_dt$x[, 'PC1'], pca_dt$x[, 'PC2'], bg=color_coding |&gt; as.character(), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of 4 continuous populations')\nlegend('topleft', legend=names_pop, pch=c(21, 21, 21, 21), pt.bg=mycolors, bty = 'n', xpd = NA)"
  },
  {
    "objectID": "content/blog/posts/2023-09-03-parallelization-in-R/index.html",
    "href": "content/blog/posts/2023-09-03-parallelization-in-R/index.html",
    "title": "Parallelization in R",
    "section": "",
    "text": "When running computationally-heavy tasks in R, it can be useful to parallelize your codes/runs. In that vein, this is a really good blogpost to read to understand when and how to parallelize. And here is another one.\nR offers many ways to do this. Usually, I prefer using some libraries.\n\n\nCode\nlibrary(doParallel)\n\n\nLoading required package: foreach\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n\nCode\nlibrary(foreach)\nlibrary(parallel)\n\n\n\n\nAssuming we want to apply a function over the rows of a matrix. This function will take each row, divide the numbers in that row by the index of that row in the matrix and return a newly-created matrix of the same shape.\n\n\nCode\nset.seed(2022)\nmyMatrix &lt;- matrix(sample(1:50000, size=300*500, replace=T), nrow=300, ncol=500)\n\ndim(myMatrix)\n\n\n[1] 300 500\n\n\n\n\nFirst, I will time this function with lapply loops. lapply is shipped with base R and is a parallel form of a regular for loop.\n\n\nCode\nlapply_fxn &lt;- function(){\n  applyMatrix &lt;- lapply(1:nrow(myMatrix), function(i){\n    out &lt;- myMatrix[i, ] / (i)\n    return(out)\n  })\n  applyMatrix &lt;- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(lapply_fxn())\n\n\n   user  system elapsed \n  0.004   0.000   0.004 \n\n\n\n\n\nNext, let’s take advantage of the cores, this time using mclapply\n\n\nCode\nmclapply_fxn &lt;- function(){\n  applyMatrix &lt;- parallel::mclapply(1:nrow(myMatrix), function(i){\n    out &lt;- myMatrix[i, ] / (i)\n    return(out)\n  }, mc.cores = 12) # using 7 cores\n  \n  applyMatrix &lt;- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(mclapply_fxn())\n\n\n   user  system elapsed \n  0.043   0.084   0.032 \n\n\nWe see that lapply is faster. This is because there is an overhead to distributing these runs and collecting their results when using mclapply\n\n\n\nHere, I will use the foreach but without a parallel back-end - this is akin to a sequential run, just like lapply or a regular for loop\n\n\nCode\nsystem.time({\n  outputMatrix &lt;- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %do% {\n    out &lt;- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.071   0.005   0.078 \n\n\n\n\n\nHere, I will use the foreach but with a parallel back-end. The parallel back-end is a cluster of cores If you are familiar with multiprocessing in python, it is equivalent to multiprocessing.Pool\nFirst we need to register a parallel back-end\nWe can query how many cores we have on this computer\n\n\nCode\nparallel::detectCores()\n\n\n[1] 8\n\n\nI will register 7 cores\n\n\nCode\nnum_clusters &lt;- 7 #- 5 # 12 - 5\ndoParallel::registerDoParallel(num_clusters)\n\ncat('Registering', num_clusters, 'clusters for a parallel run\\n')\n\n\nRegistering 7 clusters for a parallel run\n\n\n\n\nCode\nsystem.time({\n  outputMatrix &lt;- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %dopar% {\n    out &lt;- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.085   0.070   0.081 \n\n\nCode\n# stop the cluster\ndoParallel::stopImplicitCluster()\n\n\nHere we see that lapply is much faster. Of course that is because of all the overheads and all that.\nNext, I will show to use the various makeCluster() options."
  },
  {
    "objectID": "content/blog/posts/2023-09-03-parallelization-in-R/index.html#use-case",
    "href": "content/blog/posts/2023-09-03-parallelization-in-R/index.html#use-case",
    "title": "Parallelization in R",
    "section": "",
    "text": "Assuming we want to apply a function over the rows of a matrix. This function will take each row, divide the numbers in that row by the index of that row in the matrix and return a newly-created matrix of the same shape.\n\n\nCode\nset.seed(2022)\nmyMatrix &lt;- matrix(sample(1:50000, size=300*500, replace=T), nrow=300, ncol=500)\n\ndim(myMatrix)\n\n\n[1] 300 500\n\n\n\n\nFirst, I will time this function with lapply loops. lapply is shipped with base R and is a parallel form of a regular for loop.\n\n\nCode\nlapply_fxn &lt;- function(){\n  applyMatrix &lt;- lapply(1:nrow(myMatrix), function(i){\n    out &lt;- myMatrix[i, ] / (i)\n    return(out)\n  })\n  applyMatrix &lt;- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(lapply_fxn())\n\n\n   user  system elapsed \n  0.004   0.000   0.004 \n\n\n\n\n\nNext, let’s take advantage of the cores, this time using mclapply\n\n\nCode\nmclapply_fxn &lt;- function(){\n  applyMatrix &lt;- parallel::mclapply(1:nrow(myMatrix), function(i){\n    out &lt;- myMatrix[i, ] / (i)\n    return(out)\n  }, mc.cores = 12) # using 7 cores\n  \n  applyMatrix &lt;- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(mclapply_fxn())\n\n\n   user  system elapsed \n  0.043   0.084   0.032 \n\n\nWe see that lapply is faster. This is because there is an overhead to distributing these runs and collecting their results when using mclapply\n\n\n\nHere, I will use the foreach but without a parallel back-end - this is akin to a sequential run, just like lapply or a regular for loop\n\n\nCode\nsystem.time({\n  outputMatrix &lt;- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %do% {\n    out &lt;- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.071   0.005   0.078 \n\n\n\n\n\nHere, I will use the foreach but with a parallel back-end. The parallel back-end is a cluster of cores If you are familiar with multiprocessing in python, it is equivalent to multiprocessing.Pool\nFirst we need to register a parallel back-end\nWe can query how many cores we have on this computer\n\n\nCode\nparallel::detectCores()\n\n\n[1] 8\n\n\nI will register 7 cores\n\n\nCode\nnum_clusters &lt;- 7 #- 5 # 12 - 5\ndoParallel::registerDoParallel(num_clusters)\n\ncat('Registering', num_clusters, 'clusters for a parallel run\\n')\n\n\nRegistering 7 clusters for a parallel run\n\n\n\n\nCode\nsystem.time({\n  outputMatrix &lt;- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %dopar% {\n    out &lt;- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.085   0.070   0.081 \n\n\nCode\n# stop the cluster\ndoParallel::stopImplicitCluster()\n\n\nHere we see that lapply is much faster. Of course that is because of all the overheads and all that.\nNext, I will show to use the various makeCluster() options."
  },
  {
    "objectID": "content/books/index.html",
    "href": "content/books/index.html",
    "title": "Temi",
    "section": "",
    "text": "Just so you know, I don’t necessarily endorse all of these books."
  },
  {
    "objectID": "content/books/index.html#currently-reading",
    "href": "content/books/index.html#currently-reading",
    "title": "Temi",
    "section": "currently reading",
    "text": "currently reading\n\n\nThe Second Sex by Simone de Beauvoir\n\nStarted sometime in August 2023"
  },
  {
    "objectID": "content/books/index.html#previously-read",
    "href": "content/books/index.html#previously-read",
    "title": "Temi",
    "section": "previously read",
    "text": "previously read\nthere’s more, but in no particular order:\n\nFactotum by Charles Bukowski\nIn the Country of Men by Matar Hisham\nThe Secret Lives of Baba Segi’s Wives by Lola Shoneyin\nThe God of Small Things by Arundhati Roy\nA Peace To End All Peace by David Fromkin\nA Song of Ice and Fire by George R. R. Martin\n\nA Game of Thrones\nA Clash of Kings\nA Storm of Swords\n\nThe Emperor of All Maladies by Siddhartha Mukherjee\nThen We Came To The End by Joshua Ferris\nAnimal’s People by Indra Sinha\nNever Let Me Go by Kazuo Ishiguro\nThe Buried Giant by Kazuo Ishiguro\nThe First Law by Joe Abercrombie\n\nThe Blade Itself\nBefore They Are Hanged\n\nAnansi Boys by Neil Gaiman\nFuture Popes of Ireland by Darragh Martin"
  },
  {
    "objectID": "content/assets/thoughts/index.html",
    "href": "content/assets/thoughts/index.html",
    "title": "the little things filling up my head",
    "section": "",
    "text": "maybe one day, I’ll have the time to write about these things\n\n\n\n\n\n\n\n  \n    about AI\n    \n  \n  \n    genetics, pharmacogenetics, machine learning, and statistical methods\n    \n  \n  \n    climate change + recycling\n    \n  \n  \n    rap, as in the music genre\n    \n  \n  \n    what to do after a PhD\n    \n  \n  \n    becoming a good-enough chef + next DIY projects\n    \n  \n  \n    collecting/making my own bracelets\n    \n  \n  \n    football, or soccer, if you are North American\n    \n  \n  \n    why there is so much kickback against Oxford commas\n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Temi",
    "section": "",
    "text": "My name is Temi, and I’m a PhD student in the Genetics, Genomics and Systems Biology (GGSB) program at The University of Chicago. Prior to this, I completed a master’s at The University of North Dakota, supervised by the awesome Junguk Hur."
  },
  {
    "objectID": "index.html#currently",
    "href": "index.html#currently",
    "title": "Temi",
    "section": "currently",
    "text": "currently\nIn fall 2022, I joined Haky’s lab for my PhD dissertation. Here, I will be studying variations in the human epigenome (at the genomic level and the population level), how these variations influence molecular phenotypes (e.g. TF binding), and how these, in turn, influence complex traits and diseases. These are complex/interesting questions. Wish me luck!\n\ncv\nresume\npublications"
  },
  {
    "objectID": "index.html#thinking-about",
    "href": "index.html#thinking-about",
    "title": "Temi",
    "section": "thinking about",
    "text": "thinking about\nHere’s a more complete list\n\ngenetics, pharmacogenetics, machine learning, and statistical methods\nclimate change + recycling\nrap, as in the music genre\nwhat to do after this PhD\nnext random things to learn [currently doing Origami]\nbecoming a good-enough cook + improving my basic knife skills [they are very poor right now]\nnext DIY projects + Jugaaḍing when I get the opportunity\ncollecting/making my own bracelets\nfootball, or soccer, if you are North American\nwhy there is so much kickback against Oxford commas [even the AP style recommended not using them until very recently]"
  },
  {
    "objectID": "index.html#other-lives",
    "href": "index.html#other-lives",
    "title": "Temi",
    "section": "other lives",
    "text": "other lives\nOften, I am in the gym doing bodyweight workouts. Sometimes, I’ll read books + watch TV shows or movies. Other times, I’ll practice playing the guitar, play pick-up football, and do some DIYs for fun.\nAnd if I have enough time left, I’ll stare outside the window for a bit, or go out and touch grass."
  },
  {
    "objectID": "index.html#links-reach-out",
    "href": "index.html#links-reach-out",
    "title": "Temi",
    "section": "links + reach out",
    "text": "links + reach out\n\nYou can reach me by filling this form; or\nSend me an email at &gt;“tadepete at gmail dot com”; or\nLinkedIn; or \nGithub"
  },
  {
    "objectID": "content/assets/publications/index.html",
    "href": "content/assets/publications/index.html",
    "title": "publications",
    "section": "",
    "text": "“In academia, your currency is publications.” :(\nI would rather the currency be dollars but anyway…\n\n\n\n\n\n\n  \n    \n      \n        Machine Learning-Based Predictive Modeling of Postpartum Depression\n        Dayeon Shin, Kyung Ju Lee, Temidayo Adeluwa, Junguk Hur\n        2020 | Journal of Clinical Medicine\n      \n    \n  \n\n\n  \n    \n      \n        Novel Action of Vinpocetine in the Prevention of Paraquat-Induced Parkinsonism in Mice: Involvement of Oxidative Stress and Neuroinflammation\n        Ismaila Ishola, A.A. Akinyede, T.P. Adeluwa, C. Micah\n        2018 | Metabolic Brain Disease\n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "content/resources/index.html",
    "href": "content/resources/index.html",
    "title": "Some resources I like running to for help",
    "section": "",
    "text": "The pdf download links are links to my DropBox; and if they don’t work, you find the files using the other links.\n\n\nMachine learning\n\nVicky Boykis’ What are embeddings? pdf download link | Vicky’s website\nDive Into Deep Learning d2l’s website\n\n\n\nGenetics and statistical genetics\n\nMatti Pirinen’s GWAS course materials pdf download link | course materials webpage\nAn Introduction to Statistical Genetic Data Analysis pdf download link\n\n\n\nMath, calculus, and statistics\n\nCALCULUS for Biology and Medicine pdf download link\nAn Introduction to Statistical Learning pdf download link\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/blog/index.html",
    "href": "content/blog/index.html",
    "title": "codes + notes + projects",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nSep 7, 2023\n\n\nNotes on dataloaders\n\n\n\n\nSep 6, 2023\n\n\nUnderstanding pvalues, multiple testing and qvalues\n\n\n\n\nSep 3, 2023\n\n\nParallelization in R\n\n\n\n\nJul 18, 2022\n\n\nPredicting swarm behaviour\n\n\n\n\nMay 13, 2022\n\n\nSimulating discrete and continuous populations\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/blog/posts/2023-09-06-about-dataloaders/index.html",
    "href": "content/blog/posts/2023-09-06-about-dataloaders/index.html",
    "title": "Notes on dataloaders",
    "section": "",
    "text": "Note\n\n\n\nThis post is still under construction; I am adding sutff as I get the time to.\n\n\n\n\nCode\nimport torch\nimport numpy as np, os, sys, pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\nprint(f'Kernel used is: {os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))}')\n\n\nKernel used is: dl-tools\n\n\n\nIntroduction\nWhen training deep learning models (or any machine learning model for that matter), we try to make the most of available data. One way we do this is to supply a batch of the data to the model at a training iteration. So, if you have 100 observations to train on, you can supply, say, 20 at a time.\nIn addition, loading 100 observations or more at a time may consume a lot of memory, especially if you have limited resources. Instead, we supply mini-batches of the data enough to be held in memory alongside the model during each training iteration.\npytorch gives us a convenient way to load data in this manner by letting us create our own dataset objects, which are used by pytorch’s dataloader\n\n\nClass Dataset\nFirst you import the Dataset and Dataloader classes from pytorch\n\n\nCode\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\n\nI will create a fictitious dataset, observations X and ground truth Y. They will be numpy arrays; this way I can easily manipulate them.\n\n\nCode\nX = np.random.rand(100, 48) \nY = np.random.choice([0,1], size=100)\nX.shape, Y.shape\n\n\n((100, 48), (100,))\n\n\nThe trick to creating your Dataset object is that when you call the class, or attempt to get an item from the dataset, it should return one training observation. Three methods of that class are needed:\n\nthe __init__(): which will initialize your object\nthe __len__(): to be used by the Dataloader to figure out how many training observations in total you have; will be used for shuffling and indexing necessary observations etc.\nthe __getitem__(): used to return a single training unit at a time.\n\n\nNote that in the __getitem__(), I said “training unit”. Ideally, you want to return one training observation at a time but depending on your scenario/problem/data, you may want to return more than one observation at a time. Point is, whatever number of observations you get after calling _getitem__() will be your training unit.\n\nI create a dataset class, MyDataset, that will inherit from pytorch’s Dataset, and return just one observation and ground truth at a time\n\n\nCode\nclass MyDataset(Dataset): # will inherit from the Dataset object\n    def __init__(self, X, Y):\n        self.X = X\n        self.Y = Y\n    \n    def __len__(self): # the dataloader needs to know the number of observations you have\n        return self.X.shape[0]\n\n    def __getitem__(self, idx): # this is what returns just one observation or one unit of training\n        return(self.X[idx, : ], self.Y[idx]) # essentially, I am just slicing the np array\n\n\nNow I can use the dataloader object\n\n\nCode\nmydataset = MyDataset(X, Y)\nmydataset\n\n\n&lt;__main__.MyDataset at 0x7fa39bb2da50&gt;\n\n\nYou can confirm that the dataset object works by doing this. I give it an index, 8 and it pulls the observations and ground truth corresponding to that index.\n\n\nCode\nmydataset.__getitem__(8)\n\n\n(array([1.43263063e-01, 8.19159823e-01, 9.66615361e-01, 3.04914935e-02,\n        6.74951172e-01, 2.28270871e-01, 5.07899833e-01, 7.54816753e-01,\n        8.50174001e-01, 5.85532967e-01, 2.13319662e-01, 3.74500070e-02,\n        8.69480679e-01, 9.91958073e-01, 1.06552389e-01, 6.75307504e-01,\n        4.64268091e-01, 3.97405622e-02, 3.63357637e-01, 8.51468424e-01,\n        7.07647608e-01, 3.59670787e-04, 3.27379319e-01, 1.23819926e-01,\n        6.51143229e-01, 3.65572306e-01, 8.11721461e-01, 8.81402757e-02,\n        1.46144989e-01, 7.60215261e-01, 7.05400679e-01, 6.96563049e-01,\n        8.31366812e-01, 3.80790558e-01, 9.90544126e-01, 9.84286220e-01,\n        3.50894274e-01, 5.80318077e-01, 8.59732277e-01, 7.51747094e-01,\n        3.34853644e-02, 1.76530280e-01, 4.94703167e-01, 7.28400713e-01,\n        3.35355319e-01, 2.15013442e-01, 5.02317757e-01, 2.88868790e-01]),\n 1)\n\n\nI can also check the number of observations I have\n\n\nCode\nmydataset.__len__()\n\n\n100\n\n\n\n\nClass Dataloader\nAll well and good. But I don’t want to give my model one observation at a time. Although people do this, it is too small. Instead, I want to give the model a certain batch at time. Dataloaders help with this. The Dataloader class abstracts over the Dataset class to help us shuffle the data, yield batch_size number of observations at a time, and even make use of parallel backends to yield the data faster, especially when the dataset may be large.\nI create a DataLoader object and supply it the argument batch_size. Whenever I ask the object for training examples, it gives me batch_size number of observations at a time. Here I will set batch_size to 20. Remember that I have 100 observations in total. A batch_size of 20 will yield 20 observations at a time without replacement until all 100 are exhausted and this will be 5 different batches.\nIn addition, I can shuffle the loading of the batches by setting shuffle=True. If you have shuffled your data before now, and intend to keep that, you should probable set this argument to False.\n\n\nCode\nmydataloader = DataLoader(mydataset, batch_size=20, shuffle=True)\n\n\nNow, for each batch, I will print out the total number of observations and ground truth.\n\n\nCode\nfor i, batch in enumerate(mydataloader):\n    print(f'batch {i}: number of observations and ground truth are {batch[0].shape[0]} and {batch[1].shape[0]} respectively')\n\n\nbatch 0: number of observations and ground truth are 20 and 20 respectively\nbatch 1: number of observations and ground truth are 20 and 20 respectively\nbatch 2: number of observations and ground truth are 20 and 20 respectively\nbatch 3: number of observations and ground truth are 20 and 20 respectively\nbatch 4: number of observations and ground truth are 20 and 20 respectively"
  },
  {
    "objectID": "content/blog/posts/2023-09-06-understanding-qvalues/index.html",
    "href": "content/blog/posts/2023-09-06-understanding-qvalues/index.html",
    "title": "Understanding pvalues, multiple testing and qvalues",
    "section": "",
    "text": "Note\n\n\n\nThis post is still under construction; I am adding sutff as I get the time to.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nStorey and Tibshirani’s paper is a good place to start\nHaky’s notes here are also very helpful; and this notebook was partly inspired by her notes.\n\n\n\n\nIntroduction\nWhen doing multiple tests, there is a need to control the false positive rate (fpr) because by nature of p-values, if there is nothing interesting going on, you still have an alpha % chance of detecting something, and misclassifying that.\n\n\nSimulation 1: Null and alternative effects.\nI have a simple linear function here, where \\(X\\) has some effect, \\(\\beta\\) on \\(Y\\).\n\\[\nY \\approx \\sum X\\beta + \\epsilon\n\\]\nwhere,\n\\[\nX \\approx \\mathcal{N}(0.2,1)\\\n\\]\n\\[\n\\epsilon \\approx \\mathcal{N}(0,0.1)\n\\]\n\n\nCode\nlibrary(qvalue)\n\n\n\n\nCode\ndevtools::source_gist('https://gist.github.com/TemiPete/d7e37272964e5f00af4efea01d295dc8')\n\n\nℹ Sourcing gist \"d7e37272964e5f00af4efea01d295dc8\"\nℹ SHA-1 hash of file is \"d308c0c2f4eb3d58cff6b52ad22538f09bd136e0\"\n\n\n\n\nCode\nset.seed(2023)\nnobserv &lt;- 2000 # number of observations\n\n\n\n\nCode\ntrue_mean &lt;- 0.2\ntrue_sd &lt;- 1\neps_mean &lt;- 0\neps_sd &lt;- 0.5\nbeta &lt;- 0.6\nx &lt;- rnorm(n=nobserv, mean=true_mean, sd=true_sd)\ne &lt;- rnorm(n=nobserv, mean = eps_mean, sd=eps_sd)\n\nyalt &lt;- x * beta + e\nplot(x, yalt, main='x has an effect on y', frame.plot=F)\n\n\n\n\n\n\n\nCode\nynull &lt;- rnorm(n=nobserv, mean=0, sd=beta)\nplot(x, ynull, main='x has no effect on y', frame.plot=F)\n\n\n\n\n\nNow I can simulate these tests multiple times, say, 10000\n\n\nCode\nntests &lt;- 10000\nX &lt;- matrix(rnorm(n=nobserv*ntests, mean=true_mean, sd=true_sd), nrow=nobserv, ncol=ntests)\nepsilon &lt;- matrix(rnorm(n=nobserv*ntests, mean=eps_mean, sd=eps_sd), nrow=nobserv, ncol=ntests)\ndim(X) ; dim(epsilon)\n\n\n[1]  2000 10000\n\n\n[1]  2000 10000\n\n\nCode\nYalt &lt;- X * beta + epsilon\nYnull &lt;- matrix(rnorm(n=nobserv, mean=0, sd=beta), nrow=nobserv, ncol=ntests)\ndim(Yalt) ; dim(Ynull)\n\n\n[1]  2000 10000\n\n\n[1]  2000 10000\n\n\n\n\nCode\npvec = rep(NA,ntests)\nbvec = rep(NA,ntests)\n\nfor(ss in 1:ntests)\n{\n  fit = fastlm(X[,ss], Ynull[,ss])\n  pvec[ss] = fit$pval  \n  bvec[ss] = fit$betahat\n}\n\nsummary(pvec)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000291 0.2501638 0.4916683 0.4972860 0.7465493 0.9999487 \n\n\n\n\nCode\nsum(pvec &lt; 0.05) ; mean(pvec &lt; 0.05)\n\n\n[1] 481\n\n\n[1] 0.0481\n\n\nEven under the null, we find that 5% of our tests are false positives! In real life, we would think these are true effects, which is not good.\nSo, we try to control this false positive rate. There are many methods, but we can use the Bonferroni approach\n\n\nCode\ncutoff &lt;- 0.05/length(pvec)\nsum(pvec &lt; cutoff) ; mean(pvec &lt; cutoff)\n\n\n[1] 0\n\n\n[1] 0\n\n\nWith Bonferroni correction, we see that all of the tests are null, which is what should have happened in the first place. Anyway, all that was for simulation sake. I should create a set of tests, where some proportion are under the alternative i.e. true, and the rest are not i.e. null\n\n\nSimulation 2: A mixture of outcomes under the null and alternative hypothesis.\n\n\nCode\nptrue &lt;- 0.2 # only 20% of the tests are TRUE\nwtrue &lt;- sample(x=c(0,1), size=ntests, replace=TRUE, prob=c(1-0.2, 0.2))\ntable(wtrue) |&gt; prop.table()\n\n\nwtrue\n     0      1 \n0.8046 0.1954 \n\n\nI will look through wtrue. If 0, I will select the ynull at that index, otherwise, I will select the yalt\n\n\nCode\nYboth &lt;- matrix(NA, nrow=nobserv, ncol=ntests)\nfor(i in seq_along(wtrue)){\n    if(wtrue[i] == 1){\n        Yboth[, i] &lt;- Yalt[, i]\n    } else {\n        Yboth[, i] &lt;- Ynull[, i]\n    }\n}\n\ndim(Yboth)\n\n\n[1]  2000 10000\n\n\n\n\nCode\n## run linear regression for all 10000 phenotypes in the mix of true and false associations, Ymat_mix\npvec_mix = rep(NA,ntests)\nbvec_mix = rep(NA,ntests)\nfor(ss in 1:ntests){\n  fit = fastlm(X[,ss], Yboth[,ss])\n  pvec_mix[ss] = fit$pval  \n  bvec_mix[ss] = fit$betahat\n}\nsummary(pvec_mix)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0678  0.3792  0.3992  0.6825  0.9999 \n\n\n\n\nCode\nsum(pvec_mix &lt; 0.05) ; mean(pvec_mix &lt; 0.05)\n\n\n[1] 2339\n\n\n[1] 0.2339\n\n\n\n\nCode\nhist(pvec_mix, main='')\nmtext('A simulation under the null + alt', side=3, line=1, adj = 0)\n\n\n\n\n\n\n\nQuick detour: fpr, fdr, pfdr and such\nWe expect 500 to be significant under the null, but we get 2339.\nSince we have more than what we expected under the null, we can assume that the remainder are gotten under the alternative. We can estimate this true discovery rate\n\\[\n\\frac{(nobserved - nexpected)}{nobserved}\n\\]\n\n\nCode\ntdr &lt;- ((sum(pvec_mix &lt; 0.05)) - (0.05*ntests))/(sum(pvec_mix &lt; 0.05))\ntdr \n\n\n[1] 0.7862334\n\n\nThe false discovery rate is 1 - tdr, which in this case is 0.2137666.\nAll well and good, except that our tdr here is higher than we expect. Instead we can estimate the positive false discovery rate or pFDR. Here’s how I explain this: Given that you have found a number of tests to be significant, let’s call this tsig, and we expect at least one of these to be positive i.e. under the alternative, what is the expected number of false positives? i.e. what proportion are not true but we come out as true.\nTo break this down a little, I will start from here: Assuming you do a test to classify if a a group of people eat fruits or not, and you have this table after.\n\n\n\n\n\n\n\n\n\n\neats fruits\ndoes not eat fruits\n\n\n\n\n\nclassified: eat fruits\ntrue positives\nfalse positives\n\n\n\nclassified: does not eat fruits\nfalse negatives\ntrue negatives\n\n\n\n\n\n\n\n\n\n\nThe fpr, as mentioned earlier is:\n\\[\n\\frac{false\\ positives}{(false\\ positives\\ +\\ true\\ negatives)}\n\\]\ni.e. of all the people who don’t eat fruits, how many of them do we classify to eat fruits based on our tests?\nThe fdr then is, of all the people who we classify as eating fruits, how many of them don’t actually eat fruits?\n\\[\n\\frac{false\\ positives}{(true\\ positives\\ +\\ false\\ positives)}\n\\]\nUsing the table + idea above, I can then\n\n\n\n\n\n\n\n\n\n\neats fruits\ndoes not eat fruits\n\n\n\n\n\nclassified: eat fruits\ntrue positives\nfalse positives\n\n\n\nclassified: does not eat fruits\nfalse negatives\ntrue negatives\n\n\n\n\n\n\n\n\n\n\nBecause we simulated the data, we know that 1954 tests are under the alternative, and the rest, 8046, should be under the null. But after our tests, we have found 2339 to be under the alternative and 7661 to be under the null. So, there are some 385 that have been misclassified as under the alternative when they are not, and some -385\n\n\nCode\ntp &lt;- sum(wtrue == 1 & pvec_mix &lt; 0.05)\ntn &lt;- sum(wtrue == 0 & pvec_mix &gt;= 0.05)\nfp &lt;- sum(wtrue == 1 & pvec_mix &gt;= 0.05)\nfn &lt;- sum(wtrue == 0 & pvec_mix &lt; 0.05)\n\n\nAlternatively, table(pvec_mix &gt; 0.05, wtrue) will yield the same result.\n\n\n\n\nalternative\nnull\ntotal\n\n\n\n\nclassified: alternative\n1954\n385\n2339\n\n\nclassified: null\n0\n7661\n7661\n\n\ntotal\n1954\n8046\n\n\n\n\nWith this, we can estimate the fpr to be 0 and fdr to be 0.\nOkay. Back to pfdr. Remember that I described this earlier, saying:\n\n“Given that you have found a number of tests to be significant, let’s call this tsig, and we expect at least one of these to be positive i.e. under the alternative, what is the expected number of false positives? i.e. what proportion are not true but we come out as true.”"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Temi",
    "section": "",
    "text": "hey"
  }
]