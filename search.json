[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "content/blog/posts/2023-08-29-simulating-populations/index.html",
    "href": "content/blog/posts/2023-08-29-simulating-populations/index.html",
    "title": "Simulating discrete and continuous populations",
    "section": "",
    "text": "Code\nlibrary(gtools) # a library\n\n\n\nIntroduction\nOftentimes, we want to simulate populations in order to do some analysis. In this demo, I go over a, perhaps, crude way to mimic discrete and continuous populations. Before simulating, we need to know what a population is. In genetics, a population is a grouping of individuals based on some similarity in allele frequencies between them. Of course, there are many other definitions of what a population is. In this demo, I will use the definition based on allele frequencies.\nOne way to generate allele frequencies that mimic that seen in typical populations is to use the \\(F_{ST}\\), AKA the fixation index, AKA F-statistic. \\(F_{ST}\\) measures population differentiation as a result of genetic structure. Values range between 0 and 1, and higher values mean that two populations are highly similar. There are many definitions of \\(F_{ST}\\), depending on what you are discussing - and I am still learning about most of them. In this case, however, I am primarily concerned with allele frequency-related definitions.\n\n\nSimulating discrete populations\nTo simulate populations based on the definition of \\(F_{ST}\\), one thing we can do is to simulate an ancestral population’s allele frequencies. This is similar to having a founder population from which every other population descends. Afterwards, we can simulate different populations from this ancestral population. This follows from the Balding-Nichols model.\n\\[\\begin{align}\nBeta(\\frac{1-F}{F}p, \\frac{1-F}{F}(1-p))\n\\end{align}\\]\nAssume that we have 500 alleles/loci, and 4 populations, and we intend to simulate 1000 individuals from each of these populations. We can define our \\(F_{ST}\\) to be, say, 0.09 between populations 1 and 2, 0.19 between populations 1 and 2, and population 3, and 0.4 between all these populations, and population 4. Therefore, populations 1 and 2 will be closely related, and different from population 3, and all of them will be different from population 4.\n\n\nCode\nM &lt;- 500 # number of alleles or SNPs\nn_pops &lt;- 4 # number of populations\nN &lt;- 1000 # 20 individuals in each n_pops\nf_pop12 &lt;- 0.09\nf_pop3 &lt;- 0.19 # a third, distant, unrelated population\nf_pop4 &lt;- 0.4 # a fourth population\n\n\nWhen we make plots of the allele frequencies of these populations, we expect high correlations between populations 1 and 2, and not-so-high correlations when we compare with the other populations 3 and 4.\nWe can generate some random minor allele frequencies (MAFs) to be the ancestral allele frequencies, and generate independent draws from the distribution of the ancestral allele frequencies, based on the equation above.\n\n\nCode\nancestral_allele_freqs &lt;- runif(M, 0.01, 0.5)\nsh1 &lt;- ((1-f_pop12)/f_pop12)*ancestral_allele_freqs\nsh2 &lt;- ((1-f_pop12)/(f_pop12))*(1-ancestral_allele_freqs)\npop1_allele_freqs &lt;- rbeta(M, shape1 = sh1, shape2 = sh2)\npop2_allele_freqs &lt;- rbeta(M, shape1 = sh1, shape2 = sh2)\npop3_allele_freqs &lt;- rbeta(M, shape1 = ((1-f_pop3)/f_pop3)*ancestral_allele_freqs, shape2=((1-f_pop3)/(f_pop3))*(1-ancestral_allele_freqs))\npop4_allele_freqs &lt;- rbeta(M, shape1 = ((1-f_pop4)/f_pop4)*ancestral_allele_freqs, shape2=((1-f_pop4)/(f_pop4))*(1-ancestral_allele_freqs))\n\nall_allele_freqs &lt;- list(pop1_allele_freqs, pop2_allele_freqs, pop3_allele_freqs, pop4_allele_freqs)\nnames(all_allele_freqs) &lt;- paste0('population_', 1:4)\n\n\n\nFirst, how does the expected value of the distribution of allele frequency changes as a factor of the \\(F_{ST}\\)? This should give us an idea of what to expect when we simulate allele frequencies.\n\n\nCode\n# generate many Fst values\nfst_vector &lt;- round(seq(0.01, 0.99, length.out=20), 2) # 20 fsts ranging from 0.01 to 0.99\nper_fst &lt;- lapply(fst_vector, function(each_fst){\n  \n  sh1 &lt;- ((1-each_fst)/each_fst)*ancestral_allele_freqs\n  sh2 &lt;- ((1-each_fst)/(each_fst))*(1-ancestral_allele_freqs)\n  temp_allele_frq &lt;- rbeta(M, shape1 = sh1, shape2 = sh2)\n  deviation &lt;- cor(ancestral_allele_freqs, temp_allele_frq) #|&gt; suppressWarnings()\n  return(deviation)\n})\n\n\n\n\nCode\ncor_deviation &lt;- per_fst |&gt; unlist()\n\nplot(cor_deviation, frame.plot=F, xaxt='n', yaxt='n', type='b', xlab = expression(F[ST]), ylab=expression(R^2))\naxis(1, at=rep(1:length(cor_deviation), by=1), labels = fst_vector, las=2)\naxis(2, at=seq(0, 1, 0.1))\n\n\n\n\n\nWe see that as the \\(F_{ST}\\) increases, the correlation between the ancestral allele frequency and the generated allele frequency reduces.\n\n\nCode\nlayout(matrix(c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = T))\nnames_pop &lt;- names(all_allele_freqs)\nfor(i in 1:4){\n    for(j in 1:4){\n        if(i == j){ # we don't want to plot pop1 vs pop1 e.t.c.\n            next\n        } else if (i &lt; j) {\n            #cor_ &lt;- round(cor(all_allele_freqs[[i]], all_allele_freqs[[j]]), 2)\n            plot(all_allele_freqs[[i]], all_allele_freqs[[j]], xlab = names_pop[i], ylab = names_pop[j], frame.plot = F)\n            #abline(a=0, b=cor_, col='red')\n            #mtext(cor_)\n        }\n    }\n}\nmtext('Scatterplot of allele frequencies for each population', outer = T)\n\n\n\n\n\nThe plot is approximately (roughly?) what we expect!\n\n\n\nSimulating genotypes using these allele frequencies\nGiven these allele frequencies, we can simulate genotypes from each of these populations by sampling from a binomial distribution.\n\\[\\begin{align}\nG_{k_{n}} \\sim Binom(n, p_{k})\n\\end{align}\\]\nwhere \\(n\\) is the number of success, and \\(p\\) is the probability of a success. Here, \\(n = 2\\) because we want 2 alleles per loci, and \\(p\\) is the allele frequency at that loci.\n\n\nCode\ngenotypes &lt;- lapply(seq_along(all_allele_freqs), function(i){\n    matrix(data=rbinom(n=N*M, size=2, prob = all_allele_freqs[[i]]), nrow = N, ncol = M, byrow = T)\n})\n\ngenotypes &lt;- do.call(rbind, genotypes)\nshuffle &lt;- sample(1:nrow(genotypes)) # I will shuffle the data\ngenotypes &lt;- genotypes[shuffle, ]\npops &lt;- rep(names(all_allele_freqs), each=N)\npops &lt;- pops[shuffle] # shuffle pops too, since it tells me what populations the genotypes come from\n\ncat('Here is what the genotypes look like\\n')\n\n\nHere is what the genotypes look like\n\n\nCode\ngenotypes[1:5, 1:5]\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    1    0    1    0    0\n[3,]    0    1    0    0    0\n[4,]    1    0    0    0    0\n[5,]    0    0    0    0    0\n\n\nCode\ncat('And here is what the population designation looks like \\n')\n\n\nAnd here is what the population designation looks like \n\n\nCode\npops[1:5]\n\n\n[1] \"population_1\" \"population_3\" \"population_2\" \"population_3\" \"population_4\"\n\n\nNext, we can calculate the principal components (PCs) of this data, and plot PC1 vs PC2, to see where the most variability lies in the data\n\n\nCode\n# first change the row names of the genotype data to reflect the populations\n#rownames(genotypes) &lt;- pops\n# I need to make sure that there are some variations in the SNPS - if there aren't any, I should remove them\npca_genotypes &lt;- genotypes[ , which(apply(genotypes, 2, var) != 0)]\n# PCA\npca_dt &lt;- prcomp(pca_genotypes, scale.=T, center=T)\npca_dt$x[1:5, 1:5]\n\n\n            PC1       PC2        PC3         PC4        PC5\n[1,]   3.263826 -7.849732 -6.9217690 -2.66479261 -3.9900565\n[2,]   6.544296  8.367773 -1.7339642 -0.06842935  2.8759937\n[3,]   6.086323 -4.769098  6.4930496 -1.81845701 -1.7244237\n[4,]   5.730047  7.073629 -1.7772414 -1.97970506 -0.2309777\n[5,] -12.399378  1.580702  0.6451658  0.24943503  1.3446776\n\n\n\n\nCode\n# colors\nmycolors &lt;- colorRampPalette(RColorBrewer::brewer.pal(8, \"Dark2\"))(n_pops)\n#mycolors &lt;- c('red', 'orange', 'green', 'blue')\ncolor_coding &lt;- factor(pops, labels = mycolors)\nplot(pca_dt$x[, 'PC1'], pca_dt$x[, 'PC2'], bg=as.character(color_coding), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of 4 discrete populations')\nlegend('topleft', legend=names_pop, pch=c(21, 21, 21, 21), pt.bg=mycolors, bty = 'n', xpd=NA)\n\n\n\n\n\nWe can look at populations 1 and 2 only, and see if there is any difference between them.\n\n\nCode\npop_names &lt;- c('population_1', 'population_2')\nidx &lt;- which(pops %in% pop_names)\npca_12 &lt;- pca_dt$x[idx, ]\ncolor_coding &lt;- factor(pops[idx], labels = mycolors[1:2])\nplot(pca_12[, 'PC1'], pca_12[, 'PC2'], bg=as.character(color_coding), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of the 2 similar populations 1 and 2')\nlegend('topleft', legend=pop_names, pch=c(21, 21), pt.bg=mycolors[1:2], bty = 'n', xpd=NA)\n\n\n\n\n\n\n\nSimulating continuous populations\nWhat I have done so far, is to simulate a discrete population of individuals. Think of it as people living in Mars, Earth, and Jupiter, and who have never intermingled since time immemorial. Population 3 is faraway from population 4. However, true human populations are not this way. How can we simulate a truly continuous population?\nFirst, notice that I am only simulating allele frequencies. Therefore, I can put a prior on my earlier-defined beta distribution to model the probability that an individual is from population \\(m\\). In other words, I am trying to estimate what percentage of an individual’s alleles might have come from a certain population \\(m\\). I will call this random variable, \\(Q\\).\n\\[\\begin{align}\nQ \\sim Dirichlet(\\alpha)\n\\end{align}\\]\nA trick I can use is to take an individual, ask how much of their genome is shared among each population I have, select that proportion of allele frequencies, and simulate their genotypes based on the selection for each population. To do this, I need to know how to distribute the \\(\\alpha\\) parameter used in the Dirichlet distribution. A trick I can use is to use the allele frequency distribution in the entire population. I will call this distribution the population grade distribution.\nEarlier, I simulated some allele frequencies all_allele_freqs\n\n\nCode\nss &lt;- sapply(all_allele_freqs, sum)\npopulation_grade &lt;- ss/sum(ss)\n#population_grade &lt;- rand_vect_cont(4, 1)\npopulation_grade\n\n\npopulation_1 population_2 population_3 population_4 \n   0.2462436    0.2509962    0.2512640    0.2514962 \n\n\nThen I can use this as a parameter to the Dirichlet distribution to simulate the percentage of an individual’s genotypes that is shared across all 4 populations.\n\n\nCode\nq_matrix &lt;- gtools::rdirichlet(N * n_pops, alpha = population_grade) # I want to simulate, N individuals per population\n\n\nHere is an illustration. Assuming that I am looking at the first individual\n\n\nCode\n# assuming one individual\nid1 &lt;- q_matrix[1, ]\nq_counts &lt;- floor(id1 * M) # number of loci shared\nq_counts\n\n\n[1]   0  43 340 116\n\n\nThe assumption here is that this individual shares 0 loci with population 1, 43 with population 2, 340 with population 3, and 116 loci with population 4, and 3 independent loci not share with any population. The sum is 499 (because I rounded down), but I don’t want this to be this case. Therefore, I will share the remaining loci with the highest number already available, so that the sum is 500.\n\n\nCode\nremainder_ &lt;- M - sum(q_counts)\nq_counts[which.max(q_counts)] &lt;- q_counts[which.max(q_counts)] + remainder_\nq_counts\n\n\n[1]   0  43 341 116\n\n\nNow, this sums to 500.\nPutting this into a reusable function…\n\n\nCode\nq_counts_fxn &lt;- function(q_matrix, M){\n    apply(q_matrix, 1, function(each_row){\n        temp &lt;- floor(each_row * M)\n        remainder_ &lt;- M - sum(temp)\n        temp[which.max(temp)] &lt;- temp[which.max(temp)] + remainder_\n        temp\n    }) |&gt; t()\n}\n\nq_counts &lt;- q_counts_fxn(q_matrix, M)\nq_counts[1:5, ] # for the first 5 individuals\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    0   43  341  116\n[2,]  385    0   43   72\n[3,]   10   16  364  110\n[4,]    0  370   12  118\n[5,]  103  256    0  141\n\n\n\n\nCode\ngenotypes &lt;- matrix(NA, nrow = N*n_pops, ncol = M) # remember, N individuals per population\n\nfor(q in 1:nrow(q_counts)){\n    pop_down &lt;- 1\n    m_loci &lt;- 1:M\n    i_loci &lt;- list() # this list contains the actual loci an individual has shared among the other populations\n    while(pop_down &lt;= ncol(q_counts)){\n        # sample pop 1 loci\n        pop_loci &lt;- sample(m_loci, size=q_counts[q, pop_down], replace = F)\n        if(length(pop_loci) == 0){\n            m_loci &lt;- m_loci[m_loci != 0]\n            i_loci[[pop_down]] &lt;- 0\n        } else {\n            m_loci &lt;- m_loci[!m_loci %in% pop_loci]\n            i_loci[[pop_down]] &lt;- pop_loci\n        }\n        pop_down &lt;- pop_down + 1\n    }\n    \n    g_loci &lt;- lapply(seq_along(i_loci), function(i){\n        temp &lt;- all_allele_freqs[[i]][i_loci[[i]]]\n        rbinom(n = length(temp), size = 2, prob = temp)\n    })\n\n    gi &lt;- vector(mode='numeric', length = M)\n    for(i in 1:n_pops){\n        gi[i_loci[[i]]] &lt;- g_loci[[i]]\n    }\n    \n    genotypes[q, ] &lt;- gi\n    \n}\n\n\n\n\nCode\npop &lt;- paste0('population_', apply(q_counts, 1, which.max))\n# first change the row names of the genotype data to reflect the populations\nrownames(genotypes) &lt;- pop\n# I need to make sure that there are some variations in the SNPS - if there aren't any, I should remove them\npca_genotypes &lt;- genotypes[ , which(apply(genotypes, 2, var) != 0)]\n# PCA\npca_dt &lt;- prcomp(pca_genotypes, scale.=T, center=T)\npca_dt$x[1:5, 1:5]\n\n\n                    PC1       PC2         PC3        PC4        PC5\npopulation_3  1.7973144 -5.028205 -0.35698361  2.6108432 -0.5580345\npopulation_1 -0.4990041  4.185811 -5.96843755  0.1924663  0.4836813\npopulation_3  1.9174774 -5.916136  0.09453596  1.5513878 -0.3693294\npopulation_2  1.6417866  2.424741  6.31743092 -1.8844746 -0.3070795\npopulation_2 -0.6605363  3.049585  0.82280376  1.9657889  0.9151204\n\n\n\n\nCode\nmycolors &lt;- colorRampPalette(RColorBrewer::brewer.pal(8, \"Dark2\"))(n_pops)\ncolor_coding &lt;- factor(rownames(pca_dt$x), labels = mycolors)\nplot(pca_dt$x[, 'PC1'], pca_dt$x[, 'PC2'], bg=color_coding |&gt; as.character(), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of 4 continuous populations')\nlegend('topleft', legend=names_pop, pch=c(21, 21, 21, 21), pt.bg=mycolors, bty = 'n', xpd = NA)"
  },
  {
    "objectID": "content/blog/index.html",
    "href": "content/blog/index.html",
    "title": "things I feel like sharing",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nSep 3, 2023\n\n\nParallelization in R\n\n\n\n\nJul 18, 2022\n\n\nPredicting swarm behaviour\n\n\n\n\nMay 13, 2022\n\n\nSimulating discrete and continuous populations\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/assets/publications/index.html",
    "href": "content/assets/publications/index.html",
    "title": "publications",
    "section": "",
    "text": "“In academia, your currency is publications.” :(\nI would rather the currency be dollars but anyway…\n\n\n\n\n\n\n\n\n  \n    \n      \n        Machine Learning-Based Predictive Modeling of Postpartum Depression\n        Dayeon Shin, Kyung Ju Lee, Temidayo Adeluwa, Junguk Hur\n        2020 | Journal of Clinical Medicine\n      \n    \n  \n\n\n  \n    \n      \n        Novel Action of Vinpocetine in the Prevention of Paraquat-Induced Parkinsonism in Mice: Involvement of Oxidative Stress and Neuroinflammation\n        Ismaila Ishola, A.A. Akinyede, T.P. Adeluwa, C. Micah\n        2018 | Metabolic Brain Disease\n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Temi",
    "section": "",
    "text": "My name is Temi, and I’m a PhD student in the Genetics, Genomics and Systems biology (GGSB) program at the University of Chicago. Prior to this, I completed a master’s at the University of North Dakota, supervised by the awesome Junguk Hur."
  },
  {
    "objectID": "index.html#thinking-about",
    "href": "index.html#thinking-about",
    "title": "Temi",
    "section": "thinking about",
    "text": "thinking about\n\ngenetics, pharmacogenetics, machine learning, and statistical methods\nclimate change + recycling\nrap, as in the music genre\nwhat to do after a PhD\nbecoming a good-enough chef + next DIY projects\nfootball, or soccer, if you are North American\nwhy there is so much kickback against Oxford commas"
  },
  {
    "objectID": "index.html#currently",
    "href": "index.html#currently",
    "title": "Temi",
    "section": "currently",
    "text": "currently\nIn Fall 2022, I joined Haky’s lab for my PhD thesis. Here, I will be studying variations in the human epigenome (at the genomic level and the population level), how these variations influence molecular phenotypes (e.g. TF binding), and how these, in turn, influence complex traits and diseases. These are complex/interesting questions. Wish me luck!\n\ncv\nresume\npublications"
  },
  {
    "objectID": "index.html#links-reach-out",
    "href": "index.html#links-reach-out",
    "title": "Temi",
    "section": "links + reach out",
    "text": "links + reach out\n\nYou can reach me by filling this form; or\nLinkedIn; or\nTwitter; or\nGithub"
  },
  {
    "objectID": "index.html#other-lives",
    "href": "index.html#other-lives",
    "title": "Temi",
    "section": "other lives",
    "text": "other lives\nOften, I am in the gym doing bodyweight workouts. Sometimes, I read books + watch TV. Other times, I play the guitar, football, and do some DIYs for fun."
  },
  {
    "objectID": "content/books/index.html",
    "href": "content/books/index.html",
    "title": "Temi",
    "section": "",
    "text": "I don’t necessarily endorse all of these books."
  },
  {
    "objectID": "content/books/index.html#currently-reading",
    "href": "content/books/index.html#currently-reading",
    "title": "Temi",
    "section": "currently reading",
    "text": "currently reading\n\n\nThe Second Sex by Simone de Beauvoir\n\nStarted sometime in August 2023"
  },
  {
    "objectID": "content/books/index.html#previously-read",
    "href": "content/books/index.html#previously-read",
    "title": "Temi",
    "section": "previously read",
    "text": "previously read\nin no particular order:\n\nFactotum by Charles Bukowski\nA Peace To End All Peace by David Fromkin\nA Song of Ice and Fire by George R. R. Martin\n\nA Game of Thrones\nA Clash of Kings\nA Storm of Swords\n\nThe Emperor of All Maladies by Siddhartha Mukherjee\nThen We Came To The End by Joshua Ferris\nAnimal’s People by Indra Sinha\nNever Let Me Go by Kazuo Ishiguro\nThe Buried Giant by Kazuo Ishiguro"
  },
  {
    "objectID": "content/blog/posts/2023-09-03-parallelization-in-R/index.html",
    "href": "content/blog/posts/2023-09-03-parallelization-in-R/index.html",
    "title": "Parallelization in R",
    "section": "",
    "text": "When running computationally-heavy tasks in R, it can be useful to parallelize your codes/runs. In that vein, this is a really good blogpost to read to understand when and how to parallelize. And here is another one.\nR offers many ways to do this. Usually, I prefer using some libraries.\n\n\nCode\nlibrary(doParallel)\n\n\nLoading required package: foreach\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n\nCode\nlibrary(foreach)\nlibrary(parallel)\n\n\n\n\nAssuming we want to apply a function over the rows of a matrix. This function will take each row, divide the numbers in that row by the index of that row in the matrix and return a newly-created matrix of the same shape.\n\n\nCode\nset.seed(2022)\nmyMatrix &lt;- matrix(sample(1:50000, size=300*500, replace=T), nrow=300, ncol=500)\n\ndim(myMatrix)\n\n\n[1] 300 500\n\n\n\n\nFirst, I will time this function with lapply loops. lapply is shipped with base R and is a parallel form of a regular for loop.\n\n\nCode\nlapply_fxn &lt;- function(){\n  applyMatrix &lt;- lapply(1:nrow(myMatrix), function(i){\n    out &lt;- myMatrix[i, ] / (i)\n    return(out)\n  })\n  applyMatrix &lt;- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(lapply_fxn())\n\n\n   user  system elapsed \n  0.004   0.000   0.004 \n\n\n\n\n\nNext, let’s take advantage of the cores, this time using mclapply\n\n\nCode\nmclapply_fxn &lt;- function(){\n  applyMatrix &lt;- parallel::mclapply(1:nrow(myMatrix), function(i){\n    out &lt;- myMatrix[i, ] / (i)\n    return(out)\n  }, mc.cores = 12) # using 7 cores\n  \n  applyMatrix &lt;- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(mclapply_fxn())\n\n\n   user  system elapsed \n  0.043   0.084   0.032 \n\n\nWe see that lapply is faster. This is because there is an overhead to distributing these runs and collecting their results when using mclapply\n\n\n\nHere, I will use the foreach but without a parallel back-end - this is akin to a sequential run, just like lapply or a regular for loop\n\n\nCode\nsystem.time({\n  outputMatrix &lt;- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %do% {\n    out &lt;- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.071   0.005   0.078 \n\n\n\n\n\nHere, I will use the foreach but with a parallel back-end. The parallel back-end is a cluster of cores If you are familiar with multiprocessing in python, it is equivalent to multiprocessing.Pool\nFirst we need to register a parallel back-end\nWe can query how many cores we have on this computer\n\n\nCode\nparallel::detectCores()\n\n\n[1] 8\n\n\nI will register 7 cores\n\n\nCode\nnum_clusters &lt;- 7 #- 5 # 12 - 5\ndoParallel::registerDoParallel(num_clusters)\n\ncat('Registering', num_clusters, 'clusters for a parallel run\\n')\n\n\nRegistering 7 clusters for a parallel run\n\n\n\n\nCode\nsystem.time({\n  outputMatrix &lt;- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %dopar% {\n    out &lt;- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.085   0.070   0.081 \n\n\nCode\n# stop the cluster\ndoParallel::stopImplicitCluster()\n\n\nHere we see that lapply is much faster. Of course that is because of all the overheads and all that.\nNext, I will show to use the various makeCluster() options."
  },
  {
    "objectID": "content/blog/posts/2023-09-03-parallelization-in-R/index.html#use-case",
    "href": "content/blog/posts/2023-09-03-parallelization-in-R/index.html#use-case",
    "title": "Parallelization in R",
    "section": "",
    "text": "Assuming we want to apply a function over the rows of a matrix. This function will take each row, divide the numbers in that row by the index of that row in the matrix and return a newly-created matrix of the same shape.\n\n\nCode\nset.seed(2022)\nmyMatrix &lt;- matrix(sample(1:50000, size=300*500, replace=T), nrow=300, ncol=500)\n\ndim(myMatrix)\n\n\n[1] 300 500\n\n\n\n\nFirst, I will time this function with lapply loops. lapply is shipped with base R and is a parallel form of a regular for loop.\n\n\nCode\nlapply_fxn &lt;- function(){\n  applyMatrix &lt;- lapply(1:nrow(myMatrix), function(i){\n    out &lt;- myMatrix[i, ] / (i)\n    return(out)\n  })\n  applyMatrix &lt;- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(lapply_fxn())\n\n\n   user  system elapsed \n  0.004   0.000   0.004 \n\n\n\n\n\nNext, let’s take advantage of the cores, this time using mclapply\n\n\nCode\nmclapply_fxn &lt;- function(){\n  applyMatrix &lt;- parallel::mclapply(1:nrow(myMatrix), function(i){\n    out &lt;- myMatrix[i, ] / (i)\n    return(out)\n  }, mc.cores = 12) # using 7 cores\n  \n  applyMatrix &lt;- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(mclapply_fxn())\n\n\n   user  system elapsed \n  0.043   0.084   0.032 \n\n\nWe see that lapply is faster. This is because there is an overhead to distributing these runs and collecting their results when using mclapply\n\n\n\nHere, I will use the foreach but without a parallel back-end - this is akin to a sequential run, just like lapply or a regular for loop\n\n\nCode\nsystem.time({\n  outputMatrix &lt;- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %do% {\n    out &lt;- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.071   0.005   0.078 \n\n\n\n\n\nHere, I will use the foreach but with a parallel back-end. The parallel back-end is a cluster of cores If you are familiar with multiprocessing in python, it is equivalent to multiprocessing.Pool\nFirst we need to register a parallel back-end\nWe can query how many cores we have on this computer\n\n\nCode\nparallel::detectCores()\n\n\n[1] 8\n\n\nI will register 7 cores\n\n\nCode\nnum_clusters &lt;- 7 #- 5 # 12 - 5\ndoParallel::registerDoParallel(num_clusters)\n\ncat('Registering', num_clusters, 'clusters for a parallel run\\n')\n\n\nRegistering 7 clusters for a parallel run\n\n\n\n\nCode\nsystem.time({\n  outputMatrix &lt;- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %dopar% {\n    out &lt;- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.085   0.070   0.081 \n\n\nCode\n# stop the cluster\ndoParallel::stopImplicitCluster()\n\n\nHere we see that lapply is much faster. Of course that is because of all the overheads and all that.\nNext, I will show to use the various makeCluster() options."
  },
  {
    "objectID": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html",
    "href": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html",
    "title": "Predicting swarm behaviour",
    "section": "",
    "text": "To practice my deep learning chops/model-writing skills, I pick a toy dataset. Here I aim to predict swarm behaviour.\n\n\nCode\nimport torch\nimport numpy as np, os, sys, requests, pandas as pd, zipfile as zf\nimport matplotlib.pyplot as plt\n\nprint(f'Kernel used is: {os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))}')\n\n\nKernel used is: dl-tools\n\n\n\n\nCode\nprint(f'Pytorch version is {torch.__version__}')\n\n\nPytorch version is 2.0.1+cu117\n\n\n\n\nCode\ntorch.cuda.is_available(), torch.cuda.current_device(), torch.cuda.device_count(), torch.cuda.get_device_name(0)\n\n\n(True, 0, 1, 'Quadro P1000')\n\n\n\n\nCode\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n\n\nUsing device: cuda\n\nQuadro P1000\nMemory Usage:\nAllocated: 0.0 GB\nCached:    0.0 GB\n\n\n\n\n\n\nCode\ndata_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00524/Swarm%20Behavior%20Data.zip'\ndata_dir = '/home/temi/Files/learning_stuff/pytorch_implementations/swarm behaviour'\n\n\n\n\nCode\nif not 'data' in os.listdir(data_dir):\n    os.mkdir('/home/temi/Projects/swarm_behaviour/data/')\nelse:\n    print('data directory already exists.')\n\n\ndata directory already exists.\n\n\n\n\nCode\ndef download_url(url, save_path, chunk_size=128):\n    r = requests.get(url, stream=True)\n    with open(save_path, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            fd.write(chunk)\n\n\n\n\nCode\ndata_path = os.path.join(data_dir, 'data', 'swarm_behaviour.zip')\n\nif not 'swarm_behaviour.zip' in os.listdir(os.path.join(data_dir, 'data')):\n    print('Downloading...')\n    download_url(data_link, )\n    print('Done.')\nelse:\n    print('File already present.')\n\n\nFile already present.\n\n\n\n\nCode\nwith zf.ZipFile(data_path, 'r') as obj:\n    name_of_files = obj.namelist()[1:]\n\nname_of_files\n\n\n['Swarm Behavior Data/Aligned.csv',\n 'Swarm Behavior Data/Flocking.csv',\n 'Swarm Behavior Data/Grouped.csv']\n\n\n\n\nCode\n# read in each file into a dictionary\nkeys = ['aligned', 'flocking', 'grouped']\nswarm_data = dict()\nfor i, file_name in enumerate(name_of_files):\n    zipfile = zf.ZipFile(data_path)\n    swarm_data[keys[i]] = pd.read_csv(zipfile.open(file_name))\n    zipfile.close()\n\n\n/tmp/ipykernel_184041/1264924189.py:6: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n  swarm_data[keys[i]] = pd.read_csv(zipfile.open(file_name))\n\n\n\n\nCode\nswarm_data['aligned'].iloc[0:5,:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.00\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\nswarm_data['flocking'].iloc[0:5,0:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.0\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\nswarm_data['grouped'].iloc[0:5,:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.00\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\n\ndef split_into_train_val_test(df, tr_frac=0.5, rem_frac=0.5, seed=1, where_target='last',\n                             return_what='dict'):\n    \n    # shuffle the dataframe\n    df = df.sample(frac=1, random_state=seed)\n\n    if where_target=='last':\n        df_preds = df.iloc[:, :-1]\n        df_targets = df.iloc[:, -1]\n\n    # split into training and testing\n    # 0.7, 0.3\n    split_one = int(df_preds.shape[0] * tr_frac)\n\n    df_preds_train = df_preds[:split_one]\n    df_targets_train = df_targets[:split_one]\n    remaining_preds, remaining_targets = df_preds[split_one:], df_targets[split_one:]\n\n    split_two = int(remaining_preds.shape[0]*rem_frac)\n    df_preds_val = remaining_preds[:split_two]\n    df_preds_test = remaining_preds[split_two:]\n\n    df_targets_val = remaining_targets[:split_two]\n    df_targets_test = remaining_targets[split_two:]\n\n    print(df_preds_train.shape, df_targets_train.shape)\n    print(df_preds_val.shape, df_targets_val.shape)\n    print(df_preds_test.shape, df_targets_test.shape)\n    \n    if return_what=='dict':\n        return  {'train': (df_preds_train.to_numpy(), df_targets_train.to_numpy()), \n                 'validate': (df_preds_val.to_numpy(), df_targets_val.to_numpy()),\n                 'test': (df_preds_test.to_numpy(), df_targets_test.to_numpy())}\n    elif return_what=='tuple':\n        return ((df_preds_train.to_numpy(), df_targets_train.to_numpy()), \n                (df_preds_val.to_numpy(), df_targets_val.to_numpy()), \n                (df_preds_test.to_numpy(), df_targets_test.to_numpy()))"
  },
  {
    "objectID": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html#downloading-the-data-and-so-on",
    "href": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html#downloading-the-data-and-so-on",
    "title": "Predicting swarm behaviour",
    "section": "",
    "text": "Code\ndata_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00524/Swarm%20Behavior%20Data.zip'\ndata_dir = '/home/temi/Files/learning_stuff/pytorch_implementations/swarm behaviour'\n\n\n\n\nCode\nif not 'data' in os.listdir(data_dir):\n    os.mkdir('/home/temi/Projects/swarm_behaviour/data/')\nelse:\n    print('data directory already exists.')\n\n\ndata directory already exists.\n\n\n\n\nCode\ndef download_url(url, save_path, chunk_size=128):\n    r = requests.get(url, stream=True)\n    with open(save_path, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            fd.write(chunk)\n\n\n\n\nCode\ndata_path = os.path.join(data_dir, 'data', 'swarm_behaviour.zip')\n\nif not 'swarm_behaviour.zip' in os.listdir(os.path.join(data_dir, 'data')):\n    print('Downloading...')\n    download_url(data_link, )\n    print('Done.')\nelse:\n    print('File already present.')\n\n\nFile already present.\n\n\n\n\nCode\nwith zf.ZipFile(data_path, 'r') as obj:\n    name_of_files = obj.namelist()[1:]\n\nname_of_files\n\n\n['Swarm Behavior Data/Aligned.csv',\n 'Swarm Behavior Data/Flocking.csv',\n 'Swarm Behavior Data/Grouped.csv']\n\n\n\n\nCode\n# read in each file into a dictionary\nkeys = ['aligned', 'flocking', 'grouped']\nswarm_data = dict()\nfor i, file_name in enumerate(name_of_files):\n    zipfile = zf.ZipFile(data_path)\n    swarm_data[keys[i]] = pd.read_csv(zipfile.open(file_name))\n    zipfile.close()\n\n\n/tmp/ipykernel_184041/1264924189.py:6: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n  swarm_data[keys[i]] = pd.read_csv(zipfile.open(file_name))\n\n\n\n\nCode\nswarm_data['aligned'].iloc[0:5,:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.00\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\nswarm_data['flocking'].iloc[0:5,0:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.0\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\nswarm_data['grouped'].iloc[0:5,:]\n\n\n\n\n\n\n\n\n\nx1\ny1\nxVel1\nyVel1\nxA1\nyA1\nxS1\nyS1\nxC1\nyC1\n...\nyVel200\nxA200\nyA200\nxS200\nyS200\nxC200\nyC200\nnAC200\nnS200\nClass\n\n\n\n\n0\n-1414.14\n-535.22\n-17.88\n-7.23\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.85\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n29\n0\n0\n\n\n1\n-1412.93\n597.54\n-13.55\n-5.48\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-12.09\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n44\n0\n0\n\n\n2\n-1407.38\n70.72\n-14.37\n-5.81\n0.00\n0.0\n0.00\n0.00\n0.00\n0.00\n...\n-16.20\n0.0\n0.00\n0.0\n0.0\n0.00\n0.00\n40\n0\n0\n\n\n3\n-1407.00\n-759.80\n-7.59\n-1.27\n-0.98\n-0.2\n0.00\n0.00\n0.91\n0.41\n...\n2.99\n-1.0\n-0.07\n0.0\n0.0\n-0.52\n0.86\n3\n0\n1\n\n\n4\n-1406.36\n698.39\n-16.54\n-6.95\n-1.00\n0.0\n-944.07\n-396.62\n0.00\n0.00\n...\n-12.61\n0.0\n-1.00\n0.0\n0.0\n0.00\n0.00\n13\n0\n0\n\n\n\n\n5 rows × 2401 columns\n\n\n\n\n\nCode\n\ndef split_into_train_val_test(df, tr_frac=0.5, rem_frac=0.5, seed=1, where_target='last',\n                             return_what='dict'):\n    \n    # shuffle the dataframe\n    df = df.sample(frac=1, random_state=seed)\n\n    if where_target=='last':\n        df_preds = df.iloc[:, :-1]\n        df_targets = df.iloc[:, -1]\n\n    # split into training and testing\n    # 0.7, 0.3\n    split_one = int(df_preds.shape[0] * tr_frac)\n\n    df_preds_train = df_preds[:split_one]\n    df_targets_train = df_targets[:split_one]\n    remaining_preds, remaining_targets = df_preds[split_one:], df_targets[split_one:]\n\n    split_two = int(remaining_preds.shape[0]*rem_frac)\n    df_preds_val = remaining_preds[:split_two]\n    df_preds_test = remaining_preds[split_two:]\n\n    df_targets_val = remaining_targets[:split_two]\n    df_targets_test = remaining_targets[split_two:]\n\n    print(df_preds_train.shape, df_targets_train.shape)\n    print(df_preds_val.shape, df_targets_val.shape)\n    print(df_preds_test.shape, df_targets_test.shape)\n    \n    if return_what=='dict':\n        return  {'train': (df_preds_train.to_numpy(), df_targets_train.to_numpy()), \n                 'validate': (df_preds_val.to_numpy(), df_targets_val.to_numpy()),\n                 'test': (df_preds_test.to_numpy(), df_targets_test.to_numpy())}\n    elif return_what=='tuple':\n        return ((df_preds_train.to_numpy(), df_targets_train.to_numpy()), \n                (df_preds_val.to_numpy(), df_targets_val.to_numpy()), \n                (df_preds_test.to_numpy(), df_targets_test.to_numpy()))"
  }
]