{
  "hash": "c205798d435ca86cac23640749ef2db4",
  "result": {
    "markdown": "---\ntitle: \"Understanding pvalues, multiple testing and qvalues\"\nauthor: \"Temi\"\ndescription: \"...\"\ndate: \"Wed Sep 6 2023\"\ncategories: [R, statistics]\n---\n\n\n:::{.callout-note}\nThis post is still under construction; I am adding sutff as I get the time to.\n:::\n\n:::{.callout-tip}\n1. [Storey and Tibshirani's paper](https://www.pnas.org/doi/10.1073/pnas.1530509100?url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org&rfr_dat=cr_pub++0pubmed) is a good place to start\n2. Haky's notes [here](https://lab-notes.hakyimlab.org/post/2023-03-28-multiple-testing/) are also very helpful; and this notebook was partly inspired by her notes.\n:::\n\n# Introduction\nWhen doing multiple tests, there is a need to control the false positive rate (fpr) because by nature of p-values, if there is nothing interesting going on, you still have an `alpha` % chance of detecting something, and misclassifying that.\n\n\n# Simulation 1: Null and alternative effects.\n\nI have a simple linear function here, where $X$ has some effect, $\\beta$ on $Y$.\n\n$$\nY \\approx \\sum X\\beta + \\epsilon\n$$\n\nwhere,\n\n$$\nX \\approx \\mathcal{N}(0.2,1)\\\n$$\n\n$$\n\\epsilon \\approx \\mathcal{N}(0,0.1)\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(qvalue)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndevtools::source_gist('https://gist.github.com/TemiPete/d7e37272964e5f00af4efea01d295dc8')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nℹ Sourcing gist \"d7e37272964e5f00af4efea01d295dc8\"\nℹ SHA-1 hash of file is \"d308c0c2f4eb3d58cff6b52ad22538f09bd136e0\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\nnobserv <- 2000 # number of observations\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_mean <- 0.2\ntrue_sd <- 1\neps_mean <- 0\neps_sd <- 0.5\nbeta <- 0.6\nx <- rnorm(n=nobserv, mean=true_mean, sd=true_sd)\ne <- rnorm(n=nobserv, mean = eps_mean, sd=eps_sd)\n\nyalt <- x * beta + e\nplot(x, yalt, main='x has an effect on y', frame.plot=F)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.svg)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nynull <- rnorm(n=nobserv, mean=0, sd=beta)\nplot(x, ynull, main='x has no effect on y', frame.plot=F)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.svg)\n:::\n:::\n\n\nNow I can simulate these tests multiple times, say, 10000\n\n\n::: {.cell}\n\n```{.r .cell-code}\nntests <- 10000\nX <- matrix(rnorm(n=nobserv*ntests, mean=true_mean, sd=true_sd), nrow=nobserv, ncol=ntests)\nepsilon <- matrix(rnorm(n=nobserv*ntests, mean=eps_mean, sd=eps_sd), nrow=nobserv, ncol=ntests)\ndim(X) ; dim(epsilon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2000 10000\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2000 10000\n```\n:::\n\n```{.r .cell-code}\nYalt <- X * beta + epsilon\nYnull <- matrix(rnorm(n=nobserv, mean=0, sd=beta), nrow=nobserv, ncol=ntests)\ndim(Yalt) ; dim(Ynull)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2000 10000\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2000 10000\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npvec = rep(NA,ntests)\nbvec = rep(NA,ntests)\n\nfor(ss in 1:ntests)\n{\n  fit = fastlm(X[,ss], Ynull[,ss])\n  pvec[ss] = fit$pval  \n  bvec[ss] = fit$betahat\n}\n\nsummary(pvec)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000291 0.2501638 0.4916683 0.4972860 0.7465493 0.9999487 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(pvec < 0.05) ; mean(pvec < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 481\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0481\n```\n:::\n:::\n\n\nEven under the null, we find that 5% of our tests are false positives! In real life, we would think these are true effects, which is not good.\n\nSo, we try to control this `false positive rate`. There are many methods, but we can use the Bonferroni approach\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff <- 0.05/length(pvec)\nsum(pvec < cutoff) ; mean(pvec < cutoff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nWith Bonferroni correction, we see that all of the tests are null, which is what should have happened in the first place.\nAnyway, all that was for simulation sake. I should create a set of tests, where some proportion are under the alternative i.e. true, and the rest are not i.e. null\n\n# Simulation 2: A mixture of outcomes under the null and alternative hypothesis.\n\n::: {.cell}\n\n```{.r .cell-code}\nptrue <- 0.2 # only 20% of the tests are TRUE\nwtrue <- sample(x=c(0,1), size=ntests, replace=TRUE, prob=c(1-0.2, 0.2))\ntable(wtrue) |> prop.table()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nwtrue\n     0      1 \n0.8046 0.1954 \n```\n:::\n:::\n\n\nI will look through `wtrue`. If `0`, I will select the ynull at that index, otherwise, I will select the yalt\n\n\n::: {.cell}\n\n```{.r .cell-code}\nYboth <- matrix(NA, nrow=nobserv, ncol=ntests)\nfor(i in seq_along(wtrue)){\n    if(wtrue[i] == 1){\n        Yboth[, i] <- Yalt[, i]\n    } else {\n        Yboth[, i] <- Ynull[, i]\n    }\n}\n\ndim(Yboth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2000 10000\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## run linear regression for all 10000 phenotypes in the mix of true and false associations, Ymat_mix\npvec_mix = rep(NA,ntests)\nbvec_mix = rep(NA,ntests)\nfor(ss in 1:ntests){\n  fit = fastlm(X[,ss], Yboth[,ss])\n  pvec_mix[ss] = fit$pval  \n  bvec_mix[ss] = fit$betahat\n}\nsummary(pvec_mix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0678  0.3792  0.3992  0.6825  0.9999 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(pvec_mix < 0.05) ; mean(pvec_mix < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2339\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2339\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(pvec_mix, main='')\nmtext('A simulation under the null + alt', side=3, line=1, adj = 0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.svg)\n:::\n:::\n\n\n# Quick detour: fpr, fdr, pfdr and such\nWe expect 500 to be significant under the null, but we get 2339. \n\nSince we have more than what we expected under the null, we can assume that the remainder are gotten under the alternative. We can estimate this `true discovery rate` \n\n\n$$\n\\frac{(nobserved - nexpected)}{nobserved}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\ntdr <- ((sum(pvec_mix < 0.05)) - (0.05*ntests))/(sum(pvec_mix < 0.05))\ntdr \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7862334\n```\n:::\n:::\n\n\nThe `false discovery rate` is 1 - `tdr`, which in this case is 0.2137666. \n\nAll well and good, except that our `tdr` here is higher than we expect. Instead we can estimate the positive false discovery rate or `pFDR`. Here's how I explain this: Given that you have found a number of tests to be significant, let's call this `tsig`, and we expect at least one of these to be positive i.e. under the alternative, what is the expected number of false positives? i.e. what proportion are not true but we come out as true. \n\nTo break this down a little, I will start from here:\nAssuming you do a test to classify if a a group of people eat fruits or not, and you have this table after.\n\n\n|                                 | eats fruits     | does not eat fruits |   |\n|---------------------------------|-----------------|---------------------|---|\n| classified: eat fruits          | true positives  | false positives     |   |\n| classified: does not eat fruits | false negatives | true negatives      |   |\n|                                 |                 |                     |   |\n\nThe fpr, as mentioned earlier is: \n\n$$\n\\frac{false\\ positives}{(false\\ positives\\ +\\ true\\ negatives)}\n$$\n\n\ni.e. of all the people who don't eat fruits, how many of them do we classify to eat fruits based on our tests?\n\nThe fdr then is, of all the people who we classify as eating fruits, how many of them don't actually eat fruits?\n\n$$\n\\frac{false\\ positives}{(true\\ positives\\ +\\ false\\ positives)}\n$$\n\n\nUsing the table + idea above, I can then \n\n\n|                                 | eats fruits     | does not eat fruits |   |\n|---------------------------------|-----------------|---------------------|---|\n| classified: eat fruits          | true positives  | false positives     |   |\n| classified: does not eat fruits | false negatives | true negatives      |   |\n|                                 |                 |                     |   |\n\n\nBecause we simulated the data, we know that 1954 tests are under the alternative, and the rest, 8046, should be under the null. But after our tests, we have found 2339 to be under the alternative and 7661 to be under the null. So, there are some 385 that have been misclassified as under the alternative when they are not, and some -385\n\n::: {.cell}\n\n```{.r .cell-code}\ntp <- sum(wtrue == 1 & pvec_mix < 0.05)\ntn <- sum(wtrue == 0 & pvec_mix >= 0.05)\nfp <- sum(wtrue == 1 & pvec_mix >= 0.05)\nfn <- sum(wtrue == 0 & pvec_mix < 0.05)\n```\n:::\n\n\nAlternatively, `table(pvec_mix > 0.05, wtrue)` will yield the same result.\n\n|                         | alternative | null        | total       |\n|-------------------------|-------------|-------------|-------------|\n| classified: alternative | 1954      | 385      | 2339 |\n| classified: null        | 0      | 7661      | 7661 |\n| total                   | 1954 | 8046 |             |\n\n\nWith this, we can estimate the `fpr` to be 0 and `fdr` to be 0. \n\nOkay. Back to `pfdr`. Remember that I described this earlier, saying:\n\n> \"Given that you have found a number of tests to be significant, let's call this `tsig`, and we expect at least one of these to be positive i.e. under the alternative, what is the expected number of false positives? i.e. what proportion are not true but we come out as true.\"\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}