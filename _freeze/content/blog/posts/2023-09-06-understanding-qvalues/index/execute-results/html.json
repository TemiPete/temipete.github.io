{
  "hash": "aba6030cc5c6ccfa1bf3fd5c0563cde9",
  "result": {
    "markdown": "---\ntitle: \"Understanding pvalues, multiple testing and qvalues\"\nauthor: \"Temi\"\ndescription: \"...\"\ndate: \"Wed Sep 6 2023\"\ncategories: [R, statistics]\n---\n\n\n:::{.callout-note}\nThis post is still under construction; I am adding sutff as I get the time to.\n:::\n\n:::{.callout-tip}\n1. (Storey and Tibshirani's paper)[https://www.pnas.org/doi/10.1073/pnas.1530509100?url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org&rfr_dat=cr_pub++0pubmed] is a good place to start\n:::\n\n# Introduction\nWhen you do multiple testing, you want to control the false positive rate (fpr) because by nature of p-values, if there is nothing interesting going on, you still have an `alpha` % chance of detecting something, which is a false positive.\n\n\n# Simulation 1: Null and alternative effects.\n\nI have a simple linear function here, where $X$ has some effect, $\\beta$ on $Y$.\n$$\nY \\approx \\sum X\\beta + \\epsilon\n$$\n\nwhere,\n\n$$\nX \\approx \\mathcal{N}(0.2,1)\\\n$$\n$$\n\\epsilon \\approx \\mathcal{N}(0,0.1)\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(qvalue)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndevtools::source_gist('https://gist.github.com/TemiPete/d7e37272964e5f00af4efea01d295dc8')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nℹ Sourcing gist \"d7e37272964e5f00af4efea01d295dc8\"\nℹ SHA-1 hash of file is \"d308c0c2f4eb3d58cff6b52ad22538f09bd136e0\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\nnobserv <- 2000 # number of observations\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_mean <- 0.2\ntrue_sd <- 1\neps_mean <- 0\neps_sd <- 0.5\nbeta <- 0.6\nx <- rnorm(n=nobserv, mean=true_mean, sd=true_sd)\ne <- rnorm(n=nobserv, mean = eps_mean, sd=eps_sd)\n\nyalt <- x * beta + e\nplot(x, yalt, main='x has an effect on y', frame.plot=F)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.svg)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nynull <- rnorm(n=nobserv, mean=0, sd=beta)\nplot(x, ynull, main='x has no effect on y', frame.plot=F)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.svg)\n:::\n:::\n\n\nNow I can simulate these tests multiple times, say, 1000\n\n\n::: {.cell}\n\n```{.r .cell-code}\nntests <- 10000\nX <- matrix(rnorm(n=nobserv*ntests, mean=true_mean, sd=true_sd), nrow=nobserv, ncol=ntests)\nepsilon <- matrix(rnorm(n=nobserv*ntests, mean=eps_mean, sd=eps_sd), nrow=nobserv, ncol=ntests)\ndim(X) ; dim(epsilon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2000 10000\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2000 10000\n```\n:::\n\n```{.r .cell-code}\nYalt <- X * beta + epsilon\nYnull <- matrix(rnorm(n=nobserv, mean=0, sd=beta), nrow=nobserv, ncol=ntests)\ndim(Yalt) ; dim(Ynull)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2000 10000\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2000 10000\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npvec = rep(NA,ntests)\nbvec = rep(NA,ntests)\n\nfor(ss in 1:ntests)\n{\n  fit = fastlm(X[,ss], Ynull[,ss])\n  pvec[ss] = fit$pval  \n  bvec[ss] = fit$betahat\n}\n\nsummary(pvec)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000291 0.2501638 0.4916683 0.4972860 0.7465493 0.9999487 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(pvec < 0.05) ; mean(pvec < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 481\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0481\n```\n:::\n:::\n\n\nEven under the null, we find that 5% of our tests are false positives! In real life, we would think these are true effects, which is not good.\n\nSo, we try to control this `false positive rate`. There are many methods, but we can use the Bonferroni approach\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff <- 0.05/length(pvec)\nsum(pvec < cutoff) ; mean(pvec < cutoff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nWith Bonferroni correction, we see that all of the tests are null, which is what should have happened in the first place.\nAnyway, all that was for simulation sake. I should create a set of tests, where some proportion are under the alternative i.e. true, and the rest are not i.e. null\n\n# Simulation 2: A mixture of outcomes under the null and alternative hypothesis.\n\n::: {.cell}\n\n```{.r .cell-code}\nptrue <- 0.2 # only 20% of the tests are TRUE\nwtrue <- sample(x=c(0,1), size=ntests, replace=TRUE, prob=c(1-0.2, 0.2))\ntable(wtrue) |> prop.table()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nwtrue\n     0      1 \n0.8046 0.1954 \n```\n:::\n:::\n\n\nI will look through `wtrue`. If `0`, I will select the ynull at that index, otherwise, I will select the yalt\n\n\n::: {.cell}\n\n```{.r .cell-code}\nYboth <- matrix(NA, nrow=nobserv, ncol=ntests)\nfor(i in seq_along(wtrue)){\n    if(wtrue[i] == 1){\n        Yboth[, i] <- Yalt[, i]\n    } else {\n        Yboth[, i] <- Ynull[, i]\n    }\n}\n\ndim(Yboth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2000 10000\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## run linear regression for all 10000 phenotypes in the mix of true and false associations, Ymat_mix\npvec_mix = rep(NA,ntests)\nbvec_mix = rep(NA,ntests)\nfor(ss in 1:ntests){\n  fit = fastlm(X[,ss], Yboth[,ss])\n  pvec_mix[ss] = fit$pval  \n  bvec_mix[ss] = fit$betahat\n}\nsummary(pvec_mix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0678  0.3792  0.3992  0.6825  0.9999 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(pvec_mix < 0.05) ; mean(pvec_mix < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2339\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2339\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(pvec_mix, main='')\nmtext('A simulation under the null + alt', side=3, line=1, adj = 0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.svg)\n:::\n:::\n\n\nWe expect 500 to be significant under the null, but we get 2339. \n\nSince we have more than what we expected under the null, we can assume that the remainder are gotten under the alternative. We can estimate this `true discovery rate` \n\n$$\n    \\frac{(nobserved - nexpected)}{nobserved}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntdr <- ((sum(pvec_mix < 0.05)) - (0.05*ntests))/(sum(pvec_mix < 0.05))\ntdr \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7862334\n```\n:::\n:::\n\n\nThe `false discovery rate` is 1 - `tdr`, which in this case is 0.2137666",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}