---
title: "Understanding pvalues, multiple testing and qvalues"
author: "Temi"
description: "..."
date: "Wed Sep 6 2023"
categories: [R, statistics]
---

:::{.callout-note}
This post is still under construction; I am adding sutff as I get the time to.
:::

:::{.callout-tip}
1. [Storey and Tibshirani's paper](https://www.pnas.org/doi/10.1073/pnas.1530509100?url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org&rfr_dat=cr_pub++0pubmed) is a good place to start
2. Haky's notes [here](https://lab-notes.hakyimlab.org/post/2023-03-28-multiple-testing/) are also very helpful; and this notebook was partly inspired by her notes.
:::

# Introduction
When you do multiple testing, you want to control the false positive rate (fpr) because by nature of p-values, if there is nothing interesting going on, you still have an `alpha` % chance of detecting something, which is a false positive.


# Simulation 1: Null and alternative effects.

I have a simple linear function here, where $X$ has some effect, $\beta$ on $Y$.
$$
Y \approx \sum X\beta + \epsilon
$$

where,


$$
X \approx \mathcal{N}(0.2,1)\
$$
$$
\epsilon \approx \mathcal{N}(0,0.1)
$$

```{r, warning=F, message=F}
library(qvalue)
```

```{r}
devtools::source_gist('https://gist.github.com/TemiPete/d7e37272964e5f00af4efea01d295dc8')
```

```{r}
set.seed(2023)
nobserv <- 2000 # number of observations
```

```{r}
true_mean <- 0.2
true_sd <- 1
eps_mean <- 0
eps_sd <- 0.5
beta <- 0.6
x <- rnorm(n=nobserv, mean=true_mean, sd=true_sd)
e <- rnorm(n=nobserv, mean = eps_mean, sd=eps_sd)

yalt <- x * beta + e
plot(x, yalt, main='x has an effect on y', frame.plot=F)
```

```{r}
ynull <- rnorm(n=nobserv, mean=0, sd=beta)
plot(x, ynull, main='x has no effect on y', frame.plot=F)
```

Now I can simulate these tests multiple times, say, 10000

```{r}
ntests <- 10000
X <- matrix(rnorm(n=nobserv*ntests, mean=true_mean, sd=true_sd), nrow=nobserv, ncol=ntests)
epsilon <- matrix(rnorm(n=nobserv*ntests, mean=eps_mean, sd=eps_sd), nrow=nobserv, ncol=ntests)
dim(X) ; dim(epsilon)

Yalt <- X * beta + epsilon
Ynull <- matrix(rnorm(n=nobserv, mean=0, sd=beta), nrow=nobserv, ncol=ntests)
dim(Yalt) ; dim(Ynull)
```

```{r}
pvec = rep(NA,ntests)
bvec = rep(NA,ntests)

for(ss in 1:ntests)
{
  fit = fastlm(X[,ss], Ynull[,ss])
  pvec[ss] = fit$pval  
  bvec[ss] = fit$betahat
}

summary(pvec)
```
```{r}
sum(pvec < 0.05) ; mean(pvec < 0.05)
```

Even under the null, we find that 5% of our tests are false positives! In real life, we would think these are true effects, which is not good.

So, we try to control this `false positive rate`. There are many methods, but we can use the Bonferroni approach

```{r}
cutoff <- 0.05/length(pvec)
sum(pvec < cutoff) ; mean(pvec < cutoff)
```

With Bonferroni correction, we see that all of the tests are null, which is what should have happened in the first place.
Anyway, all that was for simulation sake. I should create a set of tests, where some proportion are under the alternative i.e. true, and the rest are not i.e. null

# Simulation 2: A mixture of outcomes under the null and alternative hypothesis.
```{r}
ptrue <- 0.2 # only 20% of the tests are TRUE
wtrue <- sample(x=c(0,1), size=ntests, replace=TRUE, prob=c(1-0.2, 0.2))
table(wtrue) |> prop.table()
```

I will look through `wtrue`. If `0`, I will select the ynull at that index, otherwise, I will select the yalt

```{r}
Yboth <- matrix(NA, nrow=nobserv, ncol=ntests)
for(i in seq_along(wtrue)){
    if(wtrue[i] == 1){
        Yboth[, i] <- Yalt[, i]
    } else {
        Yboth[, i] <- Ynull[, i]
    }
}

dim(Yboth)
```


```{r}
## run linear regression for all 10000 phenotypes in the mix of true and false associations, Ymat_mix
pvec_mix = rep(NA,ntests)
bvec_mix = rep(NA,ntests)
for(ss in 1:ntests){
  fit = fastlm(X[,ss], Yboth[,ss])
  pvec_mix[ss] = fit$pval  
  bvec_mix[ss] = fit$betahat
}
summary(pvec_mix)
```

```{r}
sum(pvec_mix < 0.05) ; mean(pvec_mix < 0.05)
```

```{r}
hist(pvec_mix, main='')
mtext('A simulation under the null + alt', side=3, line=1, adj = 0)
```

We expect `r 0.05*ntests` to be significant under the null, but we get `r sum(pvec_mix < 0.05)`. 

Since we have more than what we expected under the null, we can assume that the remainder are gotten under the alternative. We can estimate this `true discovery rate` 

$$
\frac{(nobserved - nexpected)}{nobserved}
$$

```{r}
tdr <- ((sum(pvec_mix < 0.05)) - (0.05*ntests))/(sum(pvec_mix < 0.05))
tdr 
```

The `false discovery rate` is 1 - `tdr`, which in this case is `r 1 - tdr`. 

All well and good, except that our `tdr` here is higher than we expect. Instead we can estimate the positive false discovery rate or `pFDR`. Here's how I explain this:
  : Given that you have found a number of tests to be significant, let's call this `tsig`, and we expect at least one of these to be positive i.e. under the alternative, what is the expected number of false positives? i.e. what proportion are not true but we come out as true. 

To break this down a little, I will start from here:
Assuming you do a test to classify if a a group of people eat fruits or not, and you have this table after.


|                                 | eats fruits     | does not eat fruits |   |
|---------------------------------|-----------------|---------------------|---|
| classified: eat fruits          | true positives  | false positives     |   |
| classified: does not eat fruits | false negatives | true negatives      |   |
|                                 |                 |                     |   |

The fpr, as mentioned earlier is: 
$$
\frac{false\ positives}{(false\ positives\ +\ true\ negatives)}
$$

i.e. of all the people who don't eat fruits, how many of them do we classify to eat fruits based on our tests?

The fdr then is, of all the people who we classify as eating fruits, how many of them don't actually eat fruits?
$$
\frac{false\ positives}{(true\ positives\ +\ false\ positives)}
$$

Using the table + idea above, I can then 


|                                 |      | does not eat fruits |   |
|---------------------------------|-----------------|---------------------|---|
| classified: eat fruits          | true positives  | false positives     |   |
| classified: does not eat fruits | false negatives | true negatives      |   |
|                                 |                 |                     |   |



```{r}
which(pvec_mix < 0.05)
```

Because we simulated the data, we know that `r sum(wtrue == 1)` tests are under the alternative, and the rest, `r sum(wtrue == 0)`, should be under the null. But after our tests, we have found `r sum(pvec_mix < 0.05)` to be under the alternative and `r sum(pvec_mix >= 0.05)` to be under the null. So, there are some `r sum(pvec_mix < 0.05) - sum(wtrue == 1)` that have been misclassified as under the alternative when they are not, and some `r sum(pvec_mix >= 0.05) - sum(wtrue == 0)`
```{r}
tp <- sum(wtrue == 1 & pvec_mix < 0.05)
tn <- sum(wtrue == 0 & pvec_mix >= 0.05)
fp <- sum(wtrue == 1 & pvec_mix >= 0.05)
fn <- sum(wtrue == 0 & pvec_mix < 0.05)
```

Alternatively, `table(pvec_mix > 0.05, wtrue)` will yield the same result.

|                         | alternative | null        | total       |
|-------------------------|-------------|-------------|-------------|
| classified: alternative | `r tp`      | `r fn`      | `r tp + fn` |
| classified: null        | `r fp`      | `r tn`      | `r fp + tn` |
| total                   | `r tp + fp` | `r fn + tn` |             |
