{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Encoding DNA sequences for DL training\"\n",
    "author: \"Temi\"\n",
    "description: \"...\"\n",
    "date: \"Fri Sep 29 2023\"\n",
    "categories: [pytorch, statistics]\n",
    "---\n",
    "\n",
    ":::{.callout-note}\n",
    "This post is still under construction; I am adding sutff as I get the time to.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing DL with DNA sequences, you want to represent the sequences in formats that the computer can process. Since DNA sequences (ACGT) can be thought of as , we need to one-hot encode them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function to generate a sequence of a cerain length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sequence_inputs(size=10):\n",
    "    r_seq_list = np.random.choice(['A', 'G', 'T', 'C'], size)\n",
    "    return(''.join(r_seq_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGGCT'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_random_sequence_inputs(size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll generate a random DNA sequence of length 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_one = generate_random_sequence_inputs(size=501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways sequence data can be encoded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_one_array = np.array(list(dna_one)).reshape(-1, 1)\n",
    "dna_one_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "dna_one_enc = encoder.fit_transform(dna_one_array)\n",
    "print(dna_one_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['A'],\n",
       "       ['A'],\n",
       "       ['T'],\n",
       "       ['G']], dtype='<U1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_one_array[0:4, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "\n",
    "We can use k-mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_one = generate_random_sequence_inputs(size=5001)\n",
    "dna_two = generate_random_sequence_inputs(size=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5\n",
    "# offset = 3\n",
    "# for i in range(0, len(dna_one), offset):\n",
    "#     print(f'{i}:{i+k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kmers(seq, k=3, offset=1, space=True):\n",
    "    if offset == 0:\n",
    "        raise('ERROR - offset value must be greater than 0')\n",
    "    else:\n",
    "        out = [seq[i:(i+k)] for i in range(0, len(seq), offset)]\n",
    "        if space == True:\n",
    "            out = ' '.join(out)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_one_3mer = create_kmers(dna_one, k=9, offset=4)\n",
    "dna_two_3mer = create_kmers(dna_two, k=9, offset=4)\n",
    "dna_document = [dna_one_3mer, dna_two_3mer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['dna_one', 'dna_two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.fit_transform(dna_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaaaactgg', 'aaaaatttg', 'aaaacatag', ..., 'ttttagcaa',\n",
       "       'ttttcatcc', 'ttttctgtg'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaactgg</th>\n",
       "      <th>aaaaatttg</th>\n",
       "      <th>aaaacatag</th>\n",
       "      <th>aaaaccgcg</th>\n",
       "      <th>aaaacgttg</th>\n",
       "      <th>aaaagaccc</th>\n",
       "      <th>aaaagcaaa</th>\n",
       "      <th>aaaagcaag</th>\n",
       "      <th>aaaagggtg</th>\n",
       "      <th>aaaagtccc</th>\n",
       "      <th>...</th>\n",
       "      <th>tttggcata</th>\n",
       "      <th>tttgggtgc</th>\n",
       "      <th>tttgtacaa</th>\n",
       "      <th>tttgtggtt</th>\n",
       "      <th>tttgttaag</th>\n",
       "      <th>ttttaaaga</th>\n",
       "      <th>ttttaaggg</th>\n",
       "      <th>ttttagcaa</th>\n",
       "      <th>ttttcatcc</th>\n",
       "      <th>ttttctgtg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dna_one</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dna_two</th>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 2486 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaactgg  aaaaatttg  aaaacatag  aaaaccgcg  aaaacgttg  aaaagaccc  \\\n",
       "dna_one   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "dna_two   0.028261   0.028261   0.028261   0.028261   0.028261   0.028261   \n",
       "\n",
       "         aaaagcaaa  aaaagcaag  aaaagggtg  aaaagtccc  ...  tttggcata  \\\n",
       "dna_one   0.000000   0.028261   0.028261   0.000000  ...   0.000000   \n",
       "dna_two   0.028261   0.000000   0.000000   0.028261  ...   0.028261   \n",
       "\n",
       "         tttgggtgc  tttgtacaa  tttgtggtt  tttgttaag  ttttaaaga  ttttaaggg  \\\n",
       "dna_one   0.028261   0.000000   0.000000   0.028261   0.000000   0.028261   \n",
       "dna_two   0.000000   0.028261   0.028261   0.000000   0.028261   0.000000   \n",
       "\n",
       "         ttttagcaa  ttttcatcc  ttttctgtg  \n",
       "dna_one   0.028261   0.000000   0.028261  \n",
       "dna_two   0.000000   0.028261   0.000000  \n",
       "\n",
       "[2 rows x 2486 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_dt = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names_out(), index=docs)\n",
    "tfidf_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dna_one</th>\n",
       "      <th>dna_two</th>\n",
       "      <th>kmer_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaactgg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaatttg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaacatag</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaccgcg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaacgttg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttttaaaga</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttttaaggg</th>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttttagcaa</th>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttttcatcc</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttttctgtg</th>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2486 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dna_one   dna_two  kmer_frequency\n",
       "aaaaactgg  0.000000  0.028261             1.0\n",
       "aaaaatttg  0.000000  0.028261             1.0\n",
       "aaaacatag  0.000000  0.028261             1.0\n",
       "aaaaccgcg  0.000000  0.028261             1.0\n",
       "aaaacgttg  0.000000  0.028261             1.0\n",
       "...             ...       ...             ...\n",
       "ttttaaaga  0.000000  0.028261             1.0\n",
       "ttttaaggg  0.028261  0.000000             1.0\n",
       "ttttagcaa  0.028261  0.000000             1.0\n",
       "ttttcatcc  0.000000  0.028261             1.0\n",
       "ttttctgtg  0.028261  0.000000             1.0\n",
       "\n",
       "[2486 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_dt.loc['kmer_frequency'] = (tfidf_dt > 0).sum()\n",
    "tfidf_dt.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(tfidf_dt.T['dna_one'].tolist()).reshape(1,-1)\n",
    "y = np.array(tfidf_dt.T['dna_two'].tolist()).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can measure how close these two DNA sequences are using, say, the cosine similarity. The cosine similarity between two vectors is defined as follows:\n",
    "\n",
    "$$\\text{cosine distance}(\\vec{a},\\vec{b}) = 1 - \\text{cosine similarity}(\\vec{a},\\vec{b}) = \\frac{\\vec{a}\\cdot\\vec{b}}{|\\vec{a}||\\vec{b}|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00323466]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "pairwise.cosine_similarity(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous bag-of-words, or CBOW\n",
    "### A word2vec method\n",
    "\n",
    "CBOW learns embeddings by a context-target mapping. i.e., you provide a context, and it is expected to provide the target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import tqdm\n",
    "import logging\n",
    "import numpy as np\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold fast to dreams, for if dreams die, life is a broken-winged bird that cannot fly.\n"
     ]
    }
   ],
   "source": [
    "responses = [\"Hold fast to dreams, for if dreams die, life is a broken-winged bird that cannot fly.\", \"No bird soars too high if he soars with his own wings.\", \"A bird does not sing because it has an answer, it sings because it has a song.\"]\n",
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, an example of context-target pairs would be:\n",
    "- \"Hold\" \"to\" ==> \"fast\"\n",
    "- \"if\" \"die\" ==> \"dreams\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../../../../../external_data/word2vec/on_the_sweeny_wire.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreProcessor:\n",
    "    def __init__(self, input_file) -> None:\n",
    "        self.input_file = input_file\n",
    "\n",
    "    def generate_tokens(self):\n",
    "\n",
    "        if isinstance(self.input_file, list):\n",
    "            for line in self.input_file:\n",
    "                yield line.split()\n",
    "        else:\n",
    "            with open(self.input_file, encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.replace(\"\\\\\", \"\")\n",
    "                    yield line.strip().split()\n",
    "\n",
    "    # def generate_corpus(self):\n",
    "\n",
    "    def build_vocab(self):\n",
    "        vlist = list(self.generate_tokens())\n",
    "        #vlist = [f for f in vlist for f in f]\n",
    "        vocab = torchtext.vocab.build_vocab_from_iterator(vlist, specials=[\"<unk>\"], min_freq=1)\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = TextPreProcessor([responses[0]])\n",
    "text_vocab = text_corpus.build_vocab()\n",
    "#list(pp.generate_tokens())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable\n",
    "\n",
    "def one_hot_encode(id, vocab_size):\n",
    "    res = [0] * vocab_size\n",
    "    res[id] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData:\n",
    "    def __init__(self, tokens, word_to_id, context_length) -> None:\n",
    "        self.tokens = tokens\n",
    "        self.word_to_id = word_to_id\n",
    "        self.context_length = context_length\n",
    "        self.XY = list(self.generate_training_tokens())\n",
    "\n",
    "\n",
    "    def one_hot_encode(self, id, vocab_size):\n",
    "        res = [0] * vocab_size\n",
    "        res[id] = 1\n",
    "        return res\n",
    "    \n",
    "    def generate_training_words(self):\n",
    "        self.data = []\n",
    "        n_tokens = len(self.tokens)\n",
    "        \n",
    "        for i in range(self.context_length, n_tokens - self.context_length):\n",
    "            context = (\n",
    "                [self.tokens[i - j - 1] for j in range(self.context_length)]\n",
    "                + [self.tokens[i + j + 1] for j in range(self.context_length)]\n",
    "            )\n",
    "            target = self.tokens[i]\n",
    "\n",
    "            yield (context, target)\n",
    "\n",
    "    def generate_training_tokens(self):\n",
    "        for generated in self.generate_training_words():\n",
    "            context, target = generated\n",
    "            context = torch.asarray([self.word_to_id[t] for t in context])\n",
    "            target = torch.asarray(self.word_to_id[target])\n",
    "\n",
    "            yield (context, target)\n",
    "        \n",
    "    def generate_training_encoded(self):\n",
    "        for generated in self.generate_training_tokens():\n",
    "            context, target = generated\n",
    "            context = [self.one_hot_encode(t, len(self.word_to_id)) for t in context]\n",
    "            #print([t for t in context])\n",
    "            target = self.one_hot_encode(target, len(self.word_to_id)) \n",
    "\n",
    "            yield (torch.asarray(context), torch.asarray(target))\n",
    "\n",
    "    def __len__(self): # the dataloader needs to know the number of observations you have\n",
    "        return len(list(self.generate_training_tokens()))\n",
    "\n",
    "    def __getitem__(self, idx): # this is what returns just one observation or one unit of training\n",
    "        return(self.XY[idx][0], self.XY[idx][1]) # essentially, I am just slicing the np array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [f for f in list(text_corpus.generate_tokens()) for f in f]\n",
    "#tokens = list(text_corpus.generate_tokens())\n",
    "word_to_id = text_corpus.build_vocab().get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TrainingData(tokens, word_to_id, context_length=2)\n",
    "#list(training_data.generate_training_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, (tensor([11,  8,  7,  6]), tensor(12)))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.__len__(), training_data.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['fast', 'Hold', 'dreams,', 'for'], 'to')"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(training_data.generate_training_words())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I can train an embedding on the onehot encoded text or on the tokens, directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HyperparametersConfig(num_epochs=3, context_size=2, embedding_dim=100, learning_rate=0.001, vocab_size=17, num_layers=3, device='cuda', bias=False)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclasses.dataclass\n",
    "class HyperparametersConfig:\n",
    "    num_epochs: int = 3\n",
    "    context_size: int = 2\n",
    "    embedding_dim: int = 100\n",
    "    learning_rate: int = 0.001\n",
    "    vocab_size: int = len(word_to_id)\n",
    "    num_layers: int = 3\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    bias: bool = False\n",
    "\n",
    "config = HyperparametersConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(num_embeddings=self.config.vocab_size, embedding_dim=self.config.embedding_dim)\n",
    "        #self.linear = nn.Linear(in_features=self.config.embedding_dim, out_features=self.config.vocab_size)\n",
    "        #self.relu_activation = nn.ReLU()\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.config.embedding_dim, self.config.vocab_size)])\n",
    "        self.linears.extend([\n",
    "            nn.ReLU(nn.Linear(self.config.vocab_size, self.config.vocab_size))\n",
    "                for i in range(1, self.config.num_layers-1)\n",
    "        ])\n",
    "        self.linears.append(nn.Linear(self.config.vocab_size, self.config.vocab_size))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.linears:\n",
    "            x = layer(x)\n",
    "        out = torch.nn.functional.log_softmax(x, dim=0)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(17, 100)\n",
       "  (linears): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=17, bias=True)\n",
       "    (1): ReLU(\n",
       "      inplace=True\n",
       "      (inplace): Linear(in_features=17, out_features=17, bias=True)\n",
       "    )\n",
       "    (2): Linear(in_features=17, out_features=17, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model = CBOW(config)\n",
    "cbow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  1,  8, 11])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = list(training_data.generate_training_tokens())\n",
    "X = torch.LongTensor(X[0][0])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3945, -0.9238, -1.3686, -1.0968, -1.1188, -1.3827, -1.3699, -1.6717,\n",
       "         -1.0653, -1.6560, -1.2714, -1.3249, -1.8447, -1.0406, -1.4343, -1.3614,\n",
       "         -1.2163],\n",
       "        [-1.1671, -1.4844, -1.4347, -1.6444, -1.6739, -1.3932, -1.2698, -1.4112,\n",
       "         -1.3617, -1.4124, -1.5198, -1.4879, -1.4759, -1.5775, -1.4362, -1.2266,\n",
       "         -1.3263],\n",
       "        [-1.5651, -2.0091, -1.4869, -1.4529, -1.3914, -1.3272, -1.6173, -1.0889,\n",
       "         -1.8360, -1.1119, -1.7909, -1.4547, -1.0009, -2.0002, -1.5587, -1.5392,\n",
       "         -1.6190],\n",
       "        [-1.4623, -1.4179, -1.2684, -1.4311, -1.4394, -1.4456, -1.3222, -1.4625,\n",
       "         -1.4284, -1.4413, -1.0966, -1.2915, -1.4032, -1.1876, -1.1601, -1.4445,\n",
       "         -1.4266]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataloader = DataLoader(training_data, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: number of observations and ground truth are 3 and 3 respectively\n",
      "batch 1: number of observations and ground truth are 3 and 3 respectively\n",
      "batch 2: number of observations and ground truth are 3 and 3 respectively\n",
      "batch 3: number of observations and ground truth are 3 and 3 respectively\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(mydataloader):\n",
    "    print(f'batch {i}: number of observations and ground truth are {batch[0].size(0)} and {batch[1].size(0)} respectively')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in config.num_epochs:\n",
    "    print(f'INFO - Epoch {epoch}')\n",
    "    for batch, data in enumerate(mydataloader):\n",
    "        cbow_model.zero_grad()\n",
    "        X, y = data\n",
    "        loss = cbow_model(X)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self): # we pass in vocab_size and embedding_dim as hyperparams\n",
    "        super(CBOW, self).__init__()\n",
    "        self.num_epochs = 3\n",
    "        self.context_size = 2 # 2 words to the left, 2 words to the right\n",
    "        self.embedding_dim = 100 # Size of your embedding vector\n",
    "        self.learning_rate = 0.001\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.vocab = TextPreProcessor().build_vocab()\n",
    "        self.word_to_ix = self.vocab.get_stoi()\n",
    "        self.ix_to_word = self.vocab.get_itos()\n",
    "        self.vocab_list = list(self.vocab.get_stoi().keys())\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f'Vocabulary size is {self.vocab_size}')\n",
    "\n",
    "        self.model = None\n",
    "        # out: 1 x embedding_dim\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim) # initialize an Embedding matrix based on our inputs\n",
    "        self.linear1 = nn.Linear(self.embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        # out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, self.vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "\n",
    "    def make_context_vector(self, context, word_to_ix) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        For each word in the vocab, find sliding windows of [-2,1,0,1,2] indexes\n",
    "        relative to the position of the word\n",
    "        :param vocab: list of words in the vocab\n",
    "        :return: torch.LongTensor\n",
    "        \"\"\"\n",
    "        idxs = [word_to_ix[w] for w in context]\n",
    "        tensor = torch.LongTensor(idxs)\n",
    "    \n",
    "    def train_model(self):\n",
    "        # Loss and optimizer\n",
    "        self.model = CBOW().to(self.device)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        loss_function = nn.NLLLoss()\n",
    "\n",
    "        logging.warning('Building training data')\n",
    "        data = self.generate_training_data()\n",
    "\n",
    "        logging.warning('Starting forward pass')\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            # we start tracking how accurate our initial words are\n",
    "            total_loss = 0\n",
    "            # for the x, y in the training data:\n",
    "            for context, target in data:\n",
    "                context_vector = self.make_context_vector(context, self.word_to_ix)\n",
    "                # we look at loss\n",
    "                log_probs = self.model(context_vector)\n",
    "                # compare loss\n",
    "                total_loss += loss_function(\n",
    "                    log_probs, torch.tensor([self.word_to_ix[target]])\n",
    "                )\n",
    "            # optimize at the end of each epoch\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            # Log out some metrics to see if loss decreases\n",
    "            logging.warning(\"end of epoch {} | loss {:2.3f}\".format(epoch, total_loss))\n",
    "            torch.save(self.model.state_dict(), self.model_path)\n",
    "            logging.warning(f'Save model to {self.model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 9\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([9, 1, 8, 11], 16),\n",
       " ([16, 9, 11, 12], 8),\n",
       " ([8, 16, 12, 7], 11),\n",
       " ([11, 8, 7, 6], 12),\n",
       " ([12, 11, 6, 14], 7),\n",
       " ([7, 12, 14, 13], 6),\n",
       " ([6, 7, 13, 2], 14),\n",
       " ([14, 6, 2, 4], 13),\n",
       " ([13, 14, 4, 3], 2),\n",
       " ([2, 13, 3, 15], 4),\n",
       " ([4, 2, 15, 5], 3),\n",
       " ([3, 4, 5, 10], 15)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Building training data\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CBOW' object has no attribute 'build_training_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/temi/Dropbox/github/website/content/blog/posts/2023-09-29-preprocessing-dna-sequences/index.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/temi/Dropbox/github/website/content/blog/posts/2023-09-29-preprocessing-dna-sequences/index.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cbow_model\u001b[39m.\u001b[39;49mtrain_model()\n",
      "\u001b[1;32m/home/temi/Dropbox/github/website/content/blog/posts/2023-09-29-preprocessing-dna-sequences/index.ipynb Cell 46\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/temi/Dropbox/github/website/content/blog/posts/2023-09-29-preprocessing-dna-sequences/index.ipynb#X65sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss_function \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNLLLoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/temi/Dropbox/github/website/content/blog/posts/2023-09-29-preprocessing-dna-sequences/index.ipynb#X65sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m logging\u001b[39m.\u001b[39mwarning(\u001b[39m'\u001b[39m\u001b[39mBuilding training data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/temi/Dropbox/github/website/content/blog/posts/2023-09-29-preprocessing-dna-sequences/index.ipynb#X65sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_training_data()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/temi/Dropbox/github/website/content/blog/posts/2023-09-29-preprocessing-dna-sequences/index.ipynb#X65sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m logging\u001b[39m.\u001b[39mwarning(\u001b[39m'\u001b[39m\u001b[39mStarting forward pass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/temi/Dropbox/github/website/content/blog/posts/2023-09-29-preprocessing-dna-sequences/index.ipynb#X65sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_epochs)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/temi/Dropbox/github/website/content/blog/posts/2023-09-29-preprocessing-dna-sequences/index.ipynb#X65sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# we start tracking how accurate our initial words are\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl-tools/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CBOW' object has no attribute 'build_training_data'"
     ]
    }
   ],
   "source": [
    "cbow_model.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
