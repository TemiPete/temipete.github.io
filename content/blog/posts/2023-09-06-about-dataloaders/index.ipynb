{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Notes on dataloaders\"\n",
    "description: \"...\"\n",
    "author: \"Temi\"\n",
    "date: 'Thurs Sep 7 2023'\n",
    "categories: [pytorch, machine learning]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "This post is still under construction; I am adding sutff as I get the time to.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel used is: dl-tools\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np, os, sys, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "print(f'Kernel used is: {os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "During training deep learning models (or any machine learning model for that matter), we try to make the most of available data. One way we do this is to supply a batch of the data to the model at a training iteration. So, if you have 5000 observations to train on, you can supply, say, 20 at a time.\n",
    "\n",
    "In addition, loading 5000 observations all at once may consume a lot of memory, especially if you have limited resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytorch` gives us a convenient way to load data in this manner by letting us create our own `dataset` objects, which are used by pytorch's `dataloader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick to create your Dataset object is that when you call the class, or attempt to get an item from the dataset, it should return one training observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a fictitious dataset, observations `X` and ground truth `Y`. They will be numpy arrays; this way I can easily manipulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 48), (100,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(100, 48) \n",
    "Y = np.random.choice([0,1], size=100)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a dataset class, `MyDataset`, that will take just one observation and ground truth at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): # will inherit from the Dataset object\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self): # the dataloader needs to know the number of observations you have\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx): # this is what returns just one observation or one unit of training\n",
    "        return(self.X[idx, : ], self.Y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can use the dataloader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyDataset at 0x11d3808d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydataset = MyDataset(X, Y)\n",
    "mydataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm that the dataset object works by doing this. I give it an index, `8` and it pulls the observations and ground truth corresponding to that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.33197901, 0.10920114, 0.32552711, 0.55107147, 0.95523331,\n",
       "        0.82203799, 0.58211899, 0.9134724 , 0.15777504, 0.74666818,\n",
       "        0.84837099, 0.53785637, 0.49206978, 0.90127475, 0.7626803 ,\n",
       "        0.89917058, 0.01275132, 0.94277784, 0.73115781, 0.76832774,\n",
       "        0.41417915, 0.83662125, 0.69430352, 0.97880989, 0.25958756,\n",
       "        0.04993424, 0.2055082 , 0.48704122, 0.55182948, 0.72521316,\n",
       "        0.58642776, 0.95965883, 0.35750039, 0.02896049, 0.34491265,\n",
       "        0.81426974, 0.47463192, 0.08679966, 0.64945759, 0.28330604,\n",
       "        0.0216591 , 0.30981423, 0.97186651, 0.95268351, 0.42557078,\n",
       "        0.15942108, 0.79952813, 0.98738138]),\n",
       " 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydataset.__getitem__(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All well and good. But I don't want to give my model one observation at a time. Although people do this, it is too small. Instead, I want to give the model a certain batch at time. `Dataloaders` help with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a `DataLoader` object and supply it the argument `batch_size`. Whenever I ask the object for training examples, it gives me `batch_size` number of observations at a time. Here I will set `batch_size` to 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataloader = DataLoader(mydataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: number of observations and ground truth are 20 and 20 respectively\n",
      "batch 1: number of observations and ground truth are 20 and 20 respectively\n",
      "batch 2: number of observations and ground truth are 20 and 20 respectively\n",
      "batch 3: number of observations and ground truth are 20 and 20 respectively\n",
      "batch 4: number of observations and ground truth are 20 and 20 respectively\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(mydataloader):\n",
    "    print(f'batch {i}: number of observations and ground truth are {batch[0].shape[0]} and {batch[1].shape[0]} respectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
