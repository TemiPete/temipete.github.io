[
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Temi",
    "section": "",
    "text": "hey"
  },
  {
    "objectID": "content/books/index.html",
    "href": "content/books/index.html",
    "title": "Temi",
    "section": "",
    "text": "I don’t necessarily endorse all of these books."
  },
  {
    "objectID": "content/books/index.html#currently-reading",
    "href": "content/books/index.html#currently-reading",
    "title": "Temi",
    "section": "currently reading",
    "text": "currently reading\n\n\nThe Second Sex by Simone de Beauvoir\n\nStarted sometime in August 2023"
  },
  {
    "objectID": "content/books/index.html#previously-read",
    "href": "content/books/index.html#previously-read",
    "title": "Temi",
    "section": "previously read",
    "text": "previously read\nin no particular order:\n\nFactotum by Charles Bukowski\nA Peace To End All Peace by David Fromkin\nA Song of Ice and Fire by George R. R. Martin\n\nA Game of Thrones\nA Clash of Kings\nA Storm of Swords\n\nThe Emperor of All Maladies by Siddhartha Mukherjee\nThen We Came To The End by Joshua Ferris\nAnimal’s People by Indra Sinha\nNever Let Me Go by Kazuo Ishiguro\nThe Buried Giant by Kazuo Ishiguro"
  },
  {
    "objectID": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html",
    "href": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html",
    "title": "Predicting swarm behaviour",
    "section": "",
    "text": "Dataset\nI will be using the aligned dataset, and will\n\nnormalize the data\nsplit the data\nmove the data to the device i.e. gpu, in this case\n\n\n\nshow code\n# standardizing the data\nmean_ = swarm_data['aligned'].iloc[:, :-1].mean(axis=0)\nstd_ = swarm_data['aligned'].iloc[:, :-1].std(axis=0)\n\nswarm_data['aligned'].iloc[:, :-1] = (swarm_data['aligned'].iloc[:, :-1] - mean_)/std_\n\n\n\n\nshow code\ndata_split = split_into_train_val_test(df=swarm_data['aligned'], return_what='tuple')\nX_train, y_train = data_split[0]\nX_test, y_test = data_split[1]\nX_valid, y_valid = data_split[2]\n\n# \nX_train = torch.tensor(X_train, dtype=torch.float64).to(device)\ny_train = torch.tensor(y_train).type(torch.LongTensor).to(device)\n\nX_test = torch.tensor(X_test, dtype=torch.float64).to(device)\ny_test = torch.tensor(y_test).type(torch.LongTensor).to(device)\n\n\n(12008, 2400) (12008,)\n(6004, 2400) (6004,)\n(6004, 2400) (6004,)\n\n\n\n\nshow code\nX_train.is_cuda\n\n\nTrue\n\n\n\n\nDefining the model\nHere I use a multilayer perceptron, or mlp.\n\n\nshow code\nclass SwarmNN(torch.nn.Module):\n    \n    '''\n    \n    '''\n    \n    def __init__(self, in_dim, out_dim, hidden_dims=[], use_bias=True):\n        '''\n        Constructs a multilayer perceptron\n        '''\n        \n        super(SwarmNN, self).__init__()\n        \n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        # assuming we don't have any hidden layer, this will just implement a linear model\n        if len(hidden_dims) == 0:\n            layers = [torch.nn.Linear(in_dim, out_dim, bias=use_bias)]\n        else:\n            layers = [torch.nn.Linear(in_dim, hidden_dims[0], bias=use_bias), torch.nn.ReLU()]\n            \n            for i, hidden_dim in enumerate(hidden_dims[:-1]):\n                layers += [torch.nn.Linear(hidden_dim, hidden_dims[i+1], bias=use_bias), torch.nn.ReLU()]\n            layers += [torch.nn.Linear(hidden_dims[-1], out_dim, bias=use_bias)]\n            \n        self.main = torch.nn.Sequential(*layers)\n        \n    def forward(self, x):\n        \n        hidden_output = self.main(x)\n        output = torch.nn.functional.softmax(hidden_output, dim=1)\n        #output = output.argmax(dim=1)\n        \n        return output\n\n\n\n\nshow code\nmodel = SwarmNN(in_dim=X_train.shape[1], out_dim=2, hidden_dims=[500, 100]).to(device)\nmodel\n\n\nSwarmNN(\n  (main): Sequential(\n    (0): Linear(in_features=2400, out_features=500, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=500, out_features=100, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=100, out_features=2, bias=True)\n  )\n)\n\n\n\n\nshow code\nfor param in model.parameters():\n    print(param.shape)\n\n\ntorch.Size([500, 2400])\ntorch.Size([500])\ntorch.Size([100, 500])\ntorch.Size([100])\ntorch.Size([2, 100])\ntorch.Size([2])\n\n\n\n\nDataloaders\nCurrently, I am not using dataloaders. I will implement this some other time.\n\n\nshow code\n# trainloader = torch.utils.data.DataLoader((X_train, y_train), batch_size=32, shuffle=True)\n# testloader = torch.utils.data.DataLoader((X_test, y_test), batch_size=32, shuffle=False)\n\n\n\n\nTraining\nHere I use: - a learning rate of 0.001 - SGD as the optimizer - cross-entropy loss\n\n\nshow code\nlr = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\nloss_fxn = torch.nn.CrossEntropyLoss()\n\n\n\n\nshow code\nnum_epochs = 500\n\nloss_tally = []\nacc_tally = []\nmetrics = {}\n\n# for the test set\ntest_loss = []\ntest_acc = []\n\nfor epoch in range(0, num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    predictions = model(X_train.float()).to(device)\n    loss = loss_fxn(predictions, y_train)\n    acc = torch.mean(1.0 * (predictions.argmax(dim=1) == y_train))\n    loss.backward()\n    optimizer.step()\n\n    #running_loss += loss.item()\n    #loss_tally.append(running_loss)\n    loss_tally.append(loss.cpu().item())\n    acc_tally.append(acc.cpu().item())\n    \n    # on the test set\n    with torch.no_grad():\n        model.eval()\n        test_pred = model(X_test.float())\n        test_l = loss_fxn(test_pred, y_test).item()\n        \n        #print(f'Test loss {epoch}: {test_l}')\n        test_loss.append(test_l)\n        test_acc.append(torch.mean(1.0 * (test_pred.argmax(dim=1) == y_test)).item())\n        \n\nmetrics['train_loss'] = loss_tally\nmetrics['train_accuracy'] = acc_tally\nmetrics['test_loss'] = test_loss\nmetrics['test_accuracy'] = test_acc\n\n\n\n\nshow code\ndef plot_my_training(metrics):\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n\n    ax[0].plot(range(len(metrics['train_loss'])), metrics['train_loss'],\n            alpha=0.8, label='Train')\n    ax[0].plot(range(len(metrics['test_loss'])), metrics['test_loss'], label='Test')\n    ax[0].set_xlabel('Iteration/Epoch')\n    ax[0].set_ylabel('Loss')\n    ax[0].legend()\n\n    ax[1].plot(range(len(metrics['train_accuracy'])), metrics['train_accuracy'],\n            alpha=0.8, label='Train')\n    ax[1].plot(range(len(metrics['test_accuracy'])), metrics['test_accuracy'], label='Test')\n    ax[1].set_xlabel('Iteration/Epoch')\n    ax[1].set_ylabel('Accuracy')\n    ax[1].legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nPerformance\n\n\nshow code\nplot_my_training(metrics)"
  },
  {
    "objectID": "content/blog/posts/2023-09-06-understanding-qvalues/index.html",
    "href": "content/blog/posts/2023-09-06-understanding-qvalues/index.html",
    "title": "Understanding pvalues, multiple testing and qvalues",
    "section": "",
    "text": "Tip\n\n\n\n\nStorey and Tibshirani’s paper is a good place to start\nHaky’s notes here are also very helpful; and this notebook was partly inspired by her notes.\n\n\n\n\nIntroduction\nWhen you do multiple testing, you want to control the false positive rate (fpr) because by nature of p-values, if there is nothing interesting going on, you still have an alpha % chance of detecting something, which is a false positive.\n\n\nSimulation 1: Null and alternative effects.\nI have a simple linear function here, where \\(X\\) has some effect, \\(\\beta\\) on \\(Y\\).\n\\[\nY \\approx \\sum X\\beta + \\epsilon\n\\]\nwhere,\n\\[\nX \\approx \\mathcal{N}(0.2,1)\\\n\\]\n\\[\n\\epsilon \\approx \\mathcal{N}(0,0.1)\n\\]\n\n\nshow code\nlibrary(qvalue)\n\n\n\n\nshow code\ndevtools::source_gist('https://gist.github.com/TemiPete/d7e37272964e5f00af4efea01d295dc8')\n\n\nℹ Sourcing gist \"d7e37272964e5f00af4efea01d295dc8\"\nℹ SHA-1 hash of file is \"d308c0c2f4eb3d58cff6b52ad22538f09bd136e0\"\n\n\n\n\nshow code\nset.seed(2023)\nnobserv <- 2000 # number of observations\n\n\n\n\nshow code\ntrue_mean <- 0.2\ntrue_sd <- 1\neps_mean <- 0\neps_sd <- 0.5\nbeta <- 0.6\nx <- rnorm(n=nobserv, mean=true_mean, sd=true_sd)\ne <- rnorm(n=nobserv, mean = eps_mean, sd=eps_sd)\n\nyalt <- x * beta + e\nplot(x, yalt, main='x has an effect on y', frame.plot=F)\n\n\n\n\n\n\n\nshow code\nynull <- rnorm(n=nobserv, mean=0, sd=beta)\nplot(x, ynull, main='x has no effect on y', frame.plot=F)\n\n\n\n\n\nNow I can simulate these tests multiple times, say, 10000\n\n\nshow code\nntests <- 10000\nX <- matrix(rnorm(n=nobserv*ntests, mean=true_mean, sd=true_sd), nrow=nobserv, ncol=ntests)\nepsilon <- matrix(rnorm(n=nobserv*ntests, mean=eps_mean, sd=eps_sd), nrow=nobserv, ncol=ntests)\ndim(X) ; dim(epsilon)\n\n\n[1]  2000 10000\n\n\n[1]  2000 10000\n\n\nshow code\nYalt <- X * beta + epsilon\nYnull <- matrix(rnorm(n=nobserv, mean=0, sd=beta), nrow=nobserv, ncol=ntests)\ndim(Yalt) ; dim(Ynull)\n\n\n[1]  2000 10000\n\n\n[1]  2000 10000\n\n\n\n\nshow code\npvec = rep(NA,ntests)\nbvec = rep(NA,ntests)\n\nfor(ss in 1:ntests)\n{\n  fit = fastlm(X[,ss], Ynull[,ss])\n  pvec[ss] = fit$pval  \n  bvec[ss] = fit$betahat\n}\n\nsummary(pvec)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000291 0.2501638 0.4916683 0.4972860 0.7465493 0.9999487 \n\n\n\n\nshow code\nsum(pvec < 0.05) ; mean(pvec < 0.05)\n\n\n[1] 481\n\n\n[1] 0.0481\n\n\nEven under the null, we find that 5% of our tests are false positives! In real life, we would think these are true effects, which is not good.\nSo, we try to control this false positive rate. There are many methods, but we can use the Bonferroni approach\n\n\nshow code\ncutoff <- 0.05/length(pvec)\nsum(pvec < cutoff) ; mean(pvec < cutoff)\n\n\n[1] 0\n\n\n[1] 0\n\n\nWith Bonferroni correction, we see that all of the tests are null, which is what should have happened in the first place. Anyway, all that was for simulation sake. I should create a set of tests, where some proportion are under the alternative i.e. true, and the rest are not i.e. null\n\n\nSimulation 2: A mixture of outcomes under the null and alternative hypothesis.\n\n\nshow code\nptrue <- 0.2 # only 20% of the tests are TRUE\nwtrue <- sample(x=c(0,1), size=ntests, replace=TRUE, prob=c(1-0.2, 0.2))\ntable(wtrue) |> prop.table()\n\n\nwtrue\n     0      1 \n0.8046 0.1954 \n\n\nI will look through wtrue. If 0, I will select the ynull at that index, otherwise, I will select the yalt\n\n\nshow code\nYboth <- matrix(NA, nrow=nobserv, ncol=ntests)\nfor(i in seq_along(wtrue)){\n    if(wtrue[i] == 1){\n        Yboth[, i] <- Yalt[, i]\n    } else {\n        Yboth[, i] <- Ynull[, i]\n    }\n}\n\ndim(Yboth)\n\n\n[1]  2000 10000\n\n\n\n\nshow code\n## run linear regression for all 10000 phenotypes in the mix of true and false associations, Ymat_mix\npvec_mix = rep(NA,ntests)\nbvec_mix = rep(NA,ntests)\nfor(ss in 1:ntests){\n  fit = fastlm(X[,ss], Yboth[,ss])\n  pvec_mix[ss] = fit$pval  \n  bvec_mix[ss] = fit$betahat\n}\nsummary(pvec_mix)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0678  0.3792  0.3992  0.6825  0.9999 \n\n\n\n\nshow code\nsum(pvec_mix < 0.05) ; mean(pvec_mix < 0.05)\n\n\n[1] 2339\n\n\n[1] 0.2339\n\n\n\n\nshow code\nhist(pvec_mix, main='')\nmtext('A simulation under the null + alt', side=3, line=1, adj = 0)\n\n\n\n\n\nWe expect 500 to be significant under the null, but we get 2339.\nSince we have more than what we expected under the null, we can assume that the remainder are gotten under the alternative. We can estimate this true discovery rate\n\\[\n\\frac{(nobserved - nexpected)}{nobserved}\n\\]\n\n\nshow code\ntdr <- ((sum(pvec_mix < 0.05)) - (0.05*ntests))/(sum(pvec_mix < 0.05))\ntdr \n\n\n[1] 0.7862334\n\n\nThe false discovery rate is 1 - tdr, which in this case is 0.2137666.\n\nAll well and good, except that our tdr here is higher than we expect. Instead we can estimate the positive false discovery rate or pFDR. Here’s how I explain this:\n\nGiven that you have found a number of tests to be significant, let’s call this tsig, and we expect at least one of these to be positive i.e. under the alternative, what is the expected number of false positives? i.e. what proportion are not true but we come out as true.\n\n\nTo break this down a little, I will start from here: Assuming you do a test to classify if a a group of people eat fruits or not, and you have this table after.\n\n\n\n\n\n\n\n\n\n\neats fruits\ndoes not eat fruits\n\n\n\n\n\nclassified: eat fruits\ntrue positives\nfalse positives\n\n\n\nclassified: does not eat fruits\nfalse negatives\ntrue negatives\n\n\n\n\n\n\n\n\n\n\nThe fpr, as mentioned earlier is:\n\\[\n\\frac{false\\ positives}{(false\\ positives\\ +\\ true\\ negatives)}\n\\]\ni.e. of all the people who don’t eat fruits, how many of them do we classify to eat fruits based on our tests?\nThe fdr then is, of all the people who we classify as eating fruits, how many of them don’t actually eat fruits?\n\\[\n\\frac{false\\ positives}{(true\\ positives\\ +\\ false\\ positives)}\n\\]\nUsing the table + idea above, I can then\n\n\n\n\n\n\n\n\n\n\n\ndoes not eat fruits\n\n\n\n\n\nclassified: eat fruits\ntrue positives\nfalse positives\n\n\n\nclassified: does not eat fruits\nfalse negatives\ntrue negatives\n\n\n\n\n\n\n\n\n\n\n\n\nshow code\nwhich(pvec_mix < 0.05)\n\n\n   [1]     8    17    18    23    26    27    28    32    33    34    39    44\n  [13]    47    49    59    62    64    68    69    76    78    83    91    95\n  [25]    96    99   109   111   112   119   120   122   123   125   126   128\n  [37]   129   139   144   149   155   156   168   169   173   179   182   190\n  [49]   201   203   206   212   215   220   224   227   241   243   248   253\n  [61]   258   263   269   271   272   275   277   281   285   286   293   294\n  [73]   298   304   305   310   311   318   323   330   333   334   336   341\n  [85]   342   349   355   356   360   362   366   367   370   374   376   382\n  [97]   383   386   387   390   396   398   405   407   409   411   417   432\n [109]   436   437   445   448   451   452   466   468   474   476   480   487\n [121]   493   498   501   505   507   509   512   513   516   518   526   530\n [133]   539   541   543   546   551   553   554   560   565   575   577   580\n [145]   583   585   594   595   596   598   622   628   633   636   639   641\n [157]   642   645   647   648   649   653   654   656   658   668   673   679\n [169]   687   690   695   699   702   706   712   713   714   722   723   725\n [181]   726   727   729   731   741   744   747   751   756   772   776   777\n [193]   778   787   789   796   799   802   811   813   818   820   825   837\n [205]   843   845   848   849   850   854   857   858   869   874   877   878\n [217]   888   897   905   909   911   916   919   924   932   935   939   948\n [229]   949   951   958   963   964   965   972   976   979   980   989   993\n [241]   998  1007  1008  1015  1021  1023  1026  1031  1034  1036  1039  1040\n [253]  1041  1042  1049  1050  1056  1061  1063  1068  1074  1076  1078  1086\n [265]  1088  1095  1097  1103  1105  1109  1110  1116  1117  1123  1125  1127\n [277]  1131  1132  1133  1134  1135  1136  1138  1143  1146  1148  1153  1159\n [289]  1161  1166  1169  1180  1181  1182  1185  1189  1192  1200  1204  1205\n [301]  1206  1210  1217  1220  1224  1230  1231  1238  1241  1243  1244  1246\n [313]  1247  1252  1254  1264  1265  1274  1275  1278  1279  1285  1290  1300\n [325]  1312  1317  1322  1327  1330  1331  1333  1341  1348  1353  1355  1357\n [337]  1360  1362  1364  1366  1367  1372  1373  1380  1393  1401  1403  1406\n [349]  1408  1418  1427  1431  1436  1445  1447  1458  1459  1462  1467  1468\n [361]  1471  1477  1491  1498  1499  1500  1506  1511  1512  1516  1520  1522\n [373]  1526  1527  1531  1537  1538  1541  1548  1557  1573  1581  1588  1590\n [385]  1600  1601  1604  1609  1610  1613  1618  1625  1626  1627  1630  1633\n [397]  1636  1641  1649  1653  1657  1658  1664  1665  1666  1670  1673  1684\n [409]  1685  1696  1698  1705  1708  1712  1713  1714  1716  1717  1723  1726\n [421]  1728  1730  1735  1736  1739  1744  1750  1751  1753  1761  1777  1788\n [433]  1792  1794  1802  1811  1812  1816  1821  1827  1832  1834  1841  1848\n [445]  1849  1851  1852  1867  1880  1881  1894  1896  1898  1902  1905  1913\n [457]  1919  1922  1934  1940  1945  1947  1948  1950  1952  1956  1960  1966\n [469]  1969  1971  1975  1982  1983  1986  1994  1998  2006  2008  2011  2013\n [481]  2015  2020  2022  2024  2028  2031  2041  2048  2052  2057  2063  2079\n [493]  2090  2093  2096  2097  2099  2106  2108  2113  2114  2117  2121  2123\n [505]  2126  2127  2129  2130  2132  2134  2135  2141  2142  2144  2152  2153\n [517]  2154  2155  2161  2165  2169  2170  2174  2175  2177  2179  2180  2181\n [529]  2182  2187  2188  2190  2191  2194  2200  2212  2214  2220  2225  2238\n [541]  2244  2246  2253  2255  2259  2263  2269  2274  2277  2278  2279  2284\n [553]  2287  2291  2296  2306  2313  2325  2326  2333  2335  2336  2339  2340\n [565]  2344  2347  2349  2354  2363  2369  2374  2375  2377  2381  2382  2383\n [577]  2384  2386  2387  2395  2396  2407  2413  2414  2415  2419  2420  2421\n [589]  2424  2425  2428  2432  2436  2438  2440  2443  2444  2453  2454  2455\n [601]  2458  2462  2464  2465  2468  2476  2480  2481  2482  2492  2498  2509\n [613]  2522  2524  2525  2527  2528  2529  2530  2534  2537  2538  2539  2548\n [625]  2553  2558  2572  2574  2578  2579  2581  2584  2587  2592  2593  2596\n [637]  2597  2606  2614  2616  2630  2631  2641  2642  2646  2655  2657  2662\n [649]  2664  2665  2666  2669  2676  2678  2679  2683  2686  2689  2694  2695\n [661]  2703  2705  2708  2721  2725  2727  2729  2734  2738  2739  2743  2745\n [673]  2755  2762  2775  2779  2790  2791  2793  2794  2796  2799  2800  2801\n [685]  2805  2811  2815  2821  2823  2830  2833  2840  2842  2846  2847  2848\n [697]  2849  2852  2863  2874  2875  2876  2880  2888  2897  2898  2901  2904\n [709]  2908  2915  2917  2937  2941  2950  2951  2952  2953  2955  2961  2964\n [721]  2981  2983  2987  2995  2998  3004  3010  3012  3014  3016  3024  3025\n [733]  3028  3038  3039  3043  3052  3054  3057  3058  3071  3077  3078  3080\n [745]  3082  3091  3095  3096  3100  3112  3116  3130  3131  3139  3144  3146\n [757]  3147  3149  3151  3167  3168  3173  3175  3196  3199  3200  3202  3203\n [769]  3205  3206  3209  3211  3218  3221  3224  3234  3235  3236  3237  3238\n [781]  3241  3242  3246  3250  3251  3256  3258  3261  3263  3277  3281  3283\n [793]  3297  3302  3317  3320  3326  3338  3339  3344  3347  3356  3362  3377\n [805]  3380  3381  3393  3396  3400  3411  3412  3416  3418  3426  3427  3430\n [817]  3433  3437  3444  3445  3446  3448  3449  3452  3463  3466  3469  3485\n [829]  3489  3499  3508  3512  3518  3521  3522  3524  3525  3535  3536  3539\n [841]  3542  3548  3555  3558  3561  3563  3564  3565  3573  3574  3591  3596\n [853]  3601  3603  3608  3609  3611  3619  3633  3635  3636  3637  3641  3644\n [865]  3648  3649  3651  3654  3658  3664  3668  3670  3672  3673  3674  3677\n [877]  3678  3685  3686  3690  3692  3694  3697  3698  3705  3706  3710  3717\n [889]  3725  3732  3740  3750  3754  3756  3759  3760  3771  3776  3778  3784\n [901]  3788  3792  3794  3795  3807  3816  3822  3833  3834  3840  3841  3842\n [913]  3843  3848  3863  3870  3872  3875  3879  3885  3888  3896  3906  3908\n [925]  3915  3916  3923  3925  3939  3941  3953  3956  3961  3969  3972  3976\n [937]  3980  3981  3982  3989  3994  4006  4010  4016  4025  4030  4031  4038\n [949]  4045  4047  4051  4052  4061  4062  4072  4080  4081  4084  4098  4104\n [961]  4105  4110  4115  4118  4119  4121  4122  4123  4125  4128  4131  4133\n [973]  4145  4148  4152  4155  4156  4160  4161  4162  4163  4172  4174  4175\n [985]  4177  4179  4180  4181  4182  4184  4189  4190  4192  4197  4204  4210\n [997]  4213  4214  4221  4240  4244  4246  4247  4248  4252  4253  4258  4264\n[1009]  4269  4272  4281  4283  4300  4308  4311  4312  4315  4317  4321  4322\n[1021]  4330  4333  4334  4341  4342  4343  4344  4350  4361  4364  4385  4387\n[1033]  4388  4393  4398  4401  4404  4406  4407  4417  4423  4429  4430  4436\n[1045]  4437  4439  4444  4449  4453  4461  4468  4470  4472  4473  4477  4480\n[1057]  4482  4484  4487  4488  4492  4497  4507  4516  4525  4526  4533  4539\n[1069]  4542  4552  4557  4561  4562  4563  4565  4569  4586  4589  4593  4594\n[1081]  4604  4617  4618  4622  4627  4629  4645  4649  4654  4658  4661  4665\n[1093]  4666  4673  4675  4680  4691  4692  4693  4696  4697  4700  4704  4708\n[1105]  4721  4727  4737  4741  4744  4746  4749  4753  4756  4757  4759  4765\n[1117]  4771  4779  4785  4796  4800  4802  4803  4805  4809  4813  4814  4819\n[1129]  4820  4824  4832  4837  4839  4841  4842  4846  4851  4858  4865  4872\n[1141]  4877  4878  4881  4882  4888  4889  4890  4891  4894  4898  4902  4906\n[1153]  4908  4910  4911  4914  4916  4919  4922  4923  4928  4933  4935  4943\n[1165]  4948  4950  4953  4955  4982  4985  4990  4995  4997  4999  5001  5009\n[1177]  5011  5012  5014  5017  5018  5019  5023  5024  5036  5040  5047  5052\n[1189]  5053  5055  5057  5061  5062  5071  5076  5077  5079  5080  5082  5085\n[1201]  5090  5093  5097  5102  5105  5120  5132  5140  5142  5150  5154  5165\n[1213]  5172  5176  5180  5184  5189  5197  5211  5216  5217  5228  5231  5234\n[1225]  5235  5239  5242  5246  5247  5248  5256  5260  5262  5264  5265  5272\n[1237]  5273  5286  5287  5290  5302  5303  5306  5309  5327  5329  5341  5343\n[1249]  5346  5350  5361  5366  5383  5385  5386  5387  5390  5399  5400  5401\n[1261]  5409  5411  5413  5414  5415  5422  5426  5432  5440  5442  5446  5451\n[1273]  5459  5463  5465  5470  5477  5492  5495  5496  5498  5500  5505  5509\n[1285]  5515  5521  5523  5524  5525  5528  5529  5530  5531  5549  5552  5569\n[1297]  5571  5577  5581  5583  5589  5592  5595  5599  5601  5603  5604  5611\n[1309]  5615  5622  5624  5626  5628  5635  5636  5640  5644  5646  5649  5654\n[1321]  5657  5659  5680  5681  5691  5693  5695  5704  5708  5712  5716  5729\n[1333]  5735  5737  5744  5751  5753  5758  5761  5763  5764  5777  5781  5783\n[1345]  5786  5788  5789  5791  5792  5798  5807  5818  5821  5824  5829  5833\n[1357]  5844  5849  5851  5858  5861  5880  5889  5895  5896  5906  5907  5916\n[1369]  5923  5928  5938  5948  5951  5955  5963  5964  5967  5968  5969  5973\n[1381]  5980  5984  5985  5990  5991  5993  5998  6000  6007  6008  6009  6016\n[1393]  6018  6027  6034  6037  6044  6045  6046  6054  6055  6058  6061  6063\n[1405]  6067  6073  6075  6077  6082  6091  6095  6101  6103  6105  6109  6111\n[1417]  6116  6118  6119  6135  6139  6141  6143  6146  6153  6157  6160  6169\n[1429]  6175  6176  6178  6182  6186  6187  6188  6190  6193  6196  6197  6198\n[1441]  6213  6218  6220  6221  6222  6223  6226  6230  6239  6241  6242  6246\n[1453]  6248  6252  6254  6256  6261  6262  6273  6277  6280  6281  6286  6289\n[1465]  6291  6299  6302  6307  6311  6314  6315  6327  6338  6340  6342  6347\n[1477]  6348  6349  6354  6358  6367  6377  6380  6383  6390  6392  6396  6398\n[1489]  6399  6401  6413  6419  6423  6426  6427  6433  6441  6442  6444  6445\n[1501]  6448  6450  6453  6455  6456  6462  6469  6474  6481  6483  6484  6486\n[1513]  6489  6490  6491  6497  6500  6503  6509  6512  6517  6520  6525  6526\n[1525]  6533  6534  6535  6536  6538  6541  6555  6560  6565  6567  6571  6572\n[1537]  6576  6579  6580  6582  6589  6590  6593  6596  6597  6602  6608  6610\n[1549]  6612  6615  6618  6621  6633  6638  6641  6646  6652  6654  6663  6668\n[1561]  6669  6672  6675  6680  6682  6683  6684  6687  6691  6695  6697  6699\n[1573]  6704  6714  6717  6718  6721  6723  6727  6734  6737  6743  6746  6747\n[1585]  6765  6769  6773  6781  6783  6788  6790  6792  6796  6800  6807  6814\n[1597]  6815  6823  6829  6836  6839  6848  6851  6852  6855  6857  6864  6865\n[1609]  6867  6872  6883  6885  6886  6888  6891  6893  6902  6912  6918  6929\n[1621]  6931  6936  6939  6944  6945  6947  6961  6966  6968  6971  6972  6978\n[1633]  6980  6995  6998  6999  7003  7006  7012  7014  7016  7019  7023  7029\n[1645]  7036  7046  7047  7049  7062  7064  7067  7070  7072  7077  7078  7079\n[1657]  7083  7089  7093  7094  7099  7102  7103  7104  7109  7121  7122  7125\n[1669]  7127  7132  7134  7141  7146  7149  7152  7156  7159  7163  7171  7174\n[1681]  7179  7188  7191  7195  7196  7201  7203  7209  7219  7221  7222  7224\n[1693]  7225  7232  7235  7239  7241  7242  7247  7248  7256  7257  7262  7270\n[1705]  7277  7282  7293  7301  7306  7309  7314  7316  7317  7319  7321  7327\n[1717]  7330  7335  7336  7348  7349  7352  7359  7363  7366  7370  7376  7377\n[1729]  7381  7383  7385  7386  7387  7394  7395  7396  7404  7405  7407  7409\n[1741]  7411  7422  7426  7432  7439  7440  7441  7444  7452  7456  7460  7463\n[1753]  7473  7476  7495  7496  7499  7502  7504  7507  7508  7516  7518  7521\n[1765]  7522  7525  7530  7531  7535  7545  7548  7551  7554  7555  7561  7578\n[1777]  7581  7583  7585  7588  7591  7596  7599  7608  7614  7616  7621  7625\n[1789]  7626  7633  7634  7640  7641  7642  7643  7645  7646  7649  7652  7654\n[1801]  7655  7659  7663  7666  7667  7669  7673  7681  7682  7684  7687  7690\n[1813]  7692  7694  7697  7706  7708  7710  7712  7713  7715  7719  7724  7727\n[1825]  7732  7733  7734  7737  7741  7744  7745  7746  7751  7756  7758  7763\n[1837]  7773  7774  7785  7788  7791  7793  7794  7797  7799  7806  7819  7822\n[1849]  7824  7828  7837  7840  7849  7861  7867  7872  7876  7879  7883  7891\n[1861]  7892  7894  7899  7905  7908  7910  7917  7929  7930  7932  7938  7940\n[1873]  7941  7945  7961  7963  7970  7972  7975  7978  7983  7986  7994  7995\n[1885]  8002  8008  8016  8022  8024  8025  8026  8029  8031  8032  8034  8035\n[1897]  8038  8039  8044  8065  8068  8070  8071  8074  8077  8081  8082  8086\n[1909]  8093  8096  8098  8099  8101  8109  8114  8116  8119  8127  8129  8135\n[1921]  8139  8145  8149  8154  8155  8157  8162  8163  8173  8174  8175  8179\n[1933]  8181  8186  8189  8194  8213  8218  8220  8227  8228  8233  8234  8235\n[1945]  8238  8246  8247  8248  8255  8263  8264  8266  8268  8269  8272  8281\n[1957]  8288  8297  8300  8315  8317  8329  8332  8334  8339  8342  8361  8362\n[1969]  8370  8372  8373  8375  8377  8379  8380  8383  8386  8388  8395  8399\n[1981]  8401  8403  8405  8407  8409  8411  8418  8425  8427  8431  8434  8435\n[1993]  8436  8437  8439  8446  8453  8459  8460  8462  8466  8474  8478  8481\n[2005]  8484  8488  8493  8496  8499  8513  8514  8518  8522  8525  8530  8534\n[2017]  8536  8538  8541  8549  8552  8553  8554  8558  8562  8573  8574  8587\n[2029]  8588  8592  8593  8594  8597  8607  8613  8622  8626  8630  8640  8653\n[2041]  8654  8663  8664  8680  8689  8690  8694  8701  8702  8703  8704  8707\n[2053]  8716  8723  8729  8731  8735  8740  8742  8744  8748  8755  8756  8758\n[2065]  8760  8763  8765  8766  8791  8793  8797  8803  8807  8809  8811  8812\n[2077]  8813  8818  8819  8823  8824  8829  8830  8837  8840  8845  8851  8852\n[2089]  8853  8856  8857  8859  8861  8869  8876  8880  8894  8904  8906  8928\n[2101]  8929  8931  8932  8939  8948  8953  8954  8956  8957  8961  8964  8971\n[2113]  8972  8988  8990  8994  9001  9007  9015  9023  9032  9043  9056  9057\n[2125]  9064  9075  9077  9080  9081  9091  9095  9096  9097  9098  9111  9112\n[2137]  9115  9127  9129  9135  9138  9139  9141  9144  9148  9150  9152  9154\n[2149]  9160  9170  9177  9178  9185  9187  9196  9197  9206  9213  9214  9225\n[2161]  9229  9231  9237  9238  9240  9241  9254  9256  9257  9260  9261  9265\n[2173]  9272  9277  9280  9283  9291  9293  9294  9298  9301  9302  9304  9306\n[2185]  9311  9312  9316  9323  9324  9326  9327  9336  9339  9340  9343  9346\n[2197]  9350  9351  9354  9360  9362  9363  9365  9367  9368  9370  9373  9375\n[2209]  9382  9383  9405  9408  9409  9411  9418  9426  9440  9443  9452  9457\n[2221]  9476  9482  9499  9500  9504  9507  9510  9512  9513  9514  9520  9530\n[2233]  9533  9534  9542  9548  9551  9556  9560  9566  9567  9577  9578  9579\n[2245]  9587  9588  9595  9599  9603  9605  9608  9610  9612  9617  9630  9631\n[2257]  9633  9635  9636  9643  9644  9645  9651  9653  9658  9659  9667  9673\n[2269]  9679  9684  9695  9701  9715  9719  9725  9726  9738  9739  9742  9749\n[2281]  9750  9751  9759  9761  9763  9770  9777  9786  9788  9791  9794  9796\n[2293]  9798  9800  9807  9814  9826  9827  9830  9847  9850  9851  9855  9862\n[2305]  9864  9867  9874  9883  9887  9899  9900  9901  9908  9910  9911  9915\n[2317]  9916  9918  9926  9929  9930  9941  9942  9943  9945  9950  9953  9954\n[2329]  9960  9962  9967  9969  9972  9975  9976  9977  9981  9998 10000\n\n\nBecause we simulated the data, we know that 1954 tests are under the alternative, and the rest, 8046, should be under the null. But after our tests, we have found 2339 to be under the alternative and 7661 to be under the null. So, there are some 385 that have been misclassified as under the alternative when they are not, and some -385\n\n\nshow code\ntp <- sum(wtrue == 1 & pvec_mix < 0.05)\ntn <- sum(wtrue == 0 & pvec_mix >= 0.05)\nfp <- sum(wtrue == 1 & pvec_mix >= 0.05)\nfn <- sum(wtrue == 0 & pvec_mix < 0.05)\n\n\nAlternatively, table(pvec_mix > 0.05, wtrue) will yield the same result.\n\n\n\n\nalternative\nnull\ntotal\n\n\n\n\nclassified: alternative\n1954\n385\n2339\n\n\nclassified: null\n0\n7661\n7661\n\n\ntotal\n1954\n8046"
  },
  {
    "objectID": "content/blog/posts/2023-08-29-simulating-populations/index.html",
    "href": "content/blog/posts/2023-08-29-simulating-populations/index.html",
    "title": "Simulating discrete and continuous populations",
    "section": "",
    "text": "Introduction\nOftentimes, we want to simulate populations in order to do some analysis. In this demo, I go over a, perhaps, crude way to mimic discrete and continuous populations. Before simulating, we need to know what a population is. In genetics, a population is a grouping of individuals based on some similarity in allele frequencies between them. Of course, there are many other definitions of what a population is. In this demo, I will use the definition based on allele frequencies.\nOne way to generate allele frequencies that mimic that seen in typical populations is to use the \\(F_{ST}\\), AKA the fixation index, AKA F-statistic. \\(F_{ST}\\) measures population differentiation as a result of genetic structure. Values range between 0 and 1, and higher values mean that two populations are highly similar. There are many definitions of \\(F_{ST}\\), depending on what you are discussing - and I am still learning about most of them. In this case, however, I am primarily concerned with allele frequency-related definitions.\n\n\nSimulating discrete populations\nTo simulate populations based on the definition of \\(F_{ST}\\), one thing we can do is to simulate an ancestral population’s allele frequencies. This is similar to having a founder population from which every other population descends. Afterwards, we can simulate different populations from this ancestral population. This follows from the Balding-Nichols model.\n\\[\\begin{align}\nBeta(\\frac{1-F}{F}p, \\frac{1-F}{F}(1-p))\n\\end{align}\\]\nAssume that we have 500 alleles/loci, and 4 populations, and we intend to simulate 1000 individuals from each of these populations. We can define our \\(F_{ST}\\) to be, say, 0.09 between populations 1 and 2, 0.19 between populations 1 and 2, and population 3, and 0.4 between all these populations, and population 4. Therefore, populations 1 and 2 will be closely related, and different from population 3, and all of them will be different from population 4.\n\n\nshow code\nM <- 500 # number of alleles or SNPs\nn_pops <- 4 # number of populations\nN <- 1000 # 20 individuals in each n_pops\nf_pop12 <- 0.09\nf_pop3 <- 0.19 # a third, distant, unrelated population\nf_pop4 <- 0.4 # a fourth population\n\n\nWhen we make plots of the allele frequencies of these populations, we expect high correlations between populations 1 and 2, and not-so-high correlations when we compare with the other populations 3 and 4.\nWe can generate some random minor allele frequencies (MAFs) to be the ancestral allele frequencies, and generate independent draws from the distribution of the ancestral allele frequencies, based on the equation above.\n\n\nshow code\nancestral_allele_freqs <- runif(M, 0.01, 0.5)\nsh1 <- ((1-f_pop12)/f_pop12)*ancestral_allele_freqs\nsh2 <- ((1-f_pop12)/(f_pop12))*(1-ancestral_allele_freqs)\npop1_allele_freqs <- rbeta(M, shape1 = sh1, shape2 = sh2)\npop2_allele_freqs <- rbeta(M, shape1 = sh1, shape2 = sh2)\npop3_allele_freqs <- rbeta(M, shape1 = ((1-f_pop3)/f_pop3)*ancestral_allele_freqs, shape2=((1-f_pop3)/(f_pop3))*(1-ancestral_allele_freqs))\npop4_allele_freqs <- rbeta(M, shape1 = ((1-f_pop4)/f_pop4)*ancestral_allele_freqs, shape2=((1-f_pop4)/(f_pop4))*(1-ancestral_allele_freqs))\n\nall_allele_freqs <- list(pop1_allele_freqs, pop2_allele_freqs, pop3_allele_freqs, pop4_allele_freqs)\nnames(all_allele_freqs) <- paste0('population_', 1:4)\n\n\n\nFirst, how does the expected value of the distribution of allele frequency changes as a factor of the \\(F_{ST}\\)? This should give us an idea of what to expect when we simulate allele frequencies.\n\n\nshow code\n# generate many Fst values\nfst_vector <- round(seq(0.01, 0.99, length.out=20), 2) # 20 fsts ranging from 0.01 to 0.99\nper_fst <- lapply(fst_vector, function(each_fst){\n  \n  sh1 <- ((1-each_fst)/each_fst)*ancestral_allele_freqs\n  sh2 <- ((1-each_fst)/(each_fst))*(1-ancestral_allele_freqs)\n  temp_allele_frq <- rbeta(M, shape1 = sh1, shape2 = sh2)\n  deviation <- cor(ancestral_allele_freqs, temp_allele_frq) #|> suppressWarnings()\n  return(deviation)\n})\n\n\n\n\nshow code\ncor_deviation <- per_fst |> unlist()\n\nplot(cor_deviation, frame.plot=F, xaxt='n', yaxt='n', type='b', xlab = expression(F[ST]), ylab=expression(R^2))\naxis(1, at=rep(1:length(cor_deviation), by=1), labels = fst_vector, las=2)\naxis(2, at=seq(0, 1, 0.1))\n\n\n\n\n\nWe see that as the \\(F_{ST}\\) increases, the correlation between the ancestral allele frequency and the generated allele frequency reduces.\n\n\nshow code\nlayout(matrix(c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = T))\nnames_pop <- names(all_allele_freqs)\nfor(i in 1:4){\n    for(j in 1:4){\n        if(i == j){ # we don't want to plot pop1 vs pop1 e.t.c.\n            next\n        } else if (i < j) {\n            #cor_ <- round(cor(all_allele_freqs[[i]], all_allele_freqs[[j]]), 2)\n            plot(all_allele_freqs[[i]], all_allele_freqs[[j]], xlab = names_pop[i], ylab = names_pop[j], frame.plot = F)\n            #abline(a=0, b=cor_, col='red')\n            #mtext(cor_)\n        }\n    }\n}\nmtext('Scatterplot of allele frequencies for each population', outer = T)\n\n\n\n\n\nThe plot is approximately (roughly?) what we expect!\n\n\n\nSimulating genotypes using these allele frequencies\nGiven these allele frequencies, we can simulate genotypes from each of these populations by sampling from a binomial distribution.\n\\[\\begin{align}\nG_{k_{n}} \\sim Binom(n, p_{k})\n\\end{align}\\]\nwhere \\(n\\) is the number of success, and \\(p\\) is the probability of a success. Here, \\(n = 2\\) because we want 2 alleles per loci, and \\(p\\) is the allele frequency at that loci.\n\n\nshow code\ngenotypes <- lapply(seq_along(all_allele_freqs), function(i){\n    matrix(data=rbinom(n=N*M, size=2, prob = all_allele_freqs[[i]]), nrow = N, ncol = M, byrow = T)\n})\n\ngenotypes <- do.call(rbind, genotypes)\nshuffle <- sample(1:nrow(genotypes)) # I will shuffle the data\ngenotypes <- genotypes[shuffle, ]\npops <- rep(names(all_allele_freqs), each=N)\npops <- pops[shuffle] # shuffle pops too, since it tells me what populations the genotypes come from\n\ncat('Here is what the genotypes look like\\n')\n\n\nHere is what the genotypes look like\n\n\nshow code\ngenotypes[1:5, 1:5]\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    1    0    1    0    0\n[3,]    0    1    0    0    0\n[4,]    1    0    0    0    0\n[5,]    0    0    0    0    0\n\n\nshow code\ncat('And here is what the population designation looks like \\n')\n\n\nAnd here is what the population designation looks like \n\n\nshow code\npops[1:5]\n\n\n[1] \"population_1\" \"population_3\" \"population_2\" \"population_3\" \"population_4\"\n\n\nNext, we can calculate the principal components (PCs) of this data, and plot PC1 vs PC2, to see where the most variability lies in the data\n\n\nshow code\n# first change the row names of the genotype data to reflect the populations\n#rownames(genotypes) <- pops\n# I need to make sure that there are some variations in the SNPS - if there aren't any, I should remove them\npca_genotypes <- genotypes[ , which(apply(genotypes, 2, var) != 0)]\n# PCA\npca_dt <- prcomp(pca_genotypes, scale.=T, center=T)\npca_dt$x[1:5, 1:5]\n\n\n            PC1       PC2        PC3         PC4        PC5\n[1,]   3.263826 -7.849732 -6.9217690 -2.66479261 -3.9900565\n[2,]   6.544296  8.367773 -1.7339642 -0.06842935  2.8759937\n[3,]   6.086323 -4.769098  6.4930496 -1.81845701 -1.7244237\n[4,]   5.730047  7.073629 -1.7772414 -1.97970506 -0.2309777\n[5,] -12.399378  1.580702  0.6451658  0.24943503  1.3446776\n\n\n\n\nshow code\n# colors\nmycolors <- colorRampPalette(RColorBrewer::brewer.pal(8, \"Dark2\"))(n_pops)\n#mycolors <- c('red', 'orange', 'green', 'blue')\ncolor_coding <- factor(pops, labels = mycolors)\nplot(pca_dt$x[, 'PC1'], pca_dt$x[, 'PC2'], bg=as.character(color_coding), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of 4 discrete populations')\nlegend('topleft', legend=names_pop, pch=c(21, 21, 21, 21), pt.bg=mycolors, bty = 'n', xpd=NA)\n\n\n\n\n\nWe can look at populations 1 and 2 only, and see if there is any difference between them.\n\n\nshow code\npop_names <- c('population_1', 'population_2')\nidx <- which(pops %in% pop_names)\npca_12 <- pca_dt$x[idx, ]\ncolor_coding <- factor(pops[idx], labels = mycolors[1:2])\nplot(pca_12[, 'PC1'], pca_12[, 'PC2'], bg=as.character(color_coding), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of the 2 similar populations 1 and 2')\nlegend('topleft', legend=pop_names, pch=c(21, 21), pt.bg=mycolors[1:2], bty = 'n', xpd=NA)\n\n\n\n\n\n\n\nSimulating continuous populations\nWhat I have done so far, is to simulate a discrete population of individuals. Think of it as people living in Mars, Earth, and Jupiter, and who have never intermingled since time immemorial. Population 3 is faraway from population 4. However, true human populations are not this way. How can we simulate a truly continuous population?\nFirst, notice that I am only simulating allele frequencies. Therefore, I can put a prior on my earlier-defined beta distribution to model the probability that an individual is from population \\(m\\). In other words, I am trying to estimate what percentage of an individual’s alleles might have come from a certain population \\(m\\). I will call this random variable, \\(Q\\).\n\\[\\begin{align}\nQ \\sim Dirichlet(\\alpha)\n\\end{align}\\]\nA trick I can use is to take an individual, ask how much of their genome is shared among each population I have, select that proportion of allele frequencies, and simulate their genotypes based on the selection for each population. To do this, I need to know how to distribute the \\(\\alpha\\) parameter used in the Dirichlet distribution. A trick I can use is to use the allele frequency distribution in the entire population. I will call this distribution the population grade distribution.\nEarlier, I simulated some allele frequencies all_allele_freqs\n\n\nshow code\nss <- sapply(all_allele_freqs, sum)\npopulation_grade <- ss/sum(ss)\n#population_grade <- rand_vect_cont(4, 1)\npopulation_grade\n\n\npopulation_1 population_2 population_3 population_4 \n   0.2462436    0.2509962    0.2512640    0.2514962 \n\n\nThen I can use this as a parameter to the Dirichlet distribution to simulate the percentage of an individual’s genotypes that is shared across all 4 populations.\n\n\nshow code\nq_matrix <- gtools::rdirichlet(N * n_pops, alpha = population_grade) # I want to simulate, N individuals per population\n\n\nHere is an illustration. Assuming that I am looking at the first individual\n\n\nshow code\n# assuming one individual\nid1 <- q_matrix[1, ]\nq_counts <- floor(id1 * M) # number of loci shared\nq_counts\n\n\n[1]   0  43 340 116\n\n\nThe assumption here is that this individual shares 0 loci with population 1, 43 with population 2, 340 with population 3, and 116 loci with population 4, and 3 independent loci not share with any population. The sum is 499 (because I rounded down), but I don’t want this to be this case. Therefore, I will share the remaining loci with the highest number already available, so that the sum is 500.\n\n\nshow code\nremainder_ <- M - sum(q_counts)\nq_counts[which.max(q_counts)] <- q_counts[which.max(q_counts)] + remainder_\nq_counts\n\n\n[1]   0  43 341 116\n\n\nNow, this sums to 500.\nPutting this into a reusable function…\n\n\nshow code\nq_counts_fxn <- function(q_matrix, M){\n    apply(q_matrix, 1, function(each_row){\n        temp <- floor(each_row * M)\n        remainder_ <- M - sum(temp)\n        temp[which.max(temp)] <- temp[which.max(temp)] + remainder_\n        temp\n    }) |> t()\n}\n\nq_counts <- q_counts_fxn(q_matrix, M)\nq_counts[1:5, ] # for the first 5 individuals\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    0   43  341  116\n[2,]  385    0   43   72\n[3,]   10   16  364  110\n[4,]    0  370   12  118\n[5,]  103  256    0  141\n\n\n\n\nshow code\ngenotypes <- matrix(NA, nrow = N*n_pops, ncol = M) # remember, N individuals per population\n\nfor(q in 1:nrow(q_counts)){\n    pop_down <- 1\n    m_loci <- 1:M\n    i_loci <- list() # this list contains the actual loci an individual has shared among the other populations\n    while(pop_down <= ncol(q_counts)){\n        # sample pop 1 loci\n        pop_loci <- sample(m_loci, size=q_counts[q, pop_down], replace = F)\n        if(length(pop_loci) == 0){\n            m_loci <- m_loci[m_loci != 0]\n            i_loci[[pop_down]] <- 0\n        } else {\n            m_loci <- m_loci[!m_loci %in% pop_loci]\n            i_loci[[pop_down]] <- pop_loci\n        }\n        pop_down <- pop_down + 1\n    }\n    \n    g_loci <- lapply(seq_along(i_loci), function(i){\n        temp <- all_allele_freqs[[i]][i_loci[[i]]]\n        rbinom(n = length(temp), size = 2, prob = temp)\n    })\n\n    gi <- vector(mode='numeric', length = M)\n    for(i in 1:n_pops){\n        gi[i_loci[[i]]] <- g_loci[[i]]\n    }\n    \n    genotypes[q, ] <- gi\n    \n}\n\n\n\n\nshow code\npop <- paste0('population_', apply(q_counts, 1, which.max))\n# first change the row names of the genotype data to reflect the populations\nrownames(genotypes) <- pop\n# I need to make sure that there are some variations in the SNPS - if there aren't any, I should remove them\npca_genotypes <- genotypes[ , which(apply(genotypes, 2, var) != 0)]\n# PCA\npca_dt <- prcomp(pca_genotypes, scale.=T, center=T)\npca_dt$x[1:5, 1:5]\n\n\n                    PC1       PC2         PC3        PC4        PC5\npopulation_3  1.7973144 -5.028205 -0.35698361  2.6108432 -0.5580345\npopulation_1 -0.4990041  4.185811 -5.96843755  0.1924663  0.4836813\npopulation_3  1.9174774 -5.916136  0.09453596  1.5513878 -0.3693294\npopulation_2  1.6417866  2.424741  6.31743092 -1.8844746 -0.3070795\npopulation_2 -0.6605363  3.049585  0.82280376  1.9657889  0.9151204\n\n\n\n\nshow code\nmycolors <- colorRampPalette(RColorBrewer::brewer.pal(8, \"Dark2\"))(n_pops)\ncolor_coding <- factor(rownames(pca_dt$x), labels = mycolors)\nplot(pca_dt$x[, 'PC1'], pca_dt$x[, 'PC2'], bg=color_coding |> as.character(), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of 4 continuous populations')\nlegend('topleft', legend=names_pop, pch=c(21, 21, 21, 21), pt.bg=mycolors, bty = 'n', xpd = NA)"
  },
  {
    "objectID": "content/blog/posts/2023-09-03-parallelization-in-R/index.html",
    "href": "content/blog/posts/2023-09-03-parallelization-in-R/index.html",
    "title": "Parallelization in R",
    "section": "",
    "text": "When running computationally-heavy tasks in R, it can be useful to parallelize your codes/runs. In that vein, this is a really good blogpost to read to understand when and how to parallelize. And here is another one.\nR offers many ways to do this. Usually, I prefer using some libraries.\n\n\nshow code\nlibrary(doParallel)\n\n\nLoading required package: foreach\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n\nshow code\nlibrary(foreach)\nlibrary(parallel)\n\n\n\n\nAssuming we want to apply a function over the rows of a matrix. This function will take each row, divide the numbers in that row by the index of that row in the matrix and return a newly-created matrix of the same shape.\n\n\nshow code\nset.seed(2022)\nmyMatrix <- matrix(sample(1:50000, size=300*500, replace=T), nrow=300, ncol=500)\n\ndim(myMatrix)\n\n\n[1] 300 500\n\n\n\n\nFirst, I will time this function with lapply loops. lapply is shipped with base R and is a parallel form of a regular for loop.\n\n\nshow code\nlapply_fxn <- function(){\n  applyMatrix <- lapply(1:nrow(myMatrix), function(i){\n    out <- myMatrix[i, ] / (i)\n    return(out)\n  })\n  applyMatrix <- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(lapply_fxn())\n\n\n   user  system elapsed \n  0.004   0.000   0.004 \n\n\n\n\n\nNext, let’s take advantage of the cores, this time using mclapply\n\n\nshow code\nmclapply_fxn <- function(){\n  applyMatrix <- parallel::mclapply(1:nrow(myMatrix), function(i){\n    out <- myMatrix[i, ] / (i)\n    return(out)\n  }, mc.cores = 12) # using 7 cores\n  \n  applyMatrix <- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(mclapply_fxn())\n\n\n   user  system elapsed \n  0.043   0.084   0.032 \n\n\nWe see that lapply is faster. This is because there is an overhead to distributing these runs and collecting their results when using mclapply\n\n\n\nHere, I will use the foreach but without a parallel back-end - this is akin to a sequential run, just like lapply or a regular for loop\n\n\nshow code\nsystem.time({\n  outputMatrix <- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %do% {\n    out <- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.071   0.005   0.078 \n\n\n\n\n\nHere, I will use the foreach but with a parallel back-end. The parallel back-end is a cluster of cores If you are familiar with multiprocessing in python, it is equivalent to multiprocessing.Pool\nFirst we need to register a parallel back-end\nWe can query how many cores we have on this computer\n\n\nshow code\nparallel::detectCores()\n\n\n[1] 8\n\n\nI will register 7 cores\n\n\nshow code\nnum_clusters <- 7 #- 5 # 12 - 5\ndoParallel::registerDoParallel(num_clusters)\n\ncat('Registering', num_clusters, 'clusters for a parallel run\\n')\n\n\nRegistering 7 clusters for a parallel run\n\n\n\n\nshow code\nsystem.time({\n  outputMatrix <- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %dopar% {\n    out <- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.085   0.070   0.081 \n\n\nshow code\n# stop the cluster\ndoParallel::stopImplicitCluster()\n\n\nHere we see that lapply is much faster. Of course that is because of all the overheads and all that.\nNext, I will show to use the various makeCluster() options."
  },
  {
    "objectID": "content/blog/posts/2023-09-06-about-dataloaders/index.html",
    "href": "content/blog/posts/2023-09-06-about-dataloaders/index.html",
    "title": "Notes on dataloaders",
    "section": "",
    "text": "show code\nimport torch\nimport numpy as np, os, sys, pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\nprint(f'Kernel used is: {os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))}')\n\n\nFontconfig warning: ignoring UTF-8: not a valid region tag\n\n\nKernel used is: dl-tools\n\n\n\nIntroduction\nDuring training deep learning models (or any machine learning model for that matter), we try to make the most of available data. One way we do this is to supply a batch of the data to the model at a training iteration. So, if you have 5000 observations to train on, you can supply, say, 20 at a time.\nIn addition, loading 5000 observations all at once may consume a lot of memory, especially if you have limited resources.\npytorch gives us a convenient way to load data in this manner by letting us create our own dataset objects, which are used by pytorch’s dataloader\n\n\nClass Dataset\n\n\nshow code\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\n\nThe trick to create your Dataset object is that when you call the class, or attempt to get an item from the dataset, it should return one training observation.\nI will create a fictitious dataset, observations X and ground truth Y. They will be numpy arrays; this way I can easily manipulate them.\n\n\nshow code\nX = np.random.rand(100, 48) \nY = np.random.choice([0,1], size=100)\nX.shape, Y.shape\n\n\n((100, 48), (100,))\n\n\nI create a dataset class, MyDataset, that will take just one observation and ground truth at a time\n\n\nshow code\nclass MyDataset(Dataset): # will inherit from the Dataset object\n    def __init__(self, X, Y):\n        self.X = X\n        self.Y = Y\n    \n    def __len__(self): # the dataloader needs to know the number of observations you have\n        return self.X.shape[0]\n\n    def __getitem__(self, idx): # this is what returns just one observation or one unit of training\n        return(self.X[idx, : ], self.Y[idx])\n\n\nNow I can use the dataloader object\n\n\nshow code\nmydataset = MyDataset(X, Y)\nmydataset\n\n\n<__main__.MyDataset at 0x11d3808d0>\n\n\nYou can confirm that the dataset object works by doing this. I give it an index, 8 and it pulls the observations and ground truth corresponding to that index.\n\n\nshow code\nmydataset.__getitem__(8)\n\n\n(array([0.33197901, 0.10920114, 0.32552711, 0.55107147, 0.95523331,\n        0.82203799, 0.58211899, 0.9134724 , 0.15777504, 0.74666818,\n        0.84837099, 0.53785637, 0.49206978, 0.90127475, 0.7626803 ,\n        0.89917058, 0.01275132, 0.94277784, 0.73115781, 0.76832774,\n        0.41417915, 0.83662125, 0.69430352, 0.97880989, 0.25958756,\n        0.04993424, 0.2055082 , 0.48704122, 0.55182948, 0.72521316,\n        0.58642776, 0.95965883, 0.35750039, 0.02896049, 0.34491265,\n        0.81426974, 0.47463192, 0.08679966, 0.64945759, 0.28330604,\n        0.0216591 , 0.30981423, 0.97186651, 0.95268351, 0.42557078,\n        0.15942108, 0.79952813, 0.98738138]),\n 1)\n\n\n\n\nDataloader\nAll well and good. But I don’t want to give my model one observation at a time. Although people do this, it is too small. Instead, I want to give the model a certain batch at time. Dataloaders help with this.\nI create a DataLoader object and supply it the argument batch_size. Whenever I ask the object for training examples, it gives me batch_size number of observations at a time. Here I will set batch_size to 50.\n\n\nshow code\nmydataloader = DataLoader(mydataset, batch_size=20)\n\n\n\n\nshow code\nfor i, batch in enumerate(mydataloader):\n    print(f'batch {i}: number of observations and ground truth are {batch[0].shape[0]} and {batch[1].shape[0]} respectively')\n\n\nbatch 0: number of observations and ground truth are 20 and 20 respectively\nbatch 1: number of observations and ground truth are 20 and 20 respectively\nbatch 2: number of observations and ground truth are 20 and 20 respectively\nbatch 3: number of observations and ground truth are 20 and 20 respectively\nbatch 4: number of observations and ground truth are 20 and 20 respectively"
  },
  {
    "objectID": "content/blog/index.html",
    "href": "content/blog/index.html",
    "title": "things I feel like sharing",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nSep 7, 2023\n\n\nNotes on dataloaders\n\n\n\n\nSep 6, 2023\n\n\nUnderstanding pvalues, multiple testing and qvalues\n\n\n\n\nSep 3, 2023\n\n\nParallelization in R\n\n\n\n\nJul 18, 2022\n\n\nPredicting swarm behaviour\n\n\n\n\nMay 13, 2022\n\n\nSimulating discrete and continuous populations\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/assets/publications/index.html",
    "href": "content/assets/publications/index.html",
    "title": "publications",
    "section": "",
    "text": "I would rather the currency be dollars but anyway…\n\n\n\n\n\n\n  \n    \n      \n        Machine Learning-Based Predictive Modeling of Postpartum Depression\n        Dayeon Shin, Kyung Ju Lee, Temidayo Adeluwa, Junguk Hur\n        2020 | Journal of Clinical Medicine\n      \n    \n  \n\n\n  \n    \n      \n        Novel Action of Vinpocetine in the Prevention of Paraquat-Induced Parkinsonism in Mice: Involvement of Oxidative Stress and Neuroinflammation\n        Ismaila Ishola, A.A. Akinyede, T.P. Adeluwa, C. Micah\n        2018 | Metabolic Brain Disease\n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Temi",
    "section": "",
    "text": "My name is Temi, and I’m a PhD student in the Genetics, Genomics and Systems Biology (GGSB) program at The University of Chicago. Prior to this, I completed a master’s at The University of North Dakota, supervised by the awesome Junguk Hur."
  },
  {
    "objectID": "index.html#thinking-about",
    "href": "index.html#thinking-about",
    "title": "Temi",
    "section": "thinking about",
    "text": "thinking about\n\ngenetics, pharmacogenetics, machine learning, and statistical methods\nclimate change + recycling\nrap, as in the music genre\nwhat to do after a PhD\nbecoming a good-enough chef + next DIY projects\nfootball, or soccer, if you are North American\nwhy there is so much kickback against Oxford commas"
  },
  {
    "objectID": "index.html#currently",
    "href": "index.html#currently",
    "title": "Temi",
    "section": "currently",
    "text": "currently\nIn Fall 2022, I joined Haky’s lab for my PhD thesis. Here, I will be studying variations in the human epigenome (at the genomic level and the population level), how these variations influence molecular phenotypes (e.g. TF binding), and how these, in turn, influence complex traits and diseases. These are complex/interesting questions. Wish me luck!\n\ncv\nresume\npublications"
  },
  {
    "objectID": "index.html#links-reach-out",
    "href": "index.html#links-reach-out",
    "title": "Temi",
    "section": "links + reach out",
    "text": "links + reach out\n\nYou can reach me by filling this form; or\nLinkedIn; or\nTwitter; or\nGithub"
  },
  {
    "objectID": "index.html#other-lives",
    "href": "index.html#other-lives",
    "title": "Temi",
    "section": "other lives",
    "text": "other lives\nOften, I am in the gym doing bodyweight workouts. Sometimes, I read books + watch TV. Other times, I play the guitar, football, and do some DIYs for fun."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]