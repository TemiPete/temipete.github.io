[
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Temi",
    "section": "",
    "text": "hey"
  },
  {
    "objectID": "content/books/index.html",
    "href": "content/books/index.html",
    "title": "Temi",
    "section": "",
    "text": "Just so you know, I don’t necessarily endorse all of these books."
  },
  {
    "objectID": "content/books/index.html#currently-reading",
    "href": "content/books/index.html#currently-reading",
    "title": "Temi",
    "section": "currently reading",
    "text": "currently reading\n\n\nThe Second Sex by Simone de Beauvoir\n\nStarted sometime in August 2023"
  },
  {
    "objectID": "content/books/index.html#previously-read",
    "href": "content/books/index.html#previously-read",
    "title": "Temi",
    "section": "previously read",
    "text": "previously read\nthere’s more, but in no particular order:\n\nFactotum by Charles Bukowski\nA Peace To End All Peace by David Fromkin\nA Song of Ice and Fire by George R. R. Martin\n\nA Game of Thrones\nA Clash of Kings\nA Storm of Swords\n\nThe Emperor of All Maladies by Siddhartha Mukherjee\nThen We Came To The End by Joshua Ferris\nAnimal’s People by Indra Sinha\nNever Let Me Go by Kazuo Ishiguro\nThe Buried Giant by Kazuo Ishiguro\nThe First Law by Joe Abercrombie\n\nThe Blade Itself\nBefore They Are Hanged"
  },
  {
    "objectID": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html",
    "href": "content/blog/posts/2023-09-05-predicting-swarm-behaviour/index.html",
    "title": "Predicting swarm behaviour",
    "section": "",
    "text": "Dataset\nI will be using the aligned dataset, and will\n\nnormalize the data\nsplit the data\nmove the data to the device i.e. gpu, in this case\n\n\n\nshow code\n# standardizing the data\nmean_ = swarm_data['aligned'].iloc[:, :-1].mean(axis=0)\nstd_ = swarm_data['aligned'].iloc[:, :-1].std(axis=0)\n\nswarm_data['aligned'].iloc[:, :-1] = (swarm_data['aligned'].iloc[:, :-1] - mean_)/std_\n\n\n\n\nshow code\ndata_split = split_into_train_val_test(df=swarm_data['aligned'], return_what='tuple')\nX_train, y_train = data_split[0]\nX_test, y_test = data_split[1]\nX_valid, y_valid = data_split[2]\n\n# \nX_train = torch.tensor(X_train, dtype=torch.float64).to(device)\ny_train = torch.tensor(y_train).type(torch.LongTensor).to(device)\n\nX_test = torch.tensor(X_test, dtype=torch.float64).to(device)\ny_test = torch.tensor(y_test).type(torch.LongTensor).to(device)\n\n\n(12008, 2400) (12008,)\n(6004, 2400) (6004,)\n(6004, 2400) (6004,)\n\n\n\n\nshow code\nX_train.is_cuda\n\n\nTrue\n\n\n\n\nDefining the model\nHere I use a multilayer perceptron, or mlp.\n\n\nshow code\nclass SwarmNN(torch.nn.Module):\n    \n    '''\n    \n    '''\n    \n    def __init__(self, in_dim, out_dim, hidden_dims=[], use_bias=True):\n        '''\n        Constructs a multilayer perceptron\n        '''\n        \n        super(SwarmNN, self).__init__()\n        \n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        # assuming we don't have any hidden layer, this will just implement a linear model\n        if len(hidden_dims) == 0:\n            layers = [torch.nn.Linear(in_dim, out_dim, bias=use_bias)]\n        else:\n            layers = [torch.nn.Linear(in_dim, hidden_dims[0], bias=use_bias), torch.nn.ReLU()]\n            \n            for i, hidden_dim in enumerate(hidden_dims[:-1]):\n                layers += [torch.nn.Linear(hidden_dim, hidden_dims[i+1], bias=use_bias), torch.nn.ReLU()]\n            layers += [torch.nn.Linear(hidden_dims[-1], out_dim, bias=use_bias)]\n            \n        self.main = torch.nn.Sequential(*layers)\n        \n    def forward(self, x):\n        \n        hidden_output = self.main(x)\n        output = torch.nn.functional.softmax(hidden_output, dim=1)\n        #output = output.argmax(dim=1)\n        \n        return output\n\n\n\n\nshow code\nmodel = SwarmNN(in_dim=X_train.shape[1], out_dim=2, hidden_dims=[500, 100]).to(device)\nmodel\n\n\nSwarmNN(\n  (main): Sequential(\n    (0): Linear(in_features=2400, out_features=500, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=500, out_features=100, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=100, out_features=2, bias=True)\n  )\n)\n\n\n\n\nshow code\nfor param in model.parameters():\n    print(param.shape)\n\n\ntorch.Size([500, 2400])\ntorch.Size([500])\ntorch.Size([100, 500])\ntorch.Size([100])\ntorch.Size([2, 100])\ntorch.Size([2])\n\n\n\n\nDataloaders\nCurrently, I am not using dataloaders. I will implement this some other time.\n\n\nshow code\n# trainloader = torch.utils.data.DataLoader((X_train, y_train), batch_size=32, shuffle=True)\n# testloader = torch.utils.data.DataLoader((X_test, y_test), batch_size=32, shuffle=False)\n\n\n\n\nTraining\nHere I use: - a learning rate of 0.001 - SGD as the optimizer - cross-entropy loss\n\n\nshow code\nlr = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\nloss_fxn = torch.nn.CrossEntropyLoss()\n\n\n\n\nshow code\nnum_epochs = 500\n\nloss_tally = []\nacc_tally = []\nmetrics = {}\n\n# for the test set\ntest_loss = []\ntest_acc = []\n\nfor epoch in range(0, num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    predictions = model(X_train.float()).to(device)\n    loss = loss_fxn(predictions, y_train)\n    acc = torch.mean(1.0 * (predictions.argmax(dim=1) == y_train))\n    loss.backward()\n    optimizer.step()\n\n    #running_loss += loss.item()\n    #loss_tally.append(running_loss)\n    loss_tally.append(loss.cpu().item())\n    acc_tally.append(acc.cpu().item())\n    \n    # on the test set\n    with torch.no_grad():\n        model.eval()\n        test_pred = model(X_test.float())\n        test_l = loss_fxn(test_pred, y_test).item()\n        \n        #print(f'Test loss {epoch}: {test_l}')\n        test_loss.append(test_l)\n        test_acc.append(torch.mean(1.0 * (test_pred.argmax(dim=1) == y_test)).item())\n        \n\nmetrics['train_loss'] = loss_tally\nmetrics['train_accuracy'] = acc_tally\nmetrics['test_loss'] = test_loss\nmetrics['test_accuracy'] = test_acc\n\n\n\n\nshow code\ndef plot_my_training(metrics):\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n\n    ax[0].plot(range(len(metrics['train_loss'])), metrics['train_loss'],\n            alpha=0.8, label='Train')\n    ax[0].plot(range(len(metrics['test_loss'])), metrics['test_loss'], label='Test')\n    ax[0].set_xlabel('Iteration/Epoch')\n    ax[0].set_ylabel('Loss')\n    ax[0].legend()\n\n    ax[1].plot(range(len(metrics['train_accuracy'])), metrics['train_accuracy'],\n            alpha=0.8, label='Train')\n    ax[1].plot(range(len(metrics['test_accuracy'])), metrics['test_accuracy'], label='Test')\n    ax[1].set_xlabel('Iteration/Epoch')\n    ax[1].set_ylabel('Accuracy')\n    ax[1].legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nPerformance\n\n\nshow code\nplot_my_training(metrics)"
  },
  {
    "objectID": "content/blog/posts/2023-09-06-understanding-qvalues/index.html",
    "href": "content/blog/posts/2023-09-06-understanding-qvalues/index.html",
    "title": "Understanding pvalues, multiple testing and qvalues",
    "section": "",
    "text": "Tip\n\n\n\n\nStorey and Tibshirani’s paper is a good place to start\nHaky’s notes here are also very helpful; and this notebook was partly inspired by her notes.\n\n\n\n\nIntroduction\nWhen doing multiple tests, there is a need to control the false positive rate (fpr) because by nature of p-values, if there is nothing interesting going on, you still have an alpha % chance of detecting something, and misclassifying that.\n\n\nSimulation 1: Null and alternative effects.\nI have a simple linear function here, where \\(X\\) has some effect, \\(\\beta\\) on \\(Y\\).\n\\[\nY \\approx \\sum X\\beta + \\epsilon\n\\]\nwhere,\n\\[\nX \\approx \\mathcal{N}(0.2,1)\\\n\\]\n\\[\n\\epsilon \\approx \\mathcal{N}(0,0.1)\n\\]\n\n\nshow code\nlibrary(qvalue)\n\n\n\n\nshow code\ndevtools::source_gist('https://gist.github.com/TemiPete/d7e37272964e5f00af4efea01d295dc8')\n\n\nℹ Sourcing gist \"d7e37272964e5f00af4efea01d295dc8\"\nℹ SHA-1 hash of file is \"d308c0c2f4eb3d58cff6b52ad22538f09bd136e0\"\n\n\n\n\nshow code\nset.seed(2023)\nnobserv <- 2000 # number of observations\n\n\n\n\nshow code\ntrue_mean <- 0.2\ntrue_sd <- 1\neps_mean <- 0\neps_sd <- 0.5\nbeta <- 0.6\nx <- rnorm(n=nobserv, mean=true_mean, sd=true_sd)\ne <- rnorm(n=nobserv, mean = eps_mean, sd=eps_sd)\n\nyalt <- x * beta + e\nplot(x, yalt, main='x has an effect on y', frame.plot=F)\n\n\n\n\n\n\n\nshow code\nynull <- rnorm(n=nobserv, mean=0, sd=beta)\nplot(x, ynull, main='x has no effect on y', frame.plot=F)\n\n\n\n\n\nNow I can simulate these tests multiple times, say, 10000\n\n\nshow code\nntests <- 10000\nX <- matrix(rnorm(n=nobserv*ntests, mean=true_mean, sd=true_sd), nrow=nobserv, ncol=ntests)\nepsilon <- matrix(rnorm(n=nobserv*ntests, mean=eps_mean, sd=eps_sd), nrow=nobserv, ncol=ntests)\ndim(X) ; dim(epsilon)\n\n\n[1]  2000 10000\n\n\n[1]  2000 10000\n\n\nshow code\nYalt <- X * beta + epsilon\nYnull <- matrix(rnorm(n=nobserv, mean=0, sd=beta), nrow=nobserv, ncol=ntests)\ndim(Yalt) ; dim(Ynull)\n\n\n[1]  2000 10000\n\n\n[1]  2000 10000\n\n\n\n\nshow code\npvec = rep(NA,ntests)\nbvec = rep(NA,ntests)\n\nfor(ss in 1:ntests)\n{\n  fit = fastlm(X[,ss], Ynull[,ss])\n  pvec[ss] = fit$pval  \n  bvec[ss] = fit$betahat\n}\n\nsummary(pvec)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000291 0.2501638 0.4916683 0.4972860 0.7465493 0.9999487 \n\n\n\n\nshow code\nsum(pvec < 0.05) ; mean(pvec < 0.05)\n\n\n[1] 481\n\n\n[1] 0.0481\n\n\nEven under the null, we find that 5% of our tests are false positives! In real life, we would think these are true effects, which is not good.\nSo, we try to control this false positive rate. There are many methods, but we can use the Bonferroni approach\n\n\nshow code\ncutoff <- 0.05/length(pvec)\nsum(pvec < cutoff) ; mean(pvec < cutoff)\n\n\n[1] 0\n\n\n[1] 0\n\n\nWith Bonferroni correction, we see that all of the tests are null, which is what should have happened in the first place. Anyway, all that was for simulation sake. I should create a set of tests, where some proportion are under the alternative i.e. true, and the rest are not i.e. null\n\n\nSimulation 2: A mixture of outcomes under the null and alternative hypothesis.\n\n\nshow code\nptrue <- 0.2 # only 20% of the tests are TRUE\nwtrue <- sample(x=c(0,1), size=ntests, replace=TRUE, prob=c(1-0.2, 0.2))\ntable(wtrue) |> prop.table()\n\n\nwtrue\n     0      1 \n0.8046 0.1954 \n\n\nI will look through wtrue. If 0, I will select the ynull at that index, otherwise, I will select the yalt\n\n\nshow code\nYboth <- matrix(NA, nrow=nobserv, ncol=ntests)\nfor(i in seq_along(wtrue)){\n    if(wtrue[i] == 1){\n        Yboth[, i] <- Yalt[, i]\n    } else {\n        Yboth[, i] <- Ynull[, i]\n    }\n}\n\ndim(Yboth)\n\n\n[1]  2000 10000\n\n\n\n\nshow code\n## run linear regression for all 10000 phenotypes in the mix of true and false associations, Ymat_mix\npvec_mix = rep(NA,ntests)\nbvec_mix = rep(NA,ntests)\nfor(ss in 1:ntests){\n  fit = fastlm(X[,ss], Yboth[,ss])\n  pvec_mix[ss] = fit$pval  \n  bvec_mix[ss] = fit$betahat\n}\nsummary(pvec_mix)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0678  0.3792  0.3992  0.6825  0.9999 \n\n\n\n\nshow code\nsum(pvec_mix < 0.05) ; mean(pvec_mix < 0.05)\n\n\n[1] 2339\n\n\n[1] 0.2339\n\n\n\n\nshow code\nhist(pvec_mix, main='')\nmtext('A simulation under the null + alt', side=3, line=1, adj = 0)\n\n\n\n\n\n\n\nQuick detour: fpr, fdr, pfdr and such\nWe expect 500 to be significant under the null, but we get 2339.\nSince we have more than what we expected under the null, we can assume that the remainder are gotten under the alternative. We can estimate this true discovery rate\n\\[\n\\frac{(nobserved - nexpected)}{nobserved}\n\\]\n\n\nshow code\ntdr <- ((sum(pvec_mix < 0.05)) - (0.05*ntests))/(sum(pvec_mix < 0.05))\ntdr \n\n\n[1] 0.7862334\n\n\nThe false discovery rate is 1 - tdr, which in this case is 0.2137666.\nAll well and good, except that our tdr here is higher than we expect. Instead we can estimate the positive false discovery rate or pFDR. Here’s how I explain this: Given that you have found a number of tests to be significant, let’s call this tsig, and we expect at least one of these to be positive i.e. under the alternative, what is the expected number of false positives? i.e. what proportion are not true but we come out as true.\nTo break this down a little, I will start from here: Assuming you do a test to classify if a a group of people eat fruits or not, and you have this table after.\n\n\n\n\n\n\n\n\n\n\neats fruits\ndoes not eat fruits\n\n\n\n\n\nclassified: eat fruits\ntrue positives\nfalse positives\n\n\n\nclassified: does not eat fruits\nfalse negatives\ntrue negatives\n\n\n\n\n\n\n\n\n\n\nThe fpr, as mentioned earlier is:\n\\[\n\\frac{false\\ positives}{(false\\ positives\\ +\\ true\\ negatives)}\n\\]\ni.e. of all the people who don’t eat fruits, how many of them do we classify to eat fruits based on our tests?\nThe fdr then is, of all the people who we classify as eating fruits, how many of them don’t actually eat fruits?\n\\[\n\\frac{false\\ positives}{(true\\ positives\\ +\\ false\\ positives)}\n\\]\nUsing the table + idea above, I can then\n\n\n\n\n\n\n\n\n\n\neats fruits\ndoes not eat fruits\n\n\n\n\n\nclassified: eat fruits\ntrue positives\nfalse positives\n\n\n\nclassified: does not eat fruits\nfalse negatives\ntrue negatives\n\n\n\n\n\n\n\n\n\n\nBecause we simulated the data, we know that 1954 tests are under the alternative, and the rest, 8046, should be under the null. But after our tests, we have found 2339 to be under the alternative and 7661 to be under the null. So, there are some 385 that have been misclassified as under the alternative when they are not, and some -385\n\n\nshow code\ntp <- sum(wtrue == 1 & pvec_mix < 0.05)\ntn <- sum(wtrue == 0 & pvec_mix >= 0.05)\nfp <- sum(wtrue == 1 & pvec_mix >= 0.05)\nfn <- sum(wtrue == 0 & pvec_mix < 0.05)\n\n\nAlternatively, table(pvec_mix > 0.05, wtrue) will yield the same result.\n\n\n\n\nalternative\nnull\ntotal\n\n\n\n\nclassified: alternative\n1954\n385\n2339\n\n\nclassified: null\n0\n7661\n7661\n\n\ntotal\n1954\n8046\n\n\n\n\nWith this, we can estimate the fpr to be 0 and fdr to be 0.\nOkay. Back to pfdr. Remember that I described this earlier, saying:\n\n“Given that you have found a number of tests to be significant, let’s call this tsig, and we expect at least one of these to be positive i.e. under the alternative, what is the expected number of false positives? i.e. what proportion are not true but we come out as true.”"
  },
  {
    "objectID": "content/blog/posts/2023-08-29-simulating-populations/index.html",
    "href": "content/blog/posts/2023-08-29-simulating-populations/index.html",
    "title": "Simulating discrete and continuous populations",
    "section": "",
    "text": "Introduction\nOftentimes, we want to simulate populations in order to do some analysis. In this demo, I go over a, perhaps, crude way to mimic discrete and continuous populations. Before simulating, we need to know what a population is. In genetics, a population is a grouping of individuals based on some similarity in allele frequencies between them. Of course, there are many other definitions of what a population is. In this demo, I will use the definition based on allele frequencies.\nOne way to generate allele frequencies that mimic that seen in typical populations is to use the \\(F_{ST}\\), AKA the fixation index, AKA F-statistic. \\(F_{ST}\\) measures population differentiation as a result of genetic structure. Values range between 0 and 1, and higher values mean that two populations are highly similar. There are many definitions of \\(F_{ST}\\), depending on what you are discussing - and I am still learning about most of them. In this case, however, I am primarily concerned with allele frequency-related definitions.\n\n\nSimulating discrete populations\nTo simulate populations based on the definition of \\(F_{ST}\\), one thing we can do is to simulate an ancestral population’s allele frequencies. This is similar to having a founder population from which every other population descends. Afterwards, we can simulate different populations from this ancestral population. This follows from the Balding-Nichols model.\n\\[\\begin{align}\nBeta(\\frac{1-F}{F}p, \\frac{1-F}{F}(1-p))\n\\end{align}\\]\nAssume that we have 500 alleles/loci, and 4 populations, and we intend to simulate 1000 individuals from each of these populations. We can define our \\(F_{ST}\\) to be, say, 0.09 between populations 1 and 2, 0.19 between populations 1 and 2, and population 3, and 0.4 between all these populations, and population 4. Therefore, populations 1 and 2 will be closely related, and different from population 3, and all of them will be different from population 4.\n\n\nshow code\nM <- 500 # number of alleles or SNPs\nn_pops <- 4 # number of populations\nN <- 1000 # 20 individuals in each n_pops\nf_pop12 <- 0.09\nf_pop3 <- 0.19 # a third, distant, unrelated population\nf_pop4 <- 0.4 # a fourth population\n\n\nWhen we make plots of the allele frequencies of these populations, we expect high correlations between populations 1 and 2, and not-so-high correlations when we compare with the other populations 3 and 4.\nWe can generate some random minor allele frequencies (MAFs) to be the ancestral allele frequencies, and generate independent draws from the distribution of the ancestral allele frequencies, based on the equation above.\n\n\nshow code\nancestral_allele_freqs <- runif(M, 0.01, 0.5)\nsh1 <- ((1-f_pop12)/f_pop12)*ancestral_allele_freqs\nsh2 <- ((1-f_pop12)/(f_pop12))*(1-ancestral_allele_freqs)\npop1_allele_freqs <- rbeta(M, shape1 = sh1, shape2 = sh2)\npop2_allele_freqs <- rbeta(M, shape1 = sh1, shape2 = sh2)\npop3_allele_freqs <- rbeta(M, shape1 = ((1-f_pop3)/f_pop3)*ancestral_allele_freqs, shape2=((1-f_pop3)/(f_pop3))*(1-ancestral_allele_freqs))\npop4_allele_freqs <- rbeta(M, shape1 = ((1-f_pop4)/f_pop4)*ancestral_allele_freqs, shape2=((1-f_pop4)/(f_pop4))*(1-ancestral_allele_freqs))\n\nall_allele_freqs <- list(pop1_allele_freqs, pop2_allele_freqs, pop3_allele_freqs, pop4_allele_freqs)\nnames(all_allele_freqs) <- paste0('population_', 1:4)\n\n\n\nFirst, how does the expected value of the distribution of allele frequency changes as a factor of the \\(F_{ST}\\)? This should give us an idea of what to expect when we simulate allele frequencies.\n\n\nshow code\n# generate many Fst values\nfst_vector <- round(seq(0.01, 0.99, length.out=20), 2) # 20 fsts ranging from 0.01 to 0.99\nper_fst <- lapply(fst_vector, function(each_fst){\n  \n  sh1 <- ((1-each_fst)/each_fst)*ancestral_allele_freqs\n  sh2 <- ((1-each_fst)/(each_fst))*(1-ancestral_allele_freqs)\n  temp_allele_frq <- rbeta(M, shape1 = sh1, shape2 = sh2)\n  deviation <- cor(ancestral_allele_freqs, temp_allele_frq) #|> suppressWarnings()\n  return(deviation)\n})\n\n\n\n\nshow code\ncor_deviation <- per_fst |> unlist()\n\nplot(cor_deviation, frame.plot=F, xaxt='n', yaxt='n', type='b', xlab = expression(F[ST]), ylab=expression(R^2))\naxis(1, at=rep(1:length(cor_deviation), by=1), labels = fst_vector, las=2)\naxis(2, at=seq(0, 1, 0.1))\n\n\n\n\n\nWe see that as the \\(F_{ST}\\) increases, the correlation between the ancestral allele frequency and the generated allele frequency reduces.\n\n\nshow code\nlayout(matrix(c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = T))\nnames_pop <- names(all_allele_freqs)\nfor(i in 1:4){\n    for(j in 1:4){\n        if(i == j){ # we don't want to plot pop1 vs pop1 e.t.c.\n            next\n        } else if (i < j) {\n            #cor_ <- round(cor(all_allele_freqs[[i]], all_allele_freqs[[j]]), 2)\n            plot(all_allele_freqs[[i]], all_allele_freqs[[j]], xlab = names_pop[i], ylab = names_pop[j], frame.plot = F)\n            #abline(a=0, b=cor_, col='red')\n            #mtext(cor_)\n        }\n    }\n}\nmtext('Scatterplot of allele frequencies for each population', outer = T)\n\n\n\n\n\nThe plot is approximately (roughly?) what we expect!\n\n\n\nSimulating genotypes using these allele frequencies\nGiven these allele frequencies, we can simulate genotypes from each of these populations by sampling from a binomial distribution.\n\\[\\begin{align}\nG_{k_{n}} \\sim Binom(n, p_{k})\n\\end{align}\\]\nwhere \\(n\\) is the number of success, and \\(p\\) is the probability of a success. Here, \\(n = 2\\) because we want 2 alleles per loci, and \\(p\\) is the allele frequency at that loci.\n\n\nshow code\ngenotypes <- lapply(seq_along(all_allele_freqs), function(i){\n    matrix(data=rbinom(n=N*M, size=2, prob = all_allele_freqs[[i]]), nrow = N, ncol = M, byrow = T)\n})\n\ngenotypes <- do.call(rbind, genotypes)\nshuffle <- sample(1:nrow(genotypes)) # I will shuffle the data\ngenotypes <- genotypes[shuffle, ]\npops <- rep(names(all_allele_freqs), each=N)\npops <- pops[shuffle] # shuffle pops too, since it tells me what populations the genotypes come from\n\ncat('Here is what the genotypes look like\\n')\n\n\nHere is what the genotypes look like\n\n\nshow code\ngenotypes[1:5, 1:5]\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    1    0    1    0    0\n[3,]    0    1    0    0    0\n[4,]    1    0    0    0    0\n[5,]    0    0    0    0    0\n\n\nshow code\ncat('And here is what the population designation looks like \\n')\n\n\nAnd here is what the population designation looks like \n\n\nshow code\npops[1:5]\n\n\n[1] \"population_1\" \"population_3\" \"population_2\" \"population_3\" \"population_4\"\n\n\nNext, we can calculate the principal components (PCs) of this data, and plot PC1 vs PC2, to see where the most variability lies in the data\n\n\nshow code\n# first change the row names of the genotype data to reflect the populations\n#rownames(genotypes) <- pops\n# I need to make sure that there are some variations in the SNPS - if there aren't any, I should remove them\npca_genotypes <- genotypes[ , which(apply(genotypes, 2, var) != 0)]\n# PCA\npca_dt <- prcomp(pca_genotypes, scale.=T, center=T)\npca_dt$x[1:5, 1:5]\n\n\n            PC1       PC2        PC3         PC4        PC5\n[1,]   3.263826 -7.849732 -6.9217690 -2.66479261 -3.9900565\n[2,]   6.544296  8.367773 -1.7339642 -0.06842935  2.8759937\n[3,]   6.086323 -4.769098  6.4930496 -1.81845701 -1.7244237\n[4,]   5.730047  7.073629 -1.7772414 -1.97970506 -0.2309777\n[5,] -12.399378  1.580702  0.6451658  0.24943503  1.3446776\n\n\n\n\nshow code\n# colors\nmycolors <- colorRampPalette(RColorBrewer::brewer.pal(8, \"Dark2\"))(n_pops)\n#mycolors <- c('red', 'orange', 'green', 'blue')\ncolor_coding <- factor(pops, labels = mycolors)\nplot(pca_dt$x[, 'PC1'], pca_dt$x[, 'PC2'], bg=as.character(color_coding), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of 4 discrete populations')\nlegend('topleft', legend=names_pop, pch=c(21, 21, 21, 21), pt.bg=mycolors, bty = 'n', xpd=NA)\n\n\n\n\n\nWe can look at populations 1 and 2 only, and see if there is any difference between them.\n\n\nshow code\npop_names <- c('population_1', 'population_2')\nidx <- which(pops %in% pop_names)\npca_12 <- pca_dt$x[idx, ]\ncolor_coding <- factor(pops[idx], labels = mycolors[1:2])\nplot(pca_12[, 'PC1'], pca_12[, 'PC2'], bg=as.character(color_coding), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of the 2 similar populations 1 and 2')\nlegend('topleft', legend=pop_names, pch=c(21, 21), pt.bg=mycolors[1:2], bty = 'n', xpd=NA)\n\n\n\n\n\n\n\nSimulating continuous populations\nWhat I have done so far, is to simulate a discrete population of individuals. Think of it as people living in Mars, Earth, and Jupiter, and who have never intermingled since time immemorial. Population 3 is faraway from population 4. However, true human populations are not this way. How can we simulate a truly continuous population?\nFirst, notice that I am only simulating allele frequencies. Therefore, I can put a prior on my earlier-defined beta distribution to model the probability that an individual is from population \\(m\\). In other words, I am trying to estimate what percentage of an individual’s alleles might have come from a certain population \\(m\\). I will call this random variable, \\(Q\\).\n\\[\\begin{align}\nQ \\sim Dirichlet(\\alpha)\n\\end{align}\\]\nA trick I can use is to take an individual, ask how much of their genome is shared among each population I have, select that proportion of allele frequencies, and simulate their genotypes based on the selection for each population. To do this, I need to know how to distribute the \\(\\alpha\\) parameter used in the Dirichlet distribution. A trick I can use is to use the allele frequency distribution in the entire population. I will call this distribution the population grade distribution.\nEarlier, I simulated some allele frequencies all_allele_freqs\n\n\nshow code\nss <- sapply(all_allele_freqs, sum)\npopulation_grade <- ss/sum(ss)\n#population_grade <- rand_vect_cont(4, 1)\npopulation_grade\n\n\npopulation_1 population_2 population_3 population_4 \n   0.2462436    0.2509962    0.2512640    0.2514962 \n\n\nThen I can use this as a parameter to the Dirichlet distribution to simulate the percentage of an individual’s genotypes that is shared across all 4 populations.\n\n\nshow code\nq_matrix <- gtools::rdirichlet(N * n_pops, alpha = population_grade) # I want to simulate, N individuals per population\n\n\nHere is an illustration. Assuming that I am looking at the first individual\n\n\nshow code\n# assuming one individual\nid1 <- q_matrix[1, ]\nq_counts <- floor(id1 * M) # number of loci shared\nq_counts\n\n\n[1]   0  43 340 116\n\n\nThe assumption here is that this individual shares 0 loci with population 1, 43 with population 2, 340 with population 3, and 116 loci with population 4, and 3 independent loci not share with any population. The sum is 499 (because I rounded down), but I don’t want this to be this case. Therefore, I will share the remaining loci with the highest number already available, so that the sum is 500.\n\n\nshow code\nremainder_ <- M - sum(q_counts)\nq_counts[which.max(q_counts)] <- q_counts[which.max(q_counts)] + remainder_\nq_counts\n\n\n[1]   0  43 341 116\n\n\nNow, this sums to 500.\nPutting this into a reusable function…\n\n\nshow code\nq_counts_fxn <- function(q_matrix, M){\n    apply(q_matrix, 1, function(each_row){\n        temp <- floor(each_row * M)\n        remainder_ <- M - sum(temp)\n        temp[which.max(temp)] <- temp[which.max(temp)] + remainder_\n        temp\n    }) |> t()\n}\n\nq_counts <- q_counts_fxn(q_matrix, M)\nq_counts[1:5, ] # for the first 5 individuals\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    0   43  341  116\n[2,]  385    0   43   72\n[3,]   10   16  364  110\n[4,]    0  370   12  118\n[5,]  103  256    0  141\n\n\n\n\nshow code\ngenotypes <- matrix(NA, nrow = N*n_pops, ncol = M) # remember, N individuals per population\n\nfor(q in 1:nrow(q_counts)){\n    pop_down <- 1\n    m_loci <- 1:M\n    i_loci <- list() # this list contains the actual loci an individual has shared among the other populations\n    while(pop_down <= ncol(q_counts)){\n        # sample pop 1 loci\n        pop_loci <- sample(m_loci, size=q_counts[q, pop_down], replace = F)\n        if(length(pop_loci) == 0){\n            m_loci <- m_loci[m_loci != 0]\n            i_loci[[pop_down]] <- 0\n        } else {\n            m_loci <- m_loci[!m_loci %in% pop_loci]\n            i_loci[[pop_down]] <- pop_loci\n        }\n        pop_down <- pop_down + 1\n    }\n    \n    g_loci <- lapply(seq_along(i_loci), function(i){\n        temp <- all_allele_freqs[[i]][i_loci[[i]]]\n        rbinom(n = length(temp), size = 2, prob = temp)\n    })\n\n    gi <- vector(mode='numeric', length = M)\n    for(i in 1:n_pops){\n        gi[i_loci[[i]]] <- g_loci[[i]]\n    }\n    \n    genotypes[q, ] <- gi\n    \n}\n\n\n\n\nshow code\npop <- paste0('population_', apply(q_counts, 1, which.max))\n# first change the row names of the genotype data to reflect the populations\nrownames(genotypes) <- pop\n# I need to make sure that there are some variations in the SNPS - if there aren't any, I should remove them\npca_genotypes <- genotypes[ , which(apply(genotypes, 2, var) != 0)]\n# PCA\npca_dt <- prcomp(pca_genotypes, scale.=T, center=T)\npca_dt$x[1:5, 1:5]\n\n\n                    PC1       PC2         PC3        PC4        PC5\npopulation_3  1.7973144 -5.028205 -0.35698361  2.6108432 -0.5580345\npopulation_1 -0.4990041  4.185811 -5.96843755  0.1924663  0.4836813\npopulation_3  1.9174774 -5.916136  0.09453596  1.5513878 -0.3693294\npopulation_2  1.6417866  2.424741  6.31743092 -1.8844746 -0.3070795\npopulation_2 -0.6605363  3.049585  0.82280376  1.9657889  0.9151204\n\n\n\n\nshow code\nmycolors <- colorRampPalette(RColorBrewer::brewer.pal(8, \"Dark2\"))(n_pops)\ncolor_coding <- factor(rownames(pca_dt$x), labels = mycolors)\nplot(pca_dt$x[, 'PC1'], pca_dt$x[, 'PC2'], bg=color_coding |> as.character(), xlab = 'PC1', ylab = 'PC2', frame.plot = F, pch=21, main='PC1 vs PC2 of 4 continuous populations')\nlegend('topleft', legend=names_pop, pch=c(21, 21, 21, 21), pt.bg=mycolors, bty = 'n', xpd = NA)"
  },
  {
    "objectID": "content/blog/posts/2023-09-03-parallelization-in-R/index.html",
    "href": "content/blog/posts/2023-09-03-parallelization-in-R/index.html",
    "title": "Parallelization in R",
    "section": "",
    "text": "When running computationally-heavy tasks in R, it can be useful to parallelize your codes/runs. In that vein, this is a really good blogpost to read to understand when and how to parallelize. And here is another one.\nR offers many ways to do this. Usually, I prefer using some libraries.\n\n\nshow code\nlibrary(doParallel)\n\n\nLoading required package: foreach\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n\nshow code\nlibrary(foreach)\nlibrary(parallel)\n\n\n\n\nAssuming we want to apply a function over the rows of a matrix. This function will take each row, divide the numbers in that row by the index of that row in the matrix and return a newly-created matrix of the same shape.\n\n\nshow code\nset.seed(2022)\nmyMatrix <- matrix(sample(1:50000, size=300*500, replace=T), nrow=300, ncol=500)\n\ndim(myMatrix)\n\n\n[1] 300 500\n\n\n\n\nFirst, I will time this function with lapply loops. lapply is shipped with base R and is a parallel form of a regular for loop.\n\n\nshow code\nlapply_fxn <- function(){\n  applyMatrix <- lapply(1:nrow(myMatrix), function(i){\n    out <- myMatrix[i, ] / (i)\n    return(out)\n  })\n  applyMatrix <- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(lapply_fxn())\n\n\n   user  system elapsed \n  0.004   0.000   0.004 \n\n\n\n\n\nNext, let’s take advantage of the cores, this time using mclapply\n\n\nshow code\nmclapply_fxn <- function(){\n  applyMatrix <- parallel::mclapply(1:nrow(myMatrix), function(i){\n    out <- myMatrix[i, ] / (i)\n    return(out)\n  }, mc.cores = 12) # using 7 cores\n  \n  applyMatrix <- do.call('rbind', applyMatrix)\n  return(applyMatrix)\n}\n\nsystem.time(mclapply_fxn())\n\n\n   user  system elapsed \n  0.043   0.084   0.032 \n\n\nWe see that lapply is faster. This is because there is an overhead to distributing these runs and collecting their results when using mclapply\n\n\n\nHere, I will use the foreach but without a parallel back-end - this is akin to a sequential run, just like lapply or a regular for loop\n\n\nshow code\nsystem.time({\n  outputMatrix <- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %do% {\n    out <- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.071   0.005   0.078 \n\n\n\n\n\nHere, I will use the foreach but with a parallel back-end. The parallel back-end is a cluster of cores If you are familiar with multiprocessing in python, it is equivalent to multiprocessing.Pool\nFirst we need to register a parallel back-end\nWe can query how many cores we have on this computer\n\n\nshow code\nparallel::detectCores()\n\n\n[1] 8\n\n\nI will register 7 cores\n\n\nshow code\nnum_clusters <- 7 #- 5 # 12 - 5\ndoParallel::registerDoParallel(num_clusters)\n\ncat('Registering', num_clusters, 'clusters for a parallel run\\n')\n\n\nRegistering 7 clusters for a parallel run\n\n\n\n\nshow code\nsystem.time({\n  outputMatrix <- foreach::foreach(i=1:nrow(myMatrix), .combine='rbind', .inorder=F) %dopar% {\n    out <- myMatrix[i, ] / i\n    return(out)\n  }\n})\n\n\n   user  system elapsed \n  0.085   0.070   0.081 \n\n\nshow code\n# stop the cluster\ndoParallel::stopImplicitCluster()\n\n\nHere we see that lapply is much faster. Of course that is because of all the overheads and all that.\nNext, I will show to use the various makeCluster() options."
  },
  {
    "objectID": "content/blog/posts/2023-09-06-about-dataloaders/index.html",
    "href": "content/blog/posts/2023-09-06-about-dataloaders/index.html",
    "title": "Notes on dataloaders",
    "section": "",
    "text": "show code\nimport torch\nimport numpy as np, os, sys, pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\nprint(f'Kernel used is: {os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))}')\n\n\nFontconfig warning: ignoring UTF-8: not a valid region tag\n\n\nKernel used is: dl-tools\n\n\n\nIntroduction\nDuring training deep learning models (or any machine learning model for that matter), we try to make the most of available data. One way we do this is to supply a batch of the data to the model at a training iteration. So, if you have 5000 observations to train on, you can supply, say, 20 at a time.\nIn addition, loading 5000 observations all at once may consume a lot of memory, especially if you have limited resources.\npytorch gives us a convenient way to load data in this manner by letting us create our own dataset objects, which are used by pytorch’s dataloader\n\n\nClass Dataset\n\n\nshow code\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\n\nThe trick to create your Dataset object is that when you call the class, or attempt to get an item from the dataset, it should return one training observation.\nI will create a fictitious dataset, observations X and ground truth Y. They will be numpy arrays; this way I can easily manipulate them.\n\n\nshow code\nX = np.random.rand(100, 48) \nY = np.random.choice([0,1], size=100)\nX.shape, Y.shape\n\n\n((100, 48), (100,))\n\n\nI create a dataset class, MyDataset, that will take just one observation and ground truth at a time\n\n\nshow code\nclass MyDataset(Dataset): # will inherit from the Dataset object\n    def __init__(self, X, Y):\n        self.X = X\n        self.Y = Y\n    \n    def __len__(self): # the dataloader needs to know the number of observations you have\n        return self.X.shape[0]\n\n    def __getitem__(self, idx): # this is what returns just one observation or one unit of training\n        return(self.X[idx, : ], self.Y[idx])\n\n\nNow I can use the dataloader object\n\n\nshow code\nmydataset = MyDataset(X, Y)\nmydataset\n\n\n<__main__.MyDataset at 0x11d3808d0>\n\n\nYou can confirm that the dataset object works by doing this. I give it an index, 8 and it pulls the observations and ground truth corresponding to that index.\n\n\nshow code\nmydataset.__getitem__(8)\n\n\n(array([0.33197901, 0.10920114, 0.32552711, 0.55107147, 0.95523331,\n        0.82203799, 0.58211899, 0.9134724 , 0.15777504, 0.74666818,\n        0.84837099, 0.53785637, 0.49206978, 0.90127475, 0.7626803 ,\n        0.89917058, 0.01275132, 0.94277784, 0.73115781, 0.76832774,\n        0.41417915, 0.83662125, 0.69430352, 0.97880989, 0.25958756,\n        0.04993424, 0.2055082 , 0.48704122, 0.55182948, 0.72521316,\n        0.58642776, 0.95965883, 0.35750039, 0.02896049, 0.34491265,\n        0.81426974, 0.47463192, 0.08679966, 0.64945759, 0.28330604,\n        0.0216591 , 0.30981423, 0.97186651, 0.95268351, 0.42557078,\n        0.15942108, 0.79952813, 0.98738138]),\n 1)\n\n\n\n\nDataloader\nAll well and good. But I don’t want to give my model one observation at a time. Although people do this, it is too small. Instead, I want to give the model a certain batch at time. Dataloaders help with this.\nI create a DataLoader object and supply it the argument batch_size. Whenever I ask the object for training examples, it gives me batch_size number of observations at a time. Here I will set batch_size to 50.\n\n\nshow code\nmydataloader = DataLoader(mydataset, batch_size=20)\n\n\n\n\nshow code\nfor i, batch in enumerate(mydataloader):\n    print(f'batch {i}: number of observations and ground truth are {batch[0].shape[0]} and {batch[1].shape[0]} respectively')\n\n\nbatch 0: number of observations and ground truth are 20 and 20 respectively\nbatch 1: number of observations and ground truth are 20 and 20 respectively\nbatch 2: number of observations and ground truth are 20 and 20 respectively\nbatch 3: number of observations and ground truth are 20 and 20 respectively\nbatch 4: number of observations and ground truth are 20 and 20 respectively"
  },
  {
    "objectID": "content/blog/index.html",
    "href": "content/blog/index.html",
    "title": "codes + notes + projects I feel like sharing",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nSep 7, 2023\n\n\nNotes on dataloaders\n\n\n\n\nSep 6, 2023\n\n\nUnderstanding pvalues, multiple testing and qvalues\n\n\n\n\nSep 3, 2023\n\n\nParallelization in R\n\n\n\n\nJul 18, 2022\n\n\nPredicting swarm behaviour\n\n\n\n\nMay 13, 2022\n\n\nSimulating discrete and continuous populations\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/assets/publications/index.html",
    "href": "content/assets/publications/index.html",
    "title": "publications",
    "section": "",
    "text": "I would rather the currency be dollars but anyway…\n\n\n\n\n\n\n  \n    \n      \n        Machine Learning-Based Predictive Modeling of Postpartum Depression\n        Dayeon Shin, Kyung Ju Lee, Temidayo Adeluwa, Junguk Hur\n        2020 | Journal of Clinical Medicine\n      \n    \n  \n\n\n  \n    \n      \n        Novel Action of Vinpocetine in the Prevention of Paraquat-Induced Parkinsonism in Mice: Involvement of Oxidative Stress and Neuroinflammation\n        Ismaila Ishola, A.A. Akinyede, T.P. Adeluwa, C. Micah\n        2018 | Metabolic Brain Disease\n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "content/assets/thoughts/index.html",
    "href": "content/assets/thoughts/index.html",
    "title": "the little things filling up my head",
    "section": "",
    "text": "about AI\n    \n  \n  \n    genetics, pharmacogenetics, machine learning, and statistical methods\n    \n  \n  \n    climate change + recycling\n    \n  \n  \n    rap, as in the music genre\n    \n  \n  \n    what to do after a PhD\n    \n  \n  \n    becoming a good-enough chef + next DIY projects\n    \n  \n  \n    collecting/making my own bracelets\n    \n  \n  \n    football, or soccer, if you are North American\n    \n  \n  \n    why there is so much kickback against Oxford commas\n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Temi",
    "section": "",
    "text": "My name is Temi, and I’m a PhD student in the Genetics, Genomics and Systems Biology (GGSB) program at The University of Chicago. Prior to this, I completed a master’s at The University of North Dakota, supervised by the awesome Junguk Hur."
  },
  {
    "objectID": "index.html#currently",
    "href": "index.html#currently",
    "title": "Temi",
    "section": "currently",
    "text": "currently\nIn fall 2022, I joined Haky’s lab for my PhD thesis. Here, I will be studying variations in the human epigenome (at the genomic level and the population level), how these variations influence molecular phenotypes (e.g. TF binding), and how these, in turn, influence complex traits and diseases. These are complex/interesting questions. Wish me luck!\n\ncv\nresume\npublications"
  },
  {
    "objectID": "index.html#thinking-about",
    "href": "index.html#thinking-about",
    "title": "Temi",
    "section": "thinking about",
    "text": "thinking about\nHere’s a complete list\n\ngenetics, pharmacogenetics, machine learning, and statistical methods\nclimate change + recycling\nrap, as in the music genre\nwhat to do after a PhD\nbecoming a good-enough chef + next DIY projects\ncollecting/making my own bracelets\nfootball, or soccer, if you are North American\nwhy there is so much kickback against Oxford commas [even the AP style recommended not using them until very recently]"
  },
  {
    "objectID": "index.html#links-reach-out",
    "href": "index.html#links-reach-out",
    "title": "Temi",
    "section": "links + reach out",
    "text": "links + reach out\n\nYou can reach me by filling this form; or\nLinkedIn; or\nTwitter; or\nGithub"
  },
  {
    "objectID": "index.html#other-lives",
    "href": "index.html#other-lives",
    "title": "Temi",
    "section": "other lives",
    "text": "other lives\nOften, I am in the gym doing bodyweight workouts. Sometimes, I’ll read books + watch TV. Other times, I’ll play the guitar, football, and do some DIYs for fun. And if I have enough time left, I’ll stare outside the window for a bit, or go out and touch grass."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]